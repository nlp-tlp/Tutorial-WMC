{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Background Survey\n",
    "\n",
    "In this section, we are going to understand your knowledge and background. Please follow the instructions:\n",
    "\n",
    "1. go to https://www.menti.com/\n",
    "2. Enter the code [6705694] to join the session\n",
    "3. Answer the questions shown in the screen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# B. Knowledge Graph Construction from Technical Short Text\n",
    "\n",
    "In this notebook we are going to construct a simple knowledge graph using Python, and run some queries on the graph in Neo4j. We have broken the notebook into several steps:\n",
    "\n",
    "1. Introduction\n",
    "2. Introduction to Natural and Technical Language Processing\n",
    "3. Loading the data\n",
    "4. Cleaning the data via **Lexical Normalisation**\n",
    "5. Extracting entities via **Named Entity Recognition** (NER)\n",
    "6. Creating relations between entities via **Relation Extraction** (RE)\n",
    "7. Combining Named Entity Recognition + Relation Extraction\n",
    "8. Creating the graph\n",
    "9. Querying the graph in Neo4j\n",
    "\n",
    "# 1. Introduction\n",
    "\n",
    "## 1.1 Requirements\n",
    "\n",
    "We will be walking through this notebook, so there is no need for you to install any of these requirements unless you are interested in running it yourself after the class. If you do wish to do run it yourself later, you need the following open-source tools:\n",
    "1. Python 3: A powerful programming language known for its simplicity, readability, and extensive library support.\n",
    "2. Neo4j: A powerful graph database that enables you to model, store, and query complex connected data with efficiency.\n",
    "3. Necessary Python packages: py2neo, gqvis, flair. \n",
    "\n",
    "To install the necessary Python packages, execute the following command using pip (as shown in the code cell below):\n",
    "- `py2neo`: A library for working with Neo4j in Python.\n",
    "- `gqvis`: Our simple tool for visualising graph queries in Jupyter.\n",
    "- `flair`: A deep learning library for natural language processing. Note this library is quite large (a couple gb I believe). If you don't wish to install this, we have provided non deep-learning based alternatives so you can still run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install py2neo\n",
    "!pip install gqvis\n",
    "!pip install flair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download Neo4j by visiting this link: : https://neo4j.com/download/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Problem description\n",
    "\n",
    "Maintenance work orders (MWOs) capture information on the maintenance performed on assets. Much of this information is structured - such as dates, the functional location (the specific identifier of the asset), costs, and so on. However, a significant volume of the knowledge buried within a MWO is unstructured and therefore inaccessible - it is buried within the short text description.\n",
    "\n",
    "Here are some examples of these short text descriptions:\n",
    "\n",
    "    replace pump\n",
    "    a/c running hot\n",
    "    repair cracked hydraulic tank\n",
    "    \n",
    "As you can see, they often contain indicators of failure modes (e.g. overheating) and end of life events (e.g. a replacement). It would be useful to be able to automatically discover these, however it is next to impossible to manually trawl through thousands of these work orders to discover patterns. We need some way to ask questions of our unstructured data, which is where knowledge graphs shine.\n",
    "\n",
    "\n",
    "Our task in this session is therefore is to transform these work orders into a **knowledge graph**. We will be primarily focusing on the short text descriptions, but there will be some discussion at the end on incorporating other structured knowledge (e.g. dates) into our graph. The goal of building this knowledge graph is to be able to **ask questions of our data** such as \"what are the failure modes observed on pumps?\" and \"which assets have had leaks in the past 6 months?\"\n",
    "\n",
    "![Insight](images/dikw_pyramid.png \"Insight\")\n",
    "\n",
    "Image sourced from [OntoText](https://www.ontotext.com/knowledgehub/fundamentals/dikw-pyramid/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Introduction to Natural (and Technical) Language Processing\n",
    "\n",
    "**Natural language processing** (NLP) is the study of the automatic interpetation and manipulation of natural language, like speech and written text. \n",
    "\n",
    "**Technical language processing** (TLP) is a subset of NLP that focuses specifically on technical text, such as the text present in maintenance work orders, doctor's notes, safety records, and so on. \n",
    "\n",
    "<img src=\"images/KG_con.png\" alt=\"KG_con\" width=\"800px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Core NLP terms and ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first define some core NLP terms and ideas that we will see numerous times throughout this notebook.\n",
    "\n",
    "### 2.1.1. Corpus\n",
    "\n",
    "A 💡 **corpus** is a set of text documents, where a document can be a word, sentence, paragraph, report, etc. Our corpus is a dataset of maintenance work order records.\n",
    "\n",
    "### 2.1.2. Tokenisation\n",
    "\n",
    "The first step for almost any NLP application is 💡 **Tokenisation**. This task splits text into smaller units, such as sentences (sentence tokenisation), words (word tokenisation), or characters (character tokenisation). For most applications, we split text into words.\n",
    "\n",
    "The simplest (but by no means best) way to tokenise is using Python's `split` function, which simply splits a string using the space character as the delimeter:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"this is a sentence\"\n",
    "words = sentence.split()\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is good enough for our purposes (tokenising maintenance work orders), but does not perform well on natural language datasets due to the prevalence of punctuation (full stops, commas etc). The `split` function fails when our sentence ends with a full stop, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'sentence.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"this is a sentence.\"\n",
    "words = sentence.split()\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately this does not happen in our dataset so for simplicity we will just be using the `split` function.\n",
    "\n",
    "If you are interested in learning more about tokenisation, there are many tokenisation libraries available in Python. The most popular perhaps is NLTK, which has a range of tokenisers available (see [here](https://www.nltk.org/api/nltk.tokenize.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. Vocabulary\n",
    "\n",
    "A 💡 **vocabulary** is set of terms that correspond to a particular subject matter. The vocabulary of maintenance texts will be quite large, even though the length of the texts are small. This is because maintainers often write the same word many different ways ('air conditioner' is written as 'a/c', 'air cond', etc). To illustrate why this is an issue, we found that for a corpus of 50,000 maintenance work order records, there were 18,238 unique tokens - predominately due to all of these different word variants.\n",
    "\n",
    "Note that depending on the domain, we may treat varying capitalisation of the same word (\"apple\" and \"Apple\", for example) as different words. In this notebook we will treat everything as lower-cased, though in many domains this is not a good idea (especially when many acronyms are present).\n",
    "\n",
    "Determining our vocabulary is a matter of creating a set of unique words in the corpus, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this', 'sentence', 'is', 'the', 'second', 'first'}\n"
     ]
    }
   ],
   "source": [
    "vocabulary = set()\n",
    "\n",
    "dataset = [\"this is the first sentence\", \"this is the second sentence\"]\n",
    "for sent in dataset:\n",
    "    words = sent.split()\n",
    "    for word in words:\n",
    "        vocabulary.add(word.lower())\n",
    "\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🤔 Machines cannot directly process or understand text data. How do we train models to understand the vocabulary in NLP?\n",
    "\n",
    "&#x1F609; By transforming vocabularies into numerical representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4. Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "💡 **Word embeddings** are numerical representations of language. We can think of an embedding as a `k` dimensional vector, whereby the embeddings of **semantically similar** items (typically words) have a **high cosine similarity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Embeddings](images/embeddings_2.png \"Embeddings\")\n",
    "\n",
    "Image sourced from [Medium](https://medium.com/@hari4om/word-embedding-d816f643140)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how \"cat\" and \"kitten\" are close, while \"cat\" and \"houses\" are far away.\n",
    "\n",
    "Actual word embeddings are often much larger (typically 512 dimensions at least), but the general principle is the same. The model that generates the embeddings is designed to ensure that words appearing in a similar context (cat and dog etc) have similar vectors, but words appearing in different contexts (cat and cup) have dissimilar vectors.\n",
    "\n",
    "For fun, you might like to check out [Semantle](https://semantle.com/), where the goal is to determine the word of the day via its cosine similarity to other words.\n",
    "\n",
    "&#x1F914; How do we generate embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language models\n",
    "\n",
    "\n",
    "💡 **Language models**, such as GPT, are used to generate embedding vectors. The choice of model depends largely on the corpus.\n",
    "\n",
    "Language models are typically **trained** on a corpus and then used to generate embeddings for words appearing in a different corpus. They can also be **fine-tuned** so that the model predicts more meaningful embeddings for words in a new corpus.\n",
    "\n",
    "Below is a table of some of the most popular language models, including how the models are trained.\n",
    "\n",
    "| Name     | Model                                                 | Method                                                                     | Granularity            |\n",
    "|:---------|:------------------------------------------------------|:---------------------------------------------------------------------------|:-----------------------|\n",
    "| word2vec | Feedforward neural network                            | Predict word given its context (CBOW), or context given a word (skip-gram) | Word-based             |\n",
    "| GloVe    | Log-bilinear regression model                         | Train word vectors from global co-occurence matrix                         | Word-based             |\n",
    "| FastText | Feedforward neural network                            | Predict context given a word (skip-gram)                                   | Character n-gram based |\n",
    "| ELMo     | Bi-directional Long Short Term Memory model (Bi-LSTM) | Predict word given all previous/next words                                 | Character-based        |\n",
    "| BERT     | Bi-directional Transformer                            | Predict masked word(s)                                                     | Wordpiece-based        |\n",
    "| Flair    | Bi-LSTM + Conditional Random Field (CRF)              | Predict next character given the previous characters                                                | Character-based     |\n",
    "\n",
    "While `word2vec` remains one of the most popular language models, it has one key downside - it is not able to generate embedding vectors for words that do not appear in the corpus on which it was trained. This is a problem for our maintenance texts as they are rife with spelling errors, acronyms, etc, as well as many terms that do not occur in common natural language.\n",
    "\n",
    "Character-level embeddings are able to deal with this issue because they are modelled at the character-level, and thus the vector for similar spellings of the same word ('pump', 'puump', etc) will have high cosine similarity. For our application we will therefore choose Flair embeddings, though FastText, ELMO and BERT would work well too.\n",
    "\n",
    "You can learn more about Flair via the papers ([here](https://aclanthology.org/N19-4010/) and [here](https://aclanthology.org/C18-1139.pdf)), or check out the [GitHub repository](https://github.com/flairNLP/flair)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5. Sequence Labelling and Text Classification\n",
    "\n",
    "There are two umbrella terms that almost all NLP tasks fall under - **sequence labelling** and **text classification**.\n",
    "\n",
    "💡 **Sequence labelling** involves assigning a label to every item in a sequence. Examples of sequence labelling tasks involve Named Entity Recognition (assigning an entity class label to every token in a sentence) and Lexical Normalisation (assigning the correct form of the word to every word in a sentence).\n",
    "\n",
    "\n",
    "💡 **Text classification** involves assigning one or more label(s) to an entire sequence. Examples of text classification tasks include sentiment analysis (determining whether a document is positive, negative, or neutral). In this notebook, our Relation Extraction model is also a text classification model (determine the relation type of a given (entity 1, entity 2, context)).\n",
    "\n",
    "![NLP Tasks](images/nlp_tasks.jpg \"NLP Tasks\")\n",
    "\n",
    "\n",
    "For our task we are going to use `Flair`, which is a Bidirectional LSTM model, for both Named Entity Recognition and Relation Extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Supervised learning and the need for annotated data\n",
    "(use google to explain annotation)\n",
    "\n",
    "The majority of feature extraction and representation learning-based models are **supervised learning** models, i.e. they learn from annotated data. Annotated data can be obtained via manual annotation using tools such as [Redcoat](https://nlp-tlp/redcoat) and [QuickGraph](https://quickgraph.nlp-tlp.org).\n",
    "\n",
    "For this notebook, our NER model is trained using an annotated dataset of ~4k MWOs tagged by the [UWA Natural & Technical Language Group](https://nlp-tlp.org). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We are going to be building a knowledge graph on a small sample set of work orders. This will not be seen by the Named Entity Recognition or Relation Extraction models prior to constructing the graph - the idea is to get our models to run *inference* over this dataset to automatically predict the entities, and relationships between the entities, to build a graph.\n",
    "\n",
    "- `sample_work_orders.csv`: A csv file containing a set of work orders.\n",
    "\n",
    "We are using the simple `csv` library to read in the data, though this can also be done using `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StartDate     FLOC          ShortText                              Cost   \n",
      "----------------------------------------------------------------------------------------------------\n",
      "              1234.02.11    broken handraill/h/s crows nest        3200   \n",
      "              1234.02.12    broken hose on cylinder                3300   \n",
      "              1234.02.13    broken l/h pulldown chain              3400   \n",
      "              1234.02.14    broken locking pin                     3500   \n",
      "17/08/2001    1234.02.15    broken tool wrench holding cylinder    3600   \n",
      "              1234.02.16    build up rod support beak              3700   \n",
      "              1234.02.17    bull hose air leak                     3800   \n",
      "25/02/2006    1234.02.18    bull hose split                        3900   \n",
      "              1234.02.19    busted hydraulic hose                  4000   \n",
      "              1234.02.20    c spanner for minning                  4100   \n"
     ]
    }
   ],
   "source": [
    "from csv import DictReader\n",
    "from helpers import print_table\n",
    "\n",
    "work_order_file = \"data/sample_work_orders.csv\"\n",
    "\n",
    "# A simple function to read in a csv file and return a list,\n",
    "# where each element in the list is a dictionary of {heading : value}\n",
    "def load_csv(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = DictReader(f)\n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "    return data\n",
    "\n",
    "        \n",
    "work_order_data = load_csv(work_order_file)\n",
    "\n",
    "# Let's have a look at 10 rows\n",
    "print_table(work_order_data[30:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#x1F914; Can we directly use the raw data in NLP tasks?\n",
    "\n",
    "&#x1F609; No, preprocessing is necessary, which helps in improving text consistency, reducing noise, and ensuring better analysis and understanding of the data by NLP models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 4. Preprocessing the data via Lexical Normalisation\n",
    "\n",
    "Our maintenance work order data is quite noisy - there are spelling errors, typos, acronyms, abbreviations, and so on. This will present challenges to us later when it comes time to build the knowledge graph. So to deal with these issues, we should first clean the text using **lexical normalisation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the interest of time/simplicity we are not going to use a neural model here, but instead we will use a simple lexicon-based normaliser. This model will simply replace a misspelled phrase with its correct form. This is not practical in the real world (as there's no way we could possibly build a lexicon of all possible misspellings) but it is good enough for our small example.\n",
    "\n",
    "The following code imports our `LexiconNormaliser` model. This model simply replaces any predefined terms with their replacements, e.g. \"puump\" will be normalised to \"pump\", and so on.\n",
    "\n",
    "If you're interested in seeing this lexicon, it's available under [data/lexicon_normalisation.csv](files/data/lexicon_normalisation.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air /con very noisy\n",
      "air conditioner very noisy\n",
      "\n",
      "broken l/h pulldown chain\n",
      "breakdown l/h pulldown chain\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from helpers import LexiconNormaliser\n",
    "\n",
    "lexicon_file = \"data/lexicon_normalisation.csv\"\n",
    "normaliser = LexiconNormaliser(lexicon_file)\n",
    "\n",
    "work_order_data = load_csv(work_order_file)\n",
    "\n",
    "for i, row in enumerate(work_order_data):\n",
    "    before = row['ShortText']    \n",
    "    row['ShortText'] = normaliser.normalise(row['ShortText'])\n",
    "    # Let's print same samples to have a look at the difference\n",
    "    if i in (12,32):#in (12):# and i <= 140:\n",
    "        print(before)\n",
    "        print(row['ShortText'])\n",
    "        print()\n",
    "        \n",
    "        \n",
    "# change the example  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 5. Named Entity Recognition\n",
    "\n",
    "The next task is to extract the entities in the short text descriptions and construct nodes from those entities. This is how we are able to unlock the knowledge captured within the short text and combine it with the structured fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER is a **sequence labelling** task. The choice of entity class depends on the corpus. NER on natural language corpora typically uses four classes (`Person`, `Organisation`, `Location`, `Miscellaneous`), though there are many schemas available. In our application, we will use the following three classes:\n",
    "\n",
    "- **`Item`**: A maintainable item such as \"exhaust\".\n",
    "- **`Activity`**: A maintenance activity performed on an item, such as \"replace\".\n",
    "- **`Observation`**: An observation on an Item, such as \"lagging\".\n",
    "\n",
    "![Sequence_Labelling](images/seq_label.png)\n",
    "\n",
    "Note the **BIO** format being used here (\"beginning\", \"inside\", \"outside\"). **B** denotes that the word is the start of en entity, **I** denotes that the word is inside an entity, and **O** denotes that a word is not an entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Load the dataset\n",
    "The dataset to train and evaluate the NER model is split into three files:\n",
    "\n",
    "- `ner_dataset/train.txt`: The dataset we will use to *train* the NER model to predict the entities appearing in each work order.\n",
    "- `ner_dataset/dev.txt`: The dataset we will use to *validate* the quality of the model during training.\n",
    "- `ner_dataset/test.txt`: The dataset we will use to *evaluate* the final performance of the NER model after training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the first row of our training dataset to make sure it loads OK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3201 documents from data/ner_dataset/train.txt.\n",
      "{'tokens': ['batteries', 'flat'], 'labels': ['B-Item', 'B-Observation']}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from helpers import load_conll_dataset\n",
    "\n",
    "NER_DATASET_PATH = \"data/ner_dataset\"\n",
    "train_dataset = load_conll_dataset(os.path.join(NER_DATASET_PATH, 'train.txt'))\n",
    "\n",
    "print(train_dataset[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Define our NER model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this tutorial we will use [Flair](https://github.com/flairNLP/flair), which simplifies the process of building a deep learning model for a variety of NLP tasks.\n",
    "\n",
    "The code below is a class representing a `FlairNERModel`. It has the same three methods, i.e `train()`, `inference()`, and `save()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"A Flair-based Named Entity Recognition model. Learns to predict entity\n",
    "classes via deep learning.\"\"\"\n",
    "\n",
    "import os\n",
    "import flair\n",
    "from helpers.NERModel import NERModel\n",
    "from flair.data import Corpus, Sentence\n",
    "from flair.datasets import ColumnCorpus\n",
    "from flair.embeddings import (\n",
    "    StackedEmbeddings,\n",
    "    FlairEmbeddings,\n",
    ")\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer\n",
    "from typing import List\n",
    "from flair.visual.training_curves import Plotter\n",
    "import torch\n",
    "\n",
    "\n",
    "HIDDEN_SIZE = 256\n",
    "\n",
    "# Check whether CUDA is available and set the device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    flair.device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    flair.device = torch.device(\"cpu\")\n",
    "print(\"Device:\", flair.device)\n",
    "\n",
    "\n",
    "class FlairNERModel(NERModel):\n",
    "\n",
    "    model_name: str = \"Flair\"\n",
    "\n",
    "    \"\"\"A Flair-based Named Entity Recognition model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FlairNERModel, self).__init__()\n",
    "\n",
    "        self.model = None\n",
    "\n",
    "    def train(self, datasets_path: os.path, trained_model_path: os.path):\n",
    "        \"\"\" Train the Flair model on the given conll datasets.\n",
    "\n",
    "        Args:\n",
    "            datasets_path (os.path): The folder containing the\n",
    "              train, dev and text CONLL-formatted datasets.\n",
    "            trained_model_path (os.path): The folder to save the trained\n",
    "              model to.\n",
    "        \"\"\"\n",
    "\n",
    "        columns = {0: \"text\", 1: \"ner\"}\n",
    "        corpus: Corpus = ColumnCorpus(\n",
    "            datasets_path,\n",
    "            columns,\n",
    "            train_file=\"train.txt\",\n",
    "            dev_file=\"dev.txt\",\n",
    "            test_file=\"test.txt\",\n",
    "        )\n",
    "        label_dict = corpus.make_label_dictionary(label_type=\"ner\")\n",
    "\n",
    "        # Train the sequence tagger\n",
    "        embedding_types = [\n",
    "            FlairEmbeddings(\"mix-forward\"),\n",
    "            FlairEmbeddings(\"mix-backward\"),\n",
    "        ]\n",
    "\n",
    "        embeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "\n",
    "        tagger = SequenceTagger(\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            embeddings=embeddings,\n",
    "            tag_dictionary=label_dict,\n",
    "            tag_type=\"ner\",\n",
    "            use_crf=True,\n",
    "        )\n",
    "\n",
    "        trainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "        sm = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            sm = \"gpu\"\n",
    "        trainer.train(\n",
    "            trained_model_path,\n",
    "            learning_rate=0.1,\n",
    "            mini_batch_size=32,\n",
    "            max_epochs=10,\n",
    "            embeddings_storage_mode=sm,\n",
    "        )\n",
    "\n",
    "        plotter = Plotter()\n",
    "        plotter.plot_weights(os.path.join(trained_model_path, \"weights.txt\"))\n",
    "\n",
    "        self.load(os.path.join(trained_model_path, 'final-model.pt'))\n",
    "\n",
    "    def inference(self, sent: list) -> dict:\n",
    "        \"\"\"Run the inference on a given list of short texts.\n",
    "\n",
    "        Args:\n",
    "            sent (list): The sentence (list of words).\n",
    "\n",
    "        Returns:\n",
    "            dict: The tagged sentence now in the form of {'tokens': [list],\n",
    "                'labels': [list]}.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the model has not yet been trained.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\n",
    "                \"The NER Model has not yet been trained. \"\n",
    "                \"Please train/load this Flair model before proceeding.\"\n",
    "            )\n",
    "        \n",
    "        sentence_obj = Sentence(sentence, use_tokenizer=False)\n",
    "        self.model.predict(sentence_obj)\n",
    "        labels = [\"O\"] * len(sentence)\n",
    "\n",
    "        for entity in sentence_obj.get_spans(\"ner\"):\n",
    "            for i, token in enumerate(entity):\n",
    "                label = entity.get_label(\"ner\").value\n",
    "                prefix = \"B-\" if i == 0 else \"I-\"\n",
    "                \n",
    "                # Token idx starts from 1 in Flair.\n",
    "                labels[token.idx - 1] = prefix + label\n",
    "\n",
    "        return { 'tokens': sent, 'labels': labels }\n",
    "\n",
    "    def load(self, model_path: str):\n",
    "        \"\"\"Load the model from the specified path.\n",
    "\n",
    "        Args:\n",
    "            model_path (os.path): The path to load.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the path does not exist i.e. model not yet trained.\n",
    "        \"\"\"\n",
    "        self.model = SequenceTagger.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Training the model\n",
    "\n",
    "In the interest of time, we will be using a simple dictionary-based model rather than the Flair-based model above. This dictionary model simply matches terms against a predefined dictionary. In real-world applications, we would always use Flair (or other deep learning models) as they are able to learn to differentiate between terms based on context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n",
      "Loaded 3201 documents from data/ner_dataset/train.txt.\n",
      "Loaded 402 documents from data/ner_dataset/dev.txt.\n"
     ]
    }
   ],
   "source": [
    "from helpers import DictionaryNERModel\n",
    "\n",
    "NER_DATASET_PATH = \"data/ner_dataset\"\n",
    "\n",
    "dictionary_ner_model = DictionaryNERModel()\n",
    "dictionary_ner_model.train(NER_DATASET_PATH, 'models/ner_models/dictionary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to train the Flair-based model yourself (not recommended unless you have a GPU with CUDA enabled), the code for doing that is as follows:\n",
    "\n",
    "    flair_ner_model = FlairNERModel()\n",
    "    flair_ner_model.train(NER_DATASET_PATH, 'models/ner_models/flair')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Testing the model\n",
    "\n",
    "The next step is to use our trained model to infer the entity type of each entity appearing in a list of previously unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['bent', 'handrail', 'at', 'back', 'cabin', 'door'], 'labels': ['B-Observation', 'B-Item', 'O', 'O', 'B-Item', 'B-Item']}\n"
     ]
    }
   ],
   "source": [
    "tagged_bio_sents = []\n",
    "\n",
    "model = dictionary_ner_model\n",
    "#model = flair_ner_model # uncomment to use Flair\n",
    "\n",
    "sentences = []\n",
    "for row in work_order_data:\n",
    "    sentence = row[\"ShortText\"].split() # We must 'tokenise' the sentence first, i.e. split into words\n",
    "    tagged_sent = model.inference(sentence) # replace 'flair' with 'dictionary' if not using flair  \n",
    "    tagged_bio_sents.append(tagged_sent)\n",
    "\n",
    "# Print an example tagged sentence\n",
    "print(tagged_bio_sents[22])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Extracting relations between the entities via Relation Extraction\n",
    "\n",
    "We have extracted the entities appearing in each work order. The next step is to extract the relationships between those entities. We can do this using Relation Extraction.\n",
    "\n",
    "For our application we are going to use of the following relationships:\n",
    "\n",
    " - **HAS_OBSERVATION**: A relationship between an `Item` and an `Observation`.\n",
    " - **HAS_ACTIVITY**: A relationship between an `Item` and an `Activity`.\n",
    " - **APPEARS_WITH**: A relationship between an `Item` and another `Item`.\n",
    "\n",
    "![alt text](images/building-relations.png \"Building relations\")\n",
    "\n",
    "The process of relation extraction is similar to NER.\n",
    "\n",
    "In the interest of time, we won't be going into the details of how to run Relation Extraction in this workshop. You are welcome to browse the source code of this repository if you are interested in diving into the finer details. The important thing to note is that the process is very similar to Named Entity Recognition:\n",
    "\n",
    " - First, load the dataset\n",
    " - Define a Relation Extraction model (we could use Flair, or any other deep learning-based model)\n",
    " - Train that Relation Extraction model on the dataset\n",
    " - Run 'inference' over unseen data in order to extract the relations between entities in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['access', 'audit', 'Item', 'Activity', 'access audit', 0, 1, 'HAS_ACTIVITY', 10]\n",
      "['access', 'repairs', 'Item', 'Activity', 'access audit repairs', 0, 2, 'HAS_ACTIVITY', 10]\n",
      "['audit', 'access', 'Activity', 'Item', '', 1, 0, 'O', 10]\n"
     ]
    }
   ],
   "source": [
    "from helpers import run_relation_extraction\n",
    "\n",
    "tagged_relations, tagged_sents = run_relation_extraction(tagged_bio_sents)\n",
    "\n",
    "# Print a few example rows.\n",
    "for row in tagged_relations[22:25]:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further implementation details, please see `helpers/RE.py` (for the Flair based model), and `helpers/run_relation_extraction.py` (for the data wrangling/running the RE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Combining Named Entity Recognition + Relation Extraction\n",
    "\n",
    "Now we have outputs from both the NER model and the RE model. The next step is to combine the two outputs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"tokens\": [\n",
      "  \"adjust\",\n",
      "  \"rotary\",\n",
      "  \"head\",\n",
      "  \"guides\"\n",
      " ],\n",
      " \"mentions\": [\n",
      "  {\n",
      "   \"start\": 0,\n",
      "   \"labels\": [\n",
      "    \"Activity\"\n",
      "   ],\n",
      "   \"end\": 1,\n",
      "   \"phrase\": \"adjust\"\n",
      "  },\n",
      "  {\n",
      "   \"start\": 1,\n",
      "   \"labels\": [\n",
      "    \"Item\"\n",
      "   ],\n",
      "   \"end\": 2,\n",
      "   \"phrase\": \"rotary\"\n",
      "  },\n",
      "  {\n",
      "   \"start\": 2,\n",
      "   \"labels\": [\n",
      "    \"Item\"\n",
      "   ],\n",
      "   \"end\": 3,\n",
      "   \"phrase\": \"head\"\n",
      "  },\n",
      "  {\n",
      "   \"start\": 3,\n",
      "   \"labels\": [\n",
      "    \"Item\"\n",
      "   ],\n",
      "   \"end\": 4,\n",
      "   \"phrase\": \"guides\"\n",
      "  }\n",
      " ],\n",
      " \"relations\": [\n",
      "  {\n",
      "   \"start\": 1,\n",
      "   \"end\": 0,\n",
      "   \"type\": \"HAS_ACTIVITY\"\n",
      "  },\n",
      "  {\n",
      "   \"start\": 1,\n",
      "   \"end\": 2,\n",
      "   \"type\": \"HAS_ITEM\"\n",
      "  },\n",
      "  {\n",
      "   \"start\": 1,\n",
      "   \"end\": 3,\n",
      "   \"type\": \"HAS_ITEM\"\n",
      "  },\n",
      "  {\n",
      "   \"start\": 2,\n",
      "   \"end\": 0,\n",
      "   \"type\": \"HAS_ACTIVITY\"\n",
      "  },\n",
      "  {\n",
      "   \"start\": 2,\n",
      "   \"end\": 1,\n",
      "   \"type\": \"HAS_ITEM\"\n",
      "  },\n",
      "  {\n",
      "   \"start\": 2,\n",
      "   \"end\": 3,\n",
      "   \"type\": \"HAS_ITEM\"\n",
      "  },\n",
      "  {\n",
      "   \"start\": 3,\n",
      "   \"end\": 0,\n",
      "   \"type\": \"HAS_ACTIVITY\"\n",
      "  },\n",
      "  {\n",
      "   \"start\": 3,\n",
      "   \"end\": 1,\n",
      "   \"type\": \"HAS_ITEM\"\n",
      "  },\n",
      "  {\n",
      "   \"start\": 3,\n",
      "   \"end\": 2,\n",
      "   \"type\": \"HAS_ITEM\"\n",
      "  }\n",
      " ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "for i, sent in enumerate(tagged_sents):\n",
    "    \n",
    "    # Note we only care about the relations that do not have the class \"O\".\n",
    "    doc_relations = [row for row in tagged_relations if row[7] != \"O\" and row[8] == i]\n",
    "    \n",
    "    sent['relations'] = []    \n",
    "    for row in doc_relations:\n",
    "        rel = {'start': row[5], 'end': row[6], 'type': row[7]}     \n",
    "        sent['relations'].append(rel)\n",
    "\n",
    "# Let's print an example...\n",
    "print(json.dumps(tagged_sents[11], indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Creating the graph\n",
    "\n",
    "We now have a data structure that stores the tokens, entity mentions, and relationships between those mentions, for each document. The last step is to put it all into a Neo4j graph so that we can query this information.\n",
    "\n",
    "There are two popular methods for doing this:\n",
    "\n",
    "- Using `py2neo` to programatically insert data into Neo4j\n",
    "- Saving CSVs of your entities and relations, then reading them in via a `LOAD CSV` query in Neo4j\n",
    "\n",
    "> ⚠️ Before proceeding, make sure you have created a new graph in Neo4j and that your new Neo4j graph is running. You can do this by opening Neo4j Browser, clicking \"Add\" at the top-right, then creating a new graph database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for building the graph\n",
    "\n",
    "The following code creates the entire graph using `py2neo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "from py2neo.data import Node, Relationship\n",
    "from dateutil.parser import parse as parse_date\n",
    "\n",
    "GRAPH_PASSWORD = \"password\" # Set this to the password of your Neo4J graph\n",
    "\n",
    "\n",
    "def get_node_id(phrase, entity_class):\n",
    "    \"\"\"A simple function to generate an id.\n",
    "    This ensures an entity that can be different classes (pump for example) can have\n",
    "    a unique node for each class type.\n",
    "    \"\"\"\n",
    "    return f\"{phrase}__{entity_class}\"\n",
    "    \n",
    "# Dates are a little awkward in Neo4j - we have to convert it to an integer representation in Python.\n",
    "# The APOC library has functions to handle this better.\n",
    "def date_to_int(date):\n",
    "    parsed_date = parse_date(str(date))\n",
    "    date = int(\"%s%s%s\" % (parsed_date.year, str(parsed_date.month).zfill(2), str(parsed_date.day).zfill(2)))\n",
    "    return date    \n",
    "    \n",
    "def create_graph(tagged_sents):\n",
    "    \"\"\"Build the Neo4j graph.\n",
    "    We do this by iterating over each tagged_sentence, and constructing the\n",
    "    graph as follows:\n",
    "     - Create a node to represent the document itself.\n",
    "     - Create nodes for each entity appearing in that document, if they have not\n",
    "       already been created. Each unique combination of entity + class will be added, so\n",
    "       pump (the Item) is different from pump (the Activity).\n",
    "     - Create a relationship between each entity and each document in which it appears.\n",
    "     - Create a relationship between each entity and each other entity it is related to,\n",
    "       via the list of relations.\n",
    "     \n",
    "    Args:\n",
    "        tagged_sents(list): The list of tagged sentences.\n",
    "    \"\"\"\n",
    "    graph = Graph(password = GRAPH_PASSWORD)\n",
    "\n",
    "    # We will start by deleting all nodes and edges in the current graph.\n",
    "    # If we don't do this, we will end up with duplicate nodes and edges when running this script again.\n",
    "    graph.delete_all() \n",
    "\n",
    "    tx = graph.begin()\n",
    "    \n",
    "    # Keep track of the created entity nodes.\n",
    "    # We need a way to map the id of the nodes to the py2neo Node objects so that we can\n",
    "    # easily create relationships between these nodes.\n",
    "    created_entity_nodes = {}\n",
    "    \n",
    "    # Iterate over the list of tagged sentences and programmatically create the graph.\n",
    "    for i, sent in enumerate(tagged_sents):\n",
    "        \n",
    "        # Let's first grab some properties from the original work order dataset.\n",
    "        # We will add date and cost into our graph on the Document nodes.\n",
    "        work_order_row = work_order_data[i]\n",
    "        \n",
    "        document_properties = {\n",
    "            'name': \" \".join(sent['tokens']),\n",
    "            'cost': work_order_row['Cost']          \n",
    "            \n",
    "        }\n",
    "        if work_order_row['StartDate'] != \"\":\n",
    "            document_properties[\"date\"] = date_to_int(work_order_row['StartDate'])\n",
    "\n",
    "        \n",
    "        \n",
    "        # Create a node to represent the document.\n",
    "        # Note that if you had additional properties in tagged_sents (such as dates, costs, etc)\n",
    "        # you could add them as properties of the Document nodes here.\n",
    "        document_node = Node(\"Document\", **document_properties)\n",
    "        tx.create(document_node)\n",
    "        \n",
    "        tokens = sent['tokens']\n",
    "        mentions = sent['mentions']\n",
    "        relations = sent['relations']\n",
    "        \n",
    "        for m in mentions:\n",
    "            start = m['start']\n",
    "            end = m['end']\n",
    "            entity_class = m['labels'][0]        \n",
    "            phrase = \" \".join(tokens[start: end])     \n",
    "                    \n",
    "            # Create a node for this entity mention.\n",
    "            # If the node has already been created (i.e. it exists in created_nodes), \n",
    "            # simply retrieve that Node from created_entity_nodes.\n",
    "            # Otherwise, create it, and add it to created_entity_nodes.\n",
    "            entity_node_id = get_node_id(phrase, entity_class)\n",
    "\n",
    "            if entity_node_id in created_entity_nodes:\n",
    "                entity_node = created_entity_nodes[entity_node_id]\n",
    "            else:\n",
    "                entity_node = Node(\"Entity\", entity_class, _id=entity_node_id, name=phrase)\n",
    "                created_entity_nodes[entity_node_id] = entity_node\n",
    "                tx.create(entity_node)            \n",
    "                        \n",
    "                \n",
    "            # Create a relationship between that node and the document\n",
    "            # in which it appears.               \n",
    "            r = Relationship(entity_node, \"APPEARS_IN\", document_node)\n",
    "            tx.create(r)\n",
    "            \n",
    "        # Create relationships between each (entity_1, entity_2) in the\n",
    "        # list of relations for this document.\n",
    "        for rel in relations:\n",
    "            start = rel['start']\n",
    "            end = rel['end']\n",
    "            \n",
    "            phrase_1 = mentions[start]['phrase']\n",
    "            entity_class_1 = mentions[start]['labels'][0]\n",
    "            \n",
    "            phrase_2 = mentions[end]['phrase']\n",
    "            entity_class_2 = mentions[end]['labels'][0]\n",
    "                       \n",
    "            node_1 = created_entity_nodes[get_node_id(phrase_1, entity_class_1)]\n",
    "            node_2 = created_entity_nodes[get_node_id(phrase_2, entity_class_2)]\n",
    "            \n",
    "            r = Relationship(node_1, rel['type'], node_2)\n",
    "            tx.create(r)\n",
    "    tx.commit()\n",
    "\n",
    "create_graph(tagged_sents)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Querying the graph\n",
    "\n",
    "\n",
    "Now that the graph has been created, we can query it in Neo4j. For example, you could run the following query to find out all activities performed on engines:\n",
    "\n",
    "    MATCH (e:Entity {name: \"engine\"})-[r:HAS_ACTIVITY]->(a:Activity)\n",
    "    RETURN e, r, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run through some more examples in Neo4j during the workshop.\n",
    "\n",
    "Note we can also query over structured data such as dates and costs. We will not be demonstrating this in the interest of time, but feel free to attempt the query yourself! Hint: The costs and dates are stored on the `Document` nodes, which are connected to the entities appearing in those documents via the `APPEARS_IN` relation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgment\n",
    "\n",
    "This work is supported by the Australian Research Council through the Centre for Transforming Maintenance through Data Science  (grant number IC180100030), funded by the Australian Government.\n",
    "\n",
    "We would like to thank Tyler Bikaun, Melinda Hodkiewicz and Wei Liu for their contributions to this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Baldwin, Timothy, Marie-Catherine De Marneffe, Bo Han, Young-Bum Kim, Alan Ritter, and Wei Xu. \"Shared tasks of the 2015 workshop on noisy user-generated text: Twitter lexical normalization and named entity recognition.\" In Proceedings of the Workshop on Noisy User-generated Text, pp. 126-135. 2015.\n",
    "- Cabot, Pere-Lluís Huguet, and Roberto Navigli. \"REBEL: Relation extraction by end-to-end language generation.\" In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 2370-2381. 2021.\n",
    "- Eberts, Markus, and Adrian Ulges. \"Span-based joint entity and relation extraction with transformer pre-training.\" arXiv preprint arXiv:1909.07755 (2019).\n",
    "- Han, Bo, and Timothy Baldwin. \"Lexical normalisation of short text messages: Makn sens a# twitter.\" In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, pp. 368-378. 2011.\n",
    "- Josifoski, Martin, Nicola De Cao, Maxime Peyrard, and Robert West. \"GenIE: generative information extraction.\" arXiv preprint arXiv:2112.08340 (2021).\n",
    "- Lourentzou, Ismini, Kabir Manghnani, and ChengXiang Zhai. \"Adapting sequence to sequence models for text normalization in social media.\" In Proceedings of the international AAAI conference on web and social media, vol. 13, pp. 335-345. 2019.\n",
    "- Muller, Benjamin, Benoît Sagot, and Djamé Seddah. \"Enhancing BERT for lexical normalization.\" In The 5th Workshop on Noisy User-generated Text (W-NUT). 2019.\n",
    "- Nguyen, Hoang, and Sandro Cavallari. \"Neural multi-task text normalization and sanitization with pointer-generator.\" In Proceedings of the First Workshop on Natural Language Interfaces, pp. 37-47. 2020.\n",
    "- Samuel, David, and Milan Straka. 2021. “ÚFAL at MultiLexNorm 2021: Improving Multilingual Lexical Normalization by Fine-Tuning ByT5.” In Proceedings of the Seventh Workshop on Noisy User-Generated Text (W-NUT 2021), 483–92. Stroudsburg, PA, USA: Association for Computational Linguistics.\n",
    "- Stewart, Michael, Wei Liu, Rachel Cardell-Oliver, and Rui Wang. \"Short-text lexical normalisation on industrial log data.\" In 2018 IEEE International Conference on Big Knowledge (ICBK), pp. 113-122. IEEE, 2018.\n",
    "- van der Goot, Rob, Alan Ramponi, Arkaitz Zubiaga, Barbara Plank, Benjamin Muller, Iñaki San Vicente Roncal, Nikola Ljubešić, et al. 2021. “MultiLexNorm: A Shared Task on Multilingual Lexical Normalization.” In Proceedings of the Seventh Workshop on Noisy User-Generated Text (W-NUT 2021), 493–509. Online: Association for Computational Linguistics.\n",
    "- van der Goot, Rob, and Gertjan van Noord. \"Monoise: Modeling noise using a modular normalization system.\" arXiv preprint arXiv:1710.03476 (2017).\n",
    "- van der Goot, Rob, Barbara Plank, and Malvina Nissim. \"To normalize, or not to normalize: The impact of normalization on part-of-speech tagging.\" arXiv preprint arXiv:1707.05116 (2017).\n",
    "- van der Goot, Rob, Rik van Noord, and Gertjan van Noord. 2018. “A Taxonomy for In-Depth Evaluation of Normalization for User Generated Content.” In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). Miyazaki, Japan: European Language Resources Association (ELRA). https://aclanthology.org/L18-1109.\n",
    "- Yamada, Ikuya and Asai, Akari and Shindo, Hiroyuki and Takeda, Hideaki and Matsumoto, Yuji. \"LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention\". Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.\n",
    "- Yan, Hang, Tao Gui, Junqi Dai, Qipeng Guo, Zheng Zhang, and Xipeng Qiu. \"A Unified Generative Framework for Various NER Subtasks.\" In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 5808-5822. 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## GQVis on Jupyter Lab\n",
    "\n",
    "GQVis works out of the box on Jupyter Notebook (which is recommended), but to get it working in Jupyter Lab, you'll need to run the following command prior to starting Jupyter lab:\n",
    "\n",
    "    jupyter labextension install jupyterlab_requirejs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Neo4j in Google Colab\n",
    "\n",
    "Neo4j should work OK on the Google Colab notebook, but I have noticed it randomly uninstalls from time to time. If it does, there will be an obvious error message that appears when you attempt to run any of the cells in Section 8 (building the graph) and Section 9 (querying the graph).\n",
    "\n",
    "To reinstall Neo4j, you can run the following code. After doing this the cells in Section 8 and 9 should work OK again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nj-4.4/bin/neo4j stop\n",
    "\n",
    "# https://gist.github.com/korakot/328aaac51d78e589b4a176228e4bb06f\n",
    "\n",
    "!curl http://dist.neo4j.org/neo4j-community-4.4.0-unix.tar.gz -o neo4j.tar.gz\n",
    "\n",
    "# decompress and rename\n",
    "!tar -xf neo4j.tar.gz  # or --strip-components=1\n",
    "!mv neo4j-community-4.4.0 nj-4.4\n",
    "\n",
    "# disable password, and start server\n",
    "!sed -i '/#dbms.security.auth_enabled/s/^#//g' nj-4.4/conf/neo4j.conf\n",
    "!nj-4.4/bin/neo4j start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are other options such as Neo4j Aura (which allows you to run Neo4j in a cloud environment), but `gqvis` is not yet able to interact with it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
