{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing a Knowledge Graph from Maintenance Work Order Data\n",
    "\n",
    "In this notebook we are going to construct a simple knowledge graph using Python, and run some queries on the graph in Neo4j. We have broken the notebook into several steps:\n",
    "\n",
    "1. Reading in the data\n",
    "2. Cleaning the data via Lexical Normalisation\n",
    "3. Extracting entities via Named Entity Recognition (NER)\n",
    "4. Creating relations between entities via Relation Extraction (RE)\n",
    "5. Combining NER + RE\n",
    "6. Creating the graph\n",
    "7. Querying the graph in Neo4j\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Installing required packages\n",
    "\n",
    "To run this notebook you will need to install the following via pip:\n",
    "\n",
    "- `py2neo`: A library for working with Neo4j in Python.\n",
    "- `gqvis`: Our simple tool for visualising graph queries in Jupyter.\n",
    "- `flair`: A deep learning library for natural language processing. Note this library is quite large (a couple gb I believe). If you don't wish to install this, we have provided non deep-learning based alternatives so you can still follow along.\n",
    "\n",
    "You will also need to have Neo4j installed for the last part of the tutorial. You can download and install Neo4j Desktop [here](https://neo4j.com/).\n",
    "\n",
    "We will be running through the code during the tutorial so there is no need to install anything unless you would also like to try the code out yourself and run some graph queries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install py2neo\n",
    "!pip install gqvis\n",
    "!pip install flair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read in the data\n",
    "\n",
    "Here is a description of the datasets we are working with in this notebook.\n",
    "\n",
    "First of all, the datasets for the NER model:\n",
    "\n",
    "- `ner_dataset/train.txt`: The dataset we will use to *train* the NER model to predict the entities appearing in each work order.\n",
    "- `ner_dataset/dev.txt`: The dataset we will use to *validate* the quality of the model during training.\n",
    "- `ner_dataset/test.txt`: The dataset we will use to *evaluate* the final performance of the NER model after training.\n",
    "\n",
    "We also have three datasets for the Relation Extraction (RE) model:\n",
    "\n",
    "- `re_dataset/train.csv`\n",
    "- `re_dataset/dev.csv`\n",
    "- `re_dataset/test.csv`\n",
    "\n",
    "We are going to be building a knowledge graph on a small sample set of work orders. This will not be seen by the NER or RE models prior to constructing the graph - the idea is to get our models to run *inference* over this dataset to automatically predict the entities, and relationships between the entities, to build a graph.\n",
    "\n",
    "- `sample_work_orders.csv`: A csv file containing a set of work orders.\n",
    "\n",
    "Here is an example of what the first few rows of each dataset look like:\n",
    "\n",
    "![alt text](images/example-data.png \"Example datasets\")\n",
    "\n",
    "We are using the simple `csv` library to read in the data, though this can also be done using `pandas`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Inspecting the data\n",
    "\n",
    "Let's start by inspecting the `sample_work_orders.csv` CSV dataset. This is the dataset we will be building the graph from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('StartDate', '10/07/2005'), ('FLOC', '1234.1.1'), ('ShortText', 'repair cracked hyd tank')])\n",
      "OrderedDict([('StartDate', '14/07/2005'), ('FLOC', '1234.1.2'), ('ShortText', 'engine wont start')])\n",
      "OrderedDict([('StartDate', '17/07/2005'), ('FLOC', '1234.1.3'), ('ShortText', 'a/c blowing hot air')])\n",
      "OrderedDict([('StartDate', '20/07/2005'), ('FLOC', '1234.1.2'), ('ShortText', 'engin u/s')])\n",
      "OrderedDict([('StartDate', '21/07/2005'), ('FLOC', '1234.1.2'), ('ShortText', 'fix engine')])\n",
      "OrderedDict([('StartDate', '22/07/2005'), ('FLOC', '1234.1.4'), ('ShortText', 'pump service')])\n",
      "OrderedDict([('StartDate', '23/07/2005'), ('FLOC', '1234.1.4'), ('ShortText', 'pump leak')])\n",
      "OrderedDict([('StartDate', '24/07/2005'), ('FLOC', '1234.1.4'), ('ShortText', 'fix leak on pump')])\n",
      "OrderedDict([('StartDate', '25/07/2005'), ('FLOC', '1234.1.2'), ('ShortText', 'engine not running')])\n",
      "OrderedDict([('StartDate', '26/07/2005'), ('FLOC', '1234.1.2'), ('ShortText', 'engine has problems starting')])\n"
     ]
    }
   ],
   "source": [
    "from csv import DictReader\n",
    "\n",
    "work_order_file = \"data/sample_work_orders.csv\"\n",
    "\n",
    "# A simple function to read in a csv file and return a list,\n",
    "# where each element in the list is a dictionary of {heading : value}\n",
    "def load_csv(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = DictReader(f)\n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "    return data\n",
    "\n",
    "        \n",
    "work_order_data = load_csv(work_order_file)\n",
    "\n",
    "# Let's have a look at the first 10 rows\n",
    "for row in work_order_data[:10]:\n",
    "    print(row)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Cleaning the data via Lexical Normalisation\n",
    "\n",
    "Before we start extracting entities from the short text, it's a good idea to do some text cleaning, i.e. \"lexical normalisation\". This is important as we would prefer to have a single node for a single concept, e.g. one node for \"engine\" as opposed to two nodes for \"engin\" and \"engine\". We can also take this opportunity to normalise different variations of the same failure mode (\"overheating\", \"blowing hot air\", etc) to a single failure mode \"overheating\".\n",
    "\n",
    "In the interest of time/simplicity we are not going to use a neural model here, but instead we will use a simple lexicon-based normaliser. This model will simply replace a misspelled phrase with its correct form. This is not practical in the real world (as there's no way we could possibly build a lexicon of all possible misspellings) but it is good enough for our small example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" A lexicon-based normaliser. Normalises sentences by replacing any ngrams \n",
    "(sequences of 1 or more words) with their replacement as per a predefined\n",
    "lexicon.\"\"\"\n",
    "\n",
    "import itertools\n",
    "\n",
    "class LexiconNormaliser:\n",
    "    \"\"\" A lexicon-based normaliser.\n",
    "    \n",
    "    Args:\n",
    "        lexicon_file: The filename of the lexicon.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, lexicon_file, max_ngram_size = 3):\n",
    "        \n",
    "        lexicon_data = load_csv(lexicon_file)\n",
    "        self.max_ngram_size = max_ngram_size\n",
    "        \n",
    "        # Convert the loaded csv into a dictionary mapping incorrect form -> correct form\n",
    "        self.lexicon = {}\n",
    "        for row in lexicon_data:\n",
    "            self.lexicon[row[\"key\"]] = row[\"value\"]      \n",
    "    \n",
    "    def normalise(self, sentence: str):\n",
    "        \"\"\" \n",
    "            Normalise the given sentence via the lexicon.\n",
    "            \n",
    "            Args:\n",
    "                sentence(str): The sentence to normalise.\n",
    "            \n",
    "            Returns:\n",
    "                str: The normalised sentence.\n",
    "        \"\"\"\n",
    "        words = sentence.split()\n",
    "        ngrams = self._get_ngrams(words)\n",
    "        \n",
    "        # Reversing ngrams ensures the larger ngrams are normalised first.\n",
    "        for ngram in reversed(ngrams):\n",
    "            if ngram in self.lexicon:\n",
    "                sentence = sentence.replace(ngram, self.lexicon[ngram])\n",
    "        \n",
    "        return sentence\n",
    "    \n",
    "    \n",
    "    def _get_ngrams(self, sentence):        \n",
    "        \"\"\"\n",
    "            Given a sentence, return a list of all combinations of ngrams\n",
    "            up to a certain size.\n",
    "            \n",
    "            Args:\n",
    "                sentence: A list of words, e.g. [\"fix\", \"broken\", \"pump\"].\n",
    "                \n",
    "            Returns:\n",
    "                ngrams: A list of ngrams containing up to max_ngram_size words.\n",
    "                        For example, given the input [\"fix\", \"broken\", \"pump\"],\n",
    "                        return [\"fix\", \"broken\", \"pump\", \"fix broken\", \"broken pump\", \"fix broken pump\"] \n",
    "        \n",
    "        \"\"\"\n",
    "        ngrams = []        \n",
    "        for n in range(self.max_ngram_size):\n",
    "            for c in itertools.combinations(sentence, n + 1):\n",
    "                ngrams.append(\" \".join(c))\n",
    "        return ngrams\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the LexiconNormaliser has been defined, let's run it over all of the ShortText fields in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repair cracked hyd tank\n",
      "repair cracked hydraulic tank\n",
      "\n",
      "engine wont start\n",
      "engine failure to start\n",
      "\n",
      "a/c blowing hot air\n",
      "air conditioner overheating\n",
      "\n",
      "engin u/s\n",
      "engine breakdown\n",
      "\n",
      "fix engine\n",
      "fix engine\n",
      "\n",
      "pump service\n",
      "pump service\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lexicon_file = \"data/lexicon_normalisation.csv\"\n",
    "lexicon_normaliser = LexiconNormaliser(lexicon_file)\n",
    "\n",
    "work_order_data = load_csv(work_order_file)\n",
    "\n",
    "for i, row in enumerate(work_order_data):\n",
    "    before = row['ShortText']    \n",
    "    row['ShortText'] = lexicon_normaliser.normalise(row['ShortText'])\n",
    "    \n",
    "    # Let's print the first 5 to have a look at the difference\n",
    "    if i <= 5:\n",
    "        print(before)\n",
    "        print(row['ShortText'])\n",
    "        print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Named Entity Recognition\n",
    "\n",
    "Our first task is to extract the entities in the short text descriptions and construct nodes from those entities. This is how we are able to unlock the knowledge captured within the short text and combine it with the structured fields.\n",
    "\n",
    "![alt text](images/extracting-entities-v2.png \"Extracting entities\")\n",
    "\n",
    "## 3.1. Loading and inspecting the data\n",
    "\n",
    "Let's start by defining some functions for loading the CONLL-formatted data. The CONLL format is a widely used format for Named Entity Recognition, and looks like this:\n",
    "\n",
    "    Michael B-PER\n",
    "    works O\n",
    "    at O\n",
    "    The B-ORG\n",
    "    University I-ORG\n",
    "    of I-ORG\n",
    "    Western I-ORG\n",
    "    Australia I-ORG\n",
    "    \n",
    "It's a bit tricky to work with it in this format, so we are going to define some functions to parse it into something like this:\n",
    "\n",
    "    { tokens: ['Michael', 'works', 'at', 'The', 'University', 'of', 'Western', 'Australia'],\n",
    "      labels: ['B-PER', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG'] }\n",
    "      \n",
    "Note that many NLP libraries also have this functionality (NLTK for example) - but we will do it in pure Python in the interest of keeping our dependencies minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def to_conll_document(s: str):\n",
    "    \"\"\"Parse a CONLL-formatted document into a dictionary of\n",
    "    tokens and labels.\n",
    "\n",
    "    Args:\n",
    "        s (str): A string, separated by newlines, where each\n",
    "        line is a token, then a space, then a label.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dict of tokens and labels.\n",
    "    \"\"\"\n",
    "    tokens, labels = [], []\n",
    "    for line in s.split(\"\\n\"):\n",
    "        if len(line.strip()) == 0:\n",
    "            continue\n",
    "        token, label = line.split()\n",
    "\n",
    "        tokens.append(token)\n",
    "        labels.append(label)\n",
    "    return {'tokens': tokens, 'labels': labels}\n",
    "\n",
    "\n",
    "def load_conll_dataset(filename: str) -> list:\n",
    "    \"\"\"Load a list of documents from the given CONLL-formatted dataset.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The filename to load from.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of documents, where each document is a dict of tokens and labels.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        docs = f.read().split(\"\\n\\n\")\n",
    "        for d in docs:\n",
    "            if len(d) == 0:\n",
    "                continue\n",
    "            document = to_conll_document(d)\n",
    "            documents.append(document)\n",
    "    print(f\"Loaded {len(documents)} documents from {filename}.\")\n",
    "    return documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the first row of our training dataset to make sure it loads OK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3200 documents from data/ner_dataset\\train.txt.\n",
      "{'tokens': ['ram', 'on', 'cup', 'rod', 'support', 'broken'], 'labels': ['B-Item', 'B-Location', 'B-Item', 'B-Item', 'I-Item', 'B-Observation']}\n"
     ]
    }
   ],
   "source": [
    "NER_DATASET_PATH = \"data/ner_dataset\"\n",
    "train_dataset = load_conll_dataset(os.path.join(NER_DATASET_PATH, 'train.txt'))\n",
    "\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Define an abstract base class for NER Models\n",
    "\n",
    "Seeing as we would like to be able to work with a range of NER models, it's a good idea to create an 'abstract base class' to represent an NER model. This way, we can create classes for our NER models that inherit from this base class. Every model we create must have these three functions:\n",
    "\n",
    "- `train`: Train the model on the datasets in the given path.\n",
    "- `inference`: Run inference over the given sentence.\n",
    "- `load`: Load the model from the given path.\n",
    "\n",
    "If we try to create an NER model that does not have one of these functions, it will raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Abstract base class for the NER Model. \"\"\"\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class NERModel(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, datasets_path: str):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def inference(self, sent: list):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load(self, model_path):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Define our NER models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3.1. Flair-based NER Model\n",
    "\n",
    "In this tutorial we will use [Flair](https://github.com/flairNLP/flair), which simplifies the process of building a deep learning model for a variety of NLP tasks.\n",
    "\n",
    "The code below is a class representing a `FlairNERModel`, which is based on the `NERModel` class above. It has the same three methods, i.e `train()`, `inference()`, and `save()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"A Flair-based Named Entity Recognition model. Learns to predict entity\n",
    "classes via deep learning.\"\"\"\n",
    "\n",
    "\n",
    "# TODO: Tidy up, fix this code as it does not work atm in this notebook\n",
    "\n",
    "\n",
    "import os\n",
    "import flair\n",
    "from flair.data import Corpus, Sentence\n",
    "from flair.datasets import ColumnCorpus\n",
    "from flair.embeddings import (\n",
    "    StackedEmbeddings,\n",
    "    FlairEmbeddings,\n",
    ")\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer\n",
    "from typing import List\n",
    "from flair.visual.training_curves import Plotter\n",
    "import torch\n",
    "\n",
    "\n",
    "HIDDEN_SIZE = 256\n",
    "\n",
    "# Check whether CUDA is available and set the device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    flair.device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    flair.device = torch.device(\"cpu\")\n",
    "print(\"Device:\", flair.device)\n",
    "\n",
    "\n",
    "class FlairNERModel(NERModel):\n",
    "\n",
    "    model_name: str = \"Flair\"\n",
    "\n",
    "    \"\"\"A Flair-based Named Entity Recognition model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FlairNERModel, self).__init__()\n",
    "\n",
    "        self.model = None\n",
    "\n",
    "    def train(self, datasets_path: os.path, trained_model_path: os.path):\n",
    "        \"\"\" Train the Flair model on the given conll datasets.\n",
    "\n",
    "        Args:\n",
    "            datasets_path (os.path): The folder containing the\n",
    "              train, dev and text CONLL-formatted datasets.\n",
    "            trained_model_path (os.path): The folder to save the trained\n",
    "              model to.\n",
    "        \"\"\"\n",
    "\n",
    "        columns = {0: \"text\", 1: \"ner\"}\n",
    "        corpus: Corpus = ColumnCorpus(\n",
    "            datasets_path,\n",
    "            columns,\n",
    "            train_file=\"train.txt\",\n",
    "            dev_file=\"dev.txt\",\n",
    "            test_file=\"test.txt\",\n",
    "        )\n",
    "        label_dict = corpus.make_label_dictionary(label_type=\"ner\")\n",
    "\n",
    "        # Train the sequence tagger\n",
    "        embedding_types = [\n",
    "            FlairEmbeddings(\"mix-forward\"),\n",
    "            FlairEmbeddings(\"mix-backward\"),\n",
    "        ]\n",
    "\n",
    "        embeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "\n",
    "        tagger = SequenceTagger(\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            embeddings=embeddings,\n",
    "            tag_dictionary=label_dict,\n",
    "            tag_type=\"ner\",\n",
    "            use_crf=True,\n",
    "        )\n",
    "\n",
    "        trainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "        sm = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            sm = \"gpu\"\n",
    "        trainer.train(\n",
    "            trained_model_path,\n",
    "            learning_rate=0.1,\n",
    "            mini_batch_size=32,\n",
    "            max_epochs=10,\n",
    "            embeddings_storage_mode=sm,\n",
    "        )\n",
    "\n",
    "        plotter = Plotter()\n",
    "        plotter.plot_weights(os.path.join(trained_model_path, \"weights.txt\"))\n",
    "\n",
    "        self.load(os.path.join(trained_model_path, 'final-model.pt'))\n",
    "\n",
    "    def inference(self, sent: list) -> dict:\n",
    "        \"\"\"Run the inference on a given list of short texts.\n",
    "\n",
    "        Args:\n",
    "            sent (list): The sentence (list of words).\n",
    "\n",
    "        Returns:\n",
    "            dict: The tagged sentence now in the form of {'tokens': [list],\n",
    "                'labels': [list]}.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the model has not yet been trained.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\n",
    "                \"The NER Model has not yet been trained. \"\n",
    "                \"Please train/load this Flair model before proceeding.\"\n",
    "            )\n",
    "        \n",
    "        sentence_obj = Sentence(sentence, use_tokenizer=False)\n",
    "        self.model.predict(sentence_obj)\n",
    "        labels = [\"O\"] * len(sentence)\n",
    "\n",
    "        for entity in sentence_obj.get_spans(\"ner\"):\n",
    "            for i, token in enumerate(entity):\n",
    "                label = entity.get_label(\"ner\").value\n",
    "                prefix = \"B-\" if i == 0 else \"I-\"\n",
    "                \n",
    "                # Token idx starts from 1 in Flair.\n",
    "                labels[token.idx - 1] = prefix + label\n",
    "\n",
    "        return { 'tokens': sent, 'labels': labels }\n",
    "\n",
    "    def load(self, model_path: str):\n",
    "        \"\"\"Load the model from the specified path.\n",
    "\n",
    "        Args:\n",
    "            model_path (os.path): The path to load.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the path does not exist i.e. model not yet trained.\n",
    "        \"\"\"\n",
    "        self.model = SequenceTagger.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. Dictionary-based NER model\n",
    "\n",
    "If you are not able to use the Flair library, here is a simple model you can use to extract the entities, albeit with a much weaker performance. This one scans the training data, builds a mapping between each phrase (one or more tokens in a row) and the most common entity type associated with that phrase, then uses that entity type as the prediction when seeing that token in the test data.\n",
    "\n",
    "The model is super simple, so we won't show the code here, but feel free to have a look under `helpers/DictionaryNERModel.py` if you are interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import DictionaryNERModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Training the model\n",
    "\n",
    "Depending on whether you are using Flair or the DictionaryNERModel, you can run one of the cells below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.4.1. Using Flair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have trained the Flair-based model and have uploaded the model onto Huggingface. The following code will download that model and load the weights, so there is no need for you to train the model yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 13:42:17,887 Reading data from data\\ner_dataset\n",
      "2022-11-23 13:42:17,888 Train: data\\ner_dataset\\train.txt\n",
      "2022-11-23 13:42:17,889 Dev: data\\ner_dataset\\dev.txt\n",
      "2022-11-23 13:42:17,890 Test: data\\ner_dataset\\test.txt\n",
      "2022-11-23 13:42:18,546 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3200it [00:00, 42659.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 13:42:18,626 Dictionary created for label 'ner' with 12 values: Item (seen 4590 times), Activity (seen 1952 times), Observation (seen 1574 times), Location (seen 957 times), Consumable (seen 308 times), Agent (seen 191 times), Specifier (seen 122 times), Cardinality (seen 114 times), Attribute (seen 80 times), Time (seen 70 times), Event (seen 5 times)\n",
      "2022-11-23 13:42:19,003 SequenceTagger predicts: Dictionary with 45 tags: O, S-Item, B-Item, E-Item, I-Item, S-Activity, B-Activity, E-Activity, I-Activity, S-Observation, B-Observation, E-Observation, I-Observation, S-Location, B-Location, E-Location, I-Location, S-Consumable, B-Consumable, E-Consumable, I-Consumable, S-Agent, B-Agent, E-Agent, I-Agent, S-Specifier, B-Specifier, E-Specifier, I-Specifier, S-Cardinality, B-Cardinality, E-Cardinality, I-Cardinality, S-Attribute, B-Attribute, E-Attribute, I-Attribute, S-Time, B-Time, E-Time, I-Time, S-Event, B-Event, E-Event, I-Event\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flair\\trainers\\trainer.py:65: UserWarning: There should be no best model saved at epoch 1 except there is a model from previous trainings in your training folder. All previous best models will be deleted.\n",
      "  \"There should be no best model saved at epoch 1 except there \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 13:42:19,164 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-23 13:42:19,166 Model: \"SequenceTagger(\n",
      "  (embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): FlairEmbeddings(\n",
      "      (lm): LanguageModel(\n",
      "        (drop): Dropout(p=0.25, inplace=False)\n",
      "        (encoder): Embedding(275, 100)\n",
      "        (rnn): LSTM(100, 2048)\n",
      "        (decoder): Linear(in_features=2048, out_features=275, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (list_embedding_1): FlairEmbeddings(\n",
      "      (lm): LanguageModel(\n",
      "        (drop): Dropout(p=0.25, inplace=False)\n",
      "        (encoder): Embedding(275, 100)\n",
      "        (rnn): LSTM(100, 2048)\n",
      "        (decoder): Linear(in_features=2048, out_features=275, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (rnn): LSTM(4096, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=47, bias=True)\n",
      "  (loss_function): ViterbiLoss()\n",
      "  (crf): CRF()\n",
      ")\"\n",
      "2022-11-23 13:42:19,167 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-23 13:42:19,168 Corpus: \"Corpus: 3200 train + 401 dev + 401 test sentences\"\n",
      "2022-11-23 13:42:19,169 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-23 13:42:19,170 Parameters:\n",
      "2022-11-23 13:42:19,171  - learning_rate: \"0.100000\"\n",
      "2022-11-23 13:42:19,172  - mini_batch_size: \"32\"\n",
      "2022-11-23 13:42:19,173  - patience: \"3\"\n",
      "2022-11-23 13:42:19,174  - anneal_factor: \"0.5\"\n",
      "2022-11-23 13:42:19,175  - max_epochs: \"10\"\n",
      "2022-11-23 13:42:19,176  - shuffle: \"True\"\n",
      "2022-11-23 13:42:19,177  - train_with_dev: \"False\"\n",
      "2022-11-23 13:42:19,178  - batch_growth_annealing: \"False\"\n",
      "2022-11-23 13:42:19,179 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-23 13:42:19,179 Model training base path: \"models\\ner_models\\flair\"\n",
      "2022-11-23 13:42:19,180 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-23 13:42:19,181 Device: cuda:0\n",
      "2022-11-23 13:42:19,182 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-23 13:42:19,183 Embeddings storage mode: gpu\n",
      "2022-11-23 13:42:19,184 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-23 13:42:19,997 epoch 1 - iter 10/100 - loss 3.57767052 - samples/sec: 394.45 - lr: 0.100000\n",
      "2022-11-23 13:42:20,766 epoch 1 - iter 20/100 - loss 3.17685245 - samples/sec: 417.06 - lr: 0.100000\n",
      "2022-11-23 13:42:21,516 epoch 1 - iter 30/100 - loss 2.93616282 - samples/sec: 427.33 - lr: 0.100000\n",
      "2022-11-23 13:42:22,245 epoch 1 - iter 40/100 - loss 2.75386235 - samples/sec: 440.39 - lr: 0.100000\n",
      "2022-11-23 13:42:22,961 epoch 1 - iter 50/100 - loss 2.63307402 - samples/sec: 447.84 - lr: 0.100000\n",
      "2022-11-23 13:42:23,809 epoch 1 - iter 60/100 - loss 2.54009661 - samples/sec: 378.25 - lr: 0.100000\n",
      "2022-11-23 13:42:24,537 epoch 1 - iter 70/100 - loss 2.42750138 - samples/sec: 440.76 - lr: 0.100000\n",
      "2022-11-23 13:42:25,269 epoch 1 - iter 80/100 - loss 2.33842356 - samples/sec: 438.96 - lr: 0.100000\n",
      "2022-11-23 13:42:26,002 epoch 1 - iter 90/100 - loss 2.25260952 - samples/sec: 438.36 - lr: 0.100000\n",
      "2022-11-23 13:42:26,726 epoch 1 - iter 100/100 - loss 2.18480350 - samples/sec: 443.82 - lr: 0.100000\n",
      "2022-11-23 13:42:26,727 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-23 13:42:26,727 EPOCH 1 done: loss 2.1848 - lr 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [00:01<00:00, 12.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 13:42:27,809 Evaluating as a multi-label problem: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 13:42:27,822 DEV : loss 1.2472093105316162 - f1-score (micro avg)  0.6347\n",
      "2022-11-23 13:42:27,830 BAD EPOCHS (no improvement): 0\n",
      "2022-11-23 13:42:27,832 saving best model\n",
      "2022-11-23 13:42:28,530 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-23 13:42:28,763 epoch 2 - iter 10/100 - loss 1.43692249 - samples/sec: 1397.40 - lr: 0.100000\n",
      "2022-11-23 13:42:29,008 epoch 2 - iter 20/100 - loss 1.37321242 - samples/sec: 1316.87 - lr: 0.100000\n",
      "2022-11-23 13:42:29,236 epoch 2 - iter 30/100 - loss 1.34296771 - samples/sec: 1422.22 - lr: 0.100000\n",
      "2022-11-23 13:42:29,472 epoch 2 - iter 40/100 - loss 1.35099978 - samples/sec: 1367.53 - lr: 0.100000\n",
      "2022-11-23 13:42:29,701 epoch 2 - iter 50/100 - loss 1.33625601 - samples/sec: 1403.50 - lr: 0.100000\n",
      "2022-11-23 13:42:29,945 epoch 2 - iter 60/100 - loss 1.32141391 - samples/sec: 1322.32 - lr: 0.100000\n",
      "2022-11-23 13:42:30,185 epoch 2 - iter 70/100 - loss 1.30116132 - samples/sec: 1350.22 - lr: 0.100000\n",
      "2022-11-23 13:42:30,417 epoch 2 - iter 80/100 - loss 1.29030359 - samples/sec: 1397.26 - lr: 0.100000\n",
      "2022-11-23 13:42:30,643 epoch 2 - iter 90/100 - loss 1.27149652 - samples/sec: 1441.43 - lr: 0.100000\n",
      "2022-11-23 13:42:30,879 epoch 2 - iter 100/100 - loss 1.25944806 - samples/sec: 1367.52 - lr: 0.100000\n",
      "2022-11-23 13:42:30,880 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-23 13:42:30,881 EPOCH 2 done: loss 1.2594 - lr 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 30.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 13:42:31,316 Evaluating as a multi-label problem: False\n",
      "2022-11-23 13:42:31,329 DEV : loss 0.8964381814002991 - f1-score (micro avg)  0.7031\n",
      "2022-11-23 13:42:31,337 BAD EPOCHS (no improvement): 0\n",
      "2022-11-23 13:42:31,338 saving best model\n",
      "2022-11-23 13:42:32,056 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-23 13:42:32,291 epoch 3 - iter 10/100 - loss 1.01690784 - samples/sec: 1380.80 - lr: 0.100000\n",
      "2022-11-23 13:42:32,529 epoch 3 - iter 20/100 - loss 1.07365253 - samples/sec: 1362.13 - lr: 0.100000\n",
      "2022-11-23 13:42:32,766 epoch 3 - iter 30/100 - loss 1.08271915 - samples/sec: 1361.60 - lr: 0.100000\n",
      "2022-11-23 13:42:32,993 epoch 3 - iter 40/100 - loss 1.05695919 - samples/sec: 1422.57 - lr: 0.100000\n",
      "2022-11-23 13:42:33,232 epoch 3 - iter 50/100 - loss 1.05866040 - samples/sec: 1350.06 - lr: 0.100000\n",
      "2022-11-23 13:42:33,461 epoch 3 - iter 60/100 - loss 1.04329547 - samples/sec: 1403.36 - lr: 0.100000\n",
      "2022-11-23 13:42:33,699 epoch 3 - iter 70/100 - loss 1.03221356 - samples/sec: 1355.85 - lr: 0.100000\n",
      "2022-11-23 13:42:33,940 epoch 3 - iter 80/100 - loss 1.02628808 - samples/sec: 1338.78 - lr: 0.100000\n",
      "2022-11-23 13:42:34,183 epoch 3 - iter 90/100 - loss 1.01391856 - samples/sec: 1327.50 - lr: 0.100000\n",
      "2022-11-23 13:42:34,415 epoch 3 - iter 100/100 - loss 0.99847857 - samples/sec: 1391.15 - lr: 0.100000\n",
      "2022-11-23 13:42:34,417 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-23 13:42:34,418 EPOCH 3 done: loss 0.9985 - lr 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 30.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 13:42:34,855 Evaluating as a multi-label problem: False\n",
      "2022-11-23 13:42:34,867 DEV : loss 0.7291697859764099 - f1-score (micro avg)  0.7483\n",
      "2022-11-23 13:42:34,875 BAD EPOCHS (no improvement): 0\n",
      "2022-11-23 13:42:34,877 saving best model\n",
      "2022-11-23 13:42:35,584 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-23 13:42:35,817 epoch 4 - iter 10/100 - loss 0.96283753 - samples/sec: 1409.70 - lr: 0.100000\n",
      "2022-11-23 13:42:36,068 epoch 4 - iter 20/100 - loss 0.97040872 - samples/sec: 1292.85 - lr: 0.100000\n",
      "2022-11-23 13:42:36,327 epoch 4 - iter 30/100 - loss 0.94653485 - samples/sec: 1249.95 - lr: 0.100000\n",
      "2022-11-23 13:42:36,555 epoch 4 - iter 40/100 - loss 0.92465933 - samples/sec: 1409.69 - lr: 0.100000\n",
      "2022-11-23 13:42:36,786 epoch 4 - iter 50/100 - loss 0.89689591 - samples/sec: 1403.52 - lr: 0.100000\n",
      "2022-11-23 13:42:37,018 epoch 4 - iter 60/100 - loss 0.88834573 - samples/sec: 1391.30 - lr: 0.100000\n",
      "2022-11-23 13:42:37,260 epoch 4 - iter 70/100 - loss 0.88714399 - samples/sec: 1344.54 - lr: 0.100000\n",
      "2022-11-23 13:42:37,494 epoch 4 - iter 80/100 - loss 0.87960383 - samples/sec: 1373.41 - lr: 0.100000\n",
      "2022-11-23 13:42:37,722 epoch 4 - iter 90/100 - loss 0.87123885 - samples/sec: 1409.69 - lr: 0.100000\n",
      "2022-11-23 13:42:37,957 epoch 4 - iter 100/100 - loss 0.86271531 - samples/sec: 1373.40 - lr: 0.100000\n",
      "2022-11-23 13:42:37,959 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-23 13:42:37,960 EPOCH 4 done: loss 0.8627 - lr 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 30.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 13:42:38,393 Evaluating as a multi-label problem: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 13:42:38,406 DEV : loss 0.6760690212249756 - f1-score (micro avg)  0.769\n",
      "2022-11-23 13:42:38,414 BAD EPOCHS (no improvement): 0\n",
      "2022-11-23 13:42:38,416 saving best model\n",
      "2022-11-23 13:42:39,120 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-23 13:42:39,348 epoch 5 - iter 10/100 - loss 0.76627534 - samples/sec: 1428.57 - lr: 0.100000\n",
      "2022-11-23 13:42:39,596 epoch 5 - iter 20/100 - loss 0.78489846 - samples/sec: 1300.82 - lr: 0.100000\n",
      "2022-11-23 13:42:39,838 epoch 5 - iter 30/100 - loss 0.79016919 - samples/sec: 1333.33 - lr: 0.100000\n",
      "2022-11-23 13:42:40,081 epoch 5 - iter 40/100 - loss 0.79636929 - samples/sec: 1322.32 - lr: 0.100000\n",
      "2022-11-23 13:42:40,315 epoch 5 - iter 50/100 - loss 0.78293260 - samples/sec: 1373.39 - lr: 0.100000\n",
      "2022-11-23 13:42:40,555 epoch 5 - iter 60/100 - loss 0.78318672 - samples/sec: 1338.93 - lr: 0.100000\n",
      "2022-11-23 13:42:40,801 epoch 5 - iter 70/100 - loss 0.77112929 - samples/sec: 1311.48 - lr: 0.100000\n",
      "2022-11-23 13:42:41,030 epoch 5 - iter 80/100 - loss 0.76861754 - samples/sec: 1409.70 - lr: 0.100000\n",
      "2022-11-23 13:42:41,266 epoch 5 - iter 90/100 - loss 0.77178433 - samples/sec: 1373.39 - lr: 0.100000\n",
      "2022-11-23 13:42:41,499 epoch 5 - iter 100/100 - loss 0.76717575 - samples/sec: 1391.32 - lr: 0.100000\n",
      "2022-11-23 13:42:41,501 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-23 13:42:41,502 EPOCH 5 done: loss 0.7672 - lr 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 29.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 13:42:41,947 Evaluating as a multi-label problem: False\n",
      "2022-11-23 13:42:41,959 DEV : loss 0.6060569286346436 - f1-score (micro avg)  0.7787\n",
      "2022-11-23 13:42:41,968 BAD EPOCHS (no improvement): 0\n",
      "2022-11-23 13:42:41,970 saving best model\n",
      "2022-11-23 13:42:42,685 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-23 13:42:42,917 epoch 6 - iter 10/100 - loss 0.70583337 - samples/sec: 1403.53 - lr: 0.100000\n",
      "2022-11-23 13:42:43,148 epoch 6 - iter 20/100 - loss 0.75519577 - samples/sec: 1403.52 - lr: 0.100000\n",
      "2022-11-23 13:42:43,389 epoch 6 - iter 30/100 - loss 0.74832640 - samples/sec: 1333.33 - lr: 0.100000\n",
      "2022-11-23 13:42:43,635 epoch 6 - iter 40/100 - loss 0.73161762 - samples/sec: 1311.48 - lr: 0.100000\n",
      "2022-11-23 13:42:43,875 epoch 6 - iter 50/100 - loss 0.71630152 - samples/sec: 1338.92 - lr: 0.100000\n",
      "2022-11-23 13:42:44,104 epoch 6 - iter 60/100 - loss 0.71072069 - samples/sec: 1415.94 - lr: 0.100000\n",
      "2022-11-23 13:42:44,341 epoch 6 - iter 70/100 - loss 0.70566698 - samples/sec: 1361.71 - lr: 0.100000\n",
      "2022-11-23 13:42:44,576 epoch 6 - iter 80/100 - loss 0.70641259 - samples/sec: 1367.52 - lr: 0.100000\n",
      "2022-11-23 13:42:44,815 epoch 6 - iter 90/100 - loss 0.71409769 - samples/sec: 1350.22 - lr: 0.100000\n",
      "2022-11-23 13:42:45,043 epoch 6 - iter 100/100 - loss 0.70985446 - samples/sec: 1409.70 - lr: 0.100000\n",
      "2022-11-23 13:42:45,045 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-23 13:42:45,046 EPOCH 6 done: loss 0.7099 - lr 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 30.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 13:42:45,486 Evaluating as a multi-label problem: False\n",
      "2022-11-23 13:42:45,499 DEV : loss 0.6382891535758972 - f1-score (micro avg)  0.7716\n",
      "2022-11-23 13:42:45,507 BAD EPOCHS (no improvement): 1\n",
      "2022-11-23 13:42:45,509 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-23 13:42:45,745 epoch 7 - iter 10/100 - loss 0.61624884 - samples/sec: 1373.39 - lr: 0.100000\n",
      "2022-11-23 13:42:45,980 epoch 7 - iter 20/100 - loss 0.64597210 - samples/sec: 1367.52 - lr: 0.100000\n",
      "2022-11-23 13:42:46,213 epoch 7 - iter 30/100 - loss 0.61691749 - samples/sec: 1385.29 - lr: 0.100000\n",
      "2022-11-23 13:42:46,448 epoch 7 - iter 40/100 - loss 0.63591151 - samples/sec: 1373.40 - lr: 0.100000\n",
      "2022-11-23 13:42:46,673 epoch 7 - iter 50/100 - loss 0.64754442 - samples/sec: 1434.98 - lr: 0.100000\n",
      "2022-11-23 13:42:46,915 epoch 7 - iter 60/100 - loss 0.65196755 - samples/sec: 1327.81 - lr: 0.100000\n",
      "2022-11-23 13:42:47,148 epoch 7 - iter 70/100 - loss 0.64992952 - samples/sec: 1379.31 - lr: 0.100000\n",
      "2022-11-23 13:42:47,381 epoch 7 - iter 80/100 - loss 0.64862262 - samples/sec: 1379.31 - lr: 0.100000\n",
      "2022-11-23 13:42:47,611 epoch 7 - iter 90/100 - loss 0.65299037 - samples/sec: 1422.09 - lr: 0.100000\n",
      "2022-11-23 13:42:47,851 epoch 7 - iter 100/100 - loss 0.64628886 - samples/sec: 1343.32 - lr: 0.100000\n",
      "2022-11-23 13:42:47,853 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-23 13:42:47,853 EPOCH 7 done: loss 0.6463 - lr 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 29.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 13:42:48,447 Evaluating as a multi-label problem: False\n",
      "2022-11-23 13:42:48,459 DEV : loss 0.6019043326377869 - f1-score (micro avg)  0.78\n",
      "2022-11-23 13:42:48,467 BAD EPOCHS (no improvement): 0\n",
      "2022-11-23 13:42:48,469 saving best model\n",
      "2022-11-23 13:42:49,180 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-23 13:42:49,418 epoch 8 - iter 10/100 - loss 0.61074184 - samples/sec: 1361.76 - lr: 0.100000\n",
      "2022-11-23 13:42:49,649 epoch 8 - iter 20/100 - loss 0.63358930 - samples/sec: 1391.32 - lr: 0.100000\n",
      "2022-11-23 13:42:49,876 epoch 8 - iter 30/100 - loss 0.63715997 - samples/sec: 1415.95 - lr: 0.100000\n",
      "2022-11-23 13:42:50,119 epoch 8 - iter 40/100 - loss 0.61378976 - samples/sec: 1324.63 - lr: 0.100000\n",
      "2022-11-23 13:42:50,369 epoch 8 - iter 50/100 - loss 0.62808950 - samples/sec: 1297.53 - lr: 0.100000\n",
      "2022-11-23 13:42:50,602 epoch 8 - iter 60/100 - loss 0.62822052 - samples/sec: 1385.29 - lr: 0.100000\n",
      "2022-11-23 13:42:50,847 epoch 8 - iter 70/100 - loss 0.61723402 - samples/sec: 1317.92 - lr: 0.100000\n",
      "2022-11-23 13:42:51,103 epoch 8 - iter 80/100 - loss 0.61103654 - samples/sec: 1259.97 - lr: 0.100000\n",
      "2022-11-23 13:42:51,338 epoch 8 - iter 90/100 - loss 0.61181627 - samples/sec: 1379.33 - lr: 0.100000\n",
      "2022-11-23 13:42:51,585 epoch 8 - iter 100/100 - loss 0.61656977 - samples/sec: 1311.50 - lr: 0.100000\n",
      "2022-11-23 13:42:51,586 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-23 13:42:51,588 EPOCH 8 done: loss 0.6166 - lr 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 29.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 13:42:52,030 Evaluating as a multi-label problem: False\n",
      "2022-11-23 13:42:52,043 DEV : loss 0.5799474716186523 - f1-score (micro avg)  0.7824\n",
      "2022-11-23 13:42:52,050 BAD EPOCHS (no improvement): 0\n",
      "2022-11-23 13:42:52,052 saving best model\n",
      "2022-11-23 13:42:52,762 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-23 13:42:52,998 epoch 9 - iter 10/100 - loss 0.58800616 - samples/sec: 1381.63 - lr: 0.100000\n",
      "2022-11-23 13:42:53,256 epoch 9 - iter 20/100 - loss 0.59572057 - samples/sec: 1249.99 - lr: 0.100000\n",
      "2022-11-23 13:42:53,486 epoch 9 - iter 30/100 - loss 0.60261734 - samples/sec: 1409.87 - lr: 0.100000\n",
      "2022-11-23 13:42:53,732 epoch 9 - iter 40/100 - loss 0.59319719 - samples/sec: 1315.78 - lr: 0.100000\n",
      "2022-11-23 13:42:53,968 epoch 9 - iter 50/100 - loss 0.58632868 - samples/sec: 1372.56 - lr: 0.100000\n",
      "2022-11-23 13:42:54,216 epoch 9 - iter 60/100 - loss 0.58270096 - samples/sec: 1301.32 - lr: 0.100000\n",
      "2022-11-23 13:42:54,461 epoch 9 - iter 70/100 - loss 0.57930092 - samples/sec: 1327.76 - lr: 0.100000\n",
      "2022-11-23 13:42:54,698 epoch 9 - iter 80/100 - loss 0.58536733 - samples/sec: 1362.11 - lr: 0.100000\n",
      "2022-11-23 13:42:54,930 epoch 9 - iter 90/100 - loss 0.58575304 - samples/sec: 1397.32 - lr: 0.100000\n",
      "2022-11-23 13:42:55,166 epoch 9 - iter 100/100 - loss 0.58562331 - samples/sec: 1373.53 - lr: 0.100000\n",
      "2022-11-23 13:42:55,168 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-23 13:42:55,169 EPOCH 9 done: loss 0.5856 - lr 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 29.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 13:42:55,618 Evaluating as a multi-label problem: False\n",
      "2022-11-23 13:42:55,630 DEV : loss 0.5421388149261475 - f1-score (micro avg)  0.8009\n",
      "2022-11-23 13:42:55,638 BAD EPOCHS (no improvement): 0\n",
      "2022-11-23 13:42:55,640 saving best model\n",
      "2022-11-23 13:42:56,354 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-23 13:42:56,601 epoch 10 - iter 10/100 - loss 0.57466440 - samples/sec: 1316.80 - lr: 0.100000\n",
      "2022-11-23 13:42:56,830 epoch 10 - iter 20/100 - loss 0.58684094 - samples/sec: 1421.82 - lr: 0.100000\n",
      "2022-11-23 13:42:57,070 epoch 10 - iter 30/100 - loss 0.56883227 - samples/sec: 1350.36 - lr: 0.100000\n",
      "2022-11-23 13:42:57,319 epoch 10 - iter 40/100 - loss 0.55401358 - samples/sec: 1306.18 - lr: 0.100000\n",
      "2022-11-23 13:42:57,560 epoch 10 - iter 50/100 - loss 0.54733924 - samples/sec: 1349.59 - lr: 0.100000\n",
      "2022-11-23 13:42:57,812 epoch 10 - iter 60/100 - loss 0.55085324 - samples/sec: 1285.07 - lr: 0.100000\n",
      "2022-11-23 13:42:58,051 epoch 10 - iter 70/100 - loss 0.55724558 - samples/sec: 1350.88 - lr: 0.100000\n",
      "2022-11-23 13:42:58,302 epoch 10 - iter 80/100 - loss 0.55054992 - samples/sec: 1285.15 - lr: 0.100000\n",
      "2022-11-23 13:42:58,547 epoch 10 - iter 90/100 - loss 0.55533422 - samples/sec: 1311.73 - lr: 0.100000\n",
      "2022-11-23 13:42:58,781 epoch 10 - iter 100/100 - loss 0.55548531 - samples/sec: 1379.05 - lr: 0.100000\n",
      "2022-11-23 13:42:58,783 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-23 13:42:58,784 EPOCH 10 done: loss 0.5555 - lr 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 30.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 13:42:59,224 Evaluating as a multi-label problem: False\n",
      "2022-11-23 13:42:59,236 DEV : loss 0.5560851693153381 - f1-score (micro avg)  0.8116\n",
      "2022-11-23 13:42:59,243 BAD EPOCHS (no improvement): 0\n",
      "2022-11-23 13:42:59,245 saving best model\n",
      "2022-11-23 13:43:00,707 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-23 13:43:00,709 loading file models\\ner_models\\flair\\best-model.pt\n",
      "2022-11-23 13:43:01,113 SequenceTagger predicts: Dictionary with 47 tags: O, S-Item, B-Item, E-Item, I-Item, S-Activity, B-Activity, E-Activity, I-Activity, S-Observation, B-Observation, E-Observation, I-Observation, S-Location, B-Location, E-Location, I-Location, S-Consumable, B-Consumable, E-Consumable, I-Consumable, S-Agent, B-Agent, E-Agent, I-Agent, S-Specifier, B-Specifier, E-Specifier, I-Specifier, S-Cardinality, B-Cardinality, E-Cardinality, I-Cardinality, S-Attribute, B-Attribute, E-Attribute, I-Attribute, S-Time, B-Time, E-Time, I-Time, S-Event, B-Event, E-Event, I-Event, <START>, <STOP>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [00:01<00:00, 12.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-23 13:43:02,369 Evaluating as a multi-label problem: False\n",
      "2022-11-23 13:43:02,381 0.7524\t0.8009\t0.7759\t0.6451\n",
      "2022-11-23 13:43:02,383 \n",
      "Results:\n",
      "- F-score (micro) 0.7759\n",
      "- F-score (macro) 0.78\n",
      "- Accuracy 0.6451\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Item     0.6919    0.7828    0.7346       548\n",
      " Observation     0.7445    0.7578    0.7511       223\n",
      "    Activity     0.8744    0.8704    0.8724       216\n",
      "    Location     0.8014    0.8014    0.8014       141\n",
      "  Consumable     0.8444    0.8085    0.8261        47\n",
      "       Agent     0.8095    1.0000    0.8947        34\n",
      " Cardinality     0.8500    0.9444    0.8947        18\n",
      "        Time     1.0000    0.9474    0.9730        19\n",
      "   Attribute     0.1429    0.1429    0.1429        14\n",
      "   Specifier     0.9091    0.9091    0.9091        11\n",
      "\n",
      "   micro avg     0.7524    0.8009    0.7759      1271\n",
      "   macro avg     0.7668    0.7965    0.7800      1271\n",
      "weighted avg     0.7558    0.8009    0.7768      1271\n",
      "\n",
      "2022-11-23 13:43:02,383 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-23 13:43:02,575 Weights plots are saved in models\\ner_models\\flair\\weights.png\n",
      "2022-11-23 13:43:02,577 loading file models/ner_models/flair\\final-model.pt\n",
      "2022-11-23 13:43:02,973 SequenceTagger predicts: Dictionary with 47 tags: O, S-Item, B-Item, E-Item, I-Item, S-Activity, B-Activity, E-Activity, I-Activity, S-Observation, B-Observation, E-Observation, I-Observation, S-Location, B-Location, E-Location, I-Location, S-Consumable, B-Consumable, E-Consumable, I-Consumable, S-Agent, B-Agent, E-Agent, I-Agent, S-Specifier, B-Specifier, E-Specifier, I-Specifier, S-Cardinality, B-Cardinality, E-Cardinality, I-Cardinality, S-Attribute, B-Attribute, E-Attribute, I-Attribute, S-Time, B-Time, E-Time, I-Time, S-Event, B-Event, E-Event, I-Event, <START>, <STOP>\n"
     ]
    }
   ],
   "source": [
    "flair_ner_model = FlairNERModel()\n",
    "flair_ner_model.train(NER_DATASET_PATH, 'models/ner_models/flair') # Uncomment to train manually\n",
    "#m.load(\"nlp-tlp/mwo-ner-test\") # TODO: Replace with load_pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2. Using the DictionaryNERModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n",
      "Loaded 3200 documents from data/ner_dataset\\train.txt.\n",
      "Loaded 401 documents from data/ner_dataset\\dev.txt.\n"
     ]
    }
   ],
   "source": [
    "from helpers import DictionaryNERModel\n",
    "\n",
    "NER_DATASET_PATH = \"data/ner_dataset\"\n",
    "\n",
    "dictionary_ner_model = DictionaryNERModel()\n",
    "dictionary_ner_model.train(NER_DATASET_PATH, 'models/ner_models/dictionary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Running inference on unseen sentences\n",
    "\n",
    "The next step is to use our trained model to infer the entity type of each entity appearing in a list of previously unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['air', 'conditioner', 'breakdown'], 'labels': ['B-Item', 'I-Item', 'B-Observation']}\n"
     ]
    }
   ],
   "source": [
    "tagged_bio_sents = []\n",
    "\n",
    "sentences = []\n",
    "for row in work_order_data:\n",
    "    sentence = row[\"ShortText\"].split() # We must 'tokenise' the sentence first, i.e. split into words\n",
    "    tagged_sent = flair_ner_model.inference(sentence) # replace 'flair' with 'dictionary' if not using flair  \n",
    "    tagged_bio_sents.append(tagged_sent)\n",
    "\n",
    "# Print an example tagged sentence\n",
    "print(tagged_bio_sents[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Extracting relations between the entities via Relation Extraction\n",
    "\n",
    "We have extracted the entities appearing in each work order. The next step is to extract the relationships between those entities. We can do this using Relation Extraction.\n",
    "\n",
    "![alt text](images/building-relations.png \"Building relations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Loading and inspecting the data\n",
    "\n",
    "Let's take a look again at the RE dataset we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['broken', 'rod support', 'Observation', 'Item', 'rod support broken', '0', '1', 'O']\n",
      "['rod support', 'broken', 'Item', 'Observation', 'rod support broken', '1', '0', 'HAS_OBSERVATION']\n",
      "['broken', 'cup', 'Observation', 'Item', 'cup rod support broken', '0', '2', 'O']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "RE_DATASET_PATH = \"data/re_dataset\"\n",
    "\n",
    "\n",
    "def load_re_dataset(filename: str) -> list:\n",
    "    \"\"\"Load the Relation Extraction dataset into a list.\n",
    "        \n",
    "    Args:\n",
    "        filename (str): The name of the file to load.\n",
    "    \"\"\"\n",
    "    re_data = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for row in f:\n",
    "            re_data.append(row.strip().split(','))\n",
    "    return re_data\n",
    "\n",
    "train_dataset = load_re_dataset(os.path.join(RE_DATASET_PATH, 'train.csv'))\n",
    "\n",
    "# Let's take a quick look just to make sure it loads as expected...\n",
    "for row in train_dataset[:3]:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interpret this as follows:\n",
    " - 'broken': entity 1\n",
    " - 'rod support': entity 2\n",
    " - 'Observation': label of entity 1\n",
    " - 'Item': label of entity 2\n",
    " - 'rod support broken': The text between 'broken' and 'rod support', inclusive\n",
    " - '0': The mention index of entity 1\n",
    " - '1': The mention index of entity 2\n",
    " - 'O': The relation type. \"O\" means no relation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Define the Abstract Base Class\n",
    "\n",
    "We are going to see two different RE models, so let's define an abstract base class again just like we did for the NER models. Just like the NER model, we have three functions:\n",
    "\n",
    "- `inference`: Given a row (as above, but without the last column), predict the given relation type (\"O\" if no relation).\n",
    "- `train`: Train the model on the files in the given dataset path.\n",
    "- `load`: Load the model from the specified path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class REModel(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def inference(self, row: list) -> str:\n",
    "        pass        \n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, re_datasets_path: str):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load(self, model_path: str):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Define our RE model(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1. Flair-based RE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "\"\"\" A Flair-based relation extraction model.\n",
    "This one uses Flair's TextClassifier model to classify the\n",
    "relation type of a given row.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "import flair\n",
    "from flair.trainers import ModelTrainer\n",
    "from flair.datasets import CSVClassificationCorpus\n",
    "from flair.embeddings import (\n",
    "    PooledFlairEmbeddings,\n",
    "    DocumentRNNEmbeddings,\n",
    ")\n",
    "from flair.data import Sentence\n",
    "from typing import List\n",
    "from flair.models import TextClassifier\n",
    "from flair.visual.training_curves import Plotter\n",
    "\n",
    "import torch\n",
    "\n",
    "MAX_EPOCHS = 1\n",
    "HIDDEN_SIZE = 256\n",
    "\n",
    "# Check whether CUDA is available and set the device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    flair.device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    flair.device = torch.device(\"cpu\")\n",
    "print(\"Device:\", flair.device)\n",
    "\n",
    "\n",
    "class FlairREModel(REModel):\n",
    "\n",
    "    \"\"\"The Flair-based RE model.\"\"\"\n",
    "\n",
    "    model_name: str = \"Flair\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FlairREModel, self).__init__()\n",
    "        self.model = None\n",
    "\n",
    "    def train(self, datasets_path: os.path, trained_model_path: os.path):\n",
    "        \"\"\"Train the Flair RE model on the given CSV datasets.\n",
    "\n",
    "        Args:\n",
    "            datasets_path (os.path): The path containing the train and dev\n",
    "               datasets.\n",
    "            trained_model_path (os.path): The path to save the trained model.\n",
    "        \"\"\"\n",
    "\n",
    "        column_name_map = {\n",
    "            0: \"text\",\n",
    "            1: \"text\",\n",
    "            2: \"text\",\n",
    "            3: \"text\",\n",
    "            4: \"text\",\n",
    "            7: \"label_relation\",\n",
    "        }\n",
    "\n",
    "        # Define corpus, labels, word embeddings, doc embeddings\n",
    "        corpus = CSVClassificationCorpus(\n",
    "            datasets_path,\n",
    "            column_name_map,\n",
    "            delimiter=\",\",\n",
    "            label_type=\"relation\",\n",
    "        )\n",
    "\n",
    "        label_dict = corpus.make_label_dictionary(label_type=\"relation\")\n",
    "\n",
    "        word_embeddings = [\n",
    "            PooledFlairEmbeddings(\"mix-forward\"),\n",
    "            PooledFlairEmbeddings(\"mix-backward\"),\n",
    "        ]\n",
    "\n",
    "        document_embeddings = DocumentRNNEmbeddings(\n",
    "            word_embeddings, hidden_size=HIDDEN_SIZE\n",
    "        )\n",
    "\n",
    "        # Initialise sequence tagger\n",
    "        tagger = TextClassifier(\n",
    "            document_embeddings,\n",
    "            label_dictionary=label_dict,\n",
    "            label_type=\"relation\",\n",
    "        )\n",
    "\n",
    "        # Initialize trainer\n",
    "        trainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "        sm = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            sm = \"gpu\"\n",
    "        \n",
    "        # Start training\n",
    "        trainer.train(\n",
    "            trained_model_path,\n",
    "            learning_rate=0.1,\n",
    "            mini_batch_size=32,\n",
    "            max_epochs=MAX_EPOCHS,\n",
    "            patience=3,\n",
    "            embeddings_storage_mode=sm,\n",
    "        )\n",
    "\n",
    "        self.load(os.path.join(trained_model_path, 'final-model.pt'))\n",
    "\n",
    "    def load(self, model_path: str):\n",
    "        \"\"\"Load the chunked frequency dict from the given folder.\n",
    "\n",
    "        Args:\n",
    "            model_path (str): The filename containing the model.\n",
    "               Can also be the name of a repo on Huggingface.\n",
    "        \"\"\"\n",
    "        TextClassifier.load(model_path)\n",
    "\n",
    "    def inference(self, row: list) -> str:\n",
    "        \"\"\"Run the inference over the given document.\n",
    "\n",
    "        Args:\n",
    "            row (list): The row to predict the relation of.\n",
    "\n",
    "        Returns:\n",
    "            str: The relation type.\n",
    "        \"\"\"\n",
    "        \n",
    "        s = Sentence(\" \".join(rel[:5]))\n",
    "        label = \"O\"\n",
    "        self.model.predict(s)\n",
    "        if len(s.labels) > 0:\n",
    "            label = str(s.labels[0].value)\n",
    "        return label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2. 'SimpleMWO' RE model\n",
    "\n",
    "Because maintenance work orders are very short (5-7 words typically), generally speaking we can create a useful knowledge graph by simply linking each Item entity in the work order and each other entity in that work order. For example:\n",
    "\n",
    "    replace pump\n",
    "    \n",
    "We can say the \"pump\" entity `HAS_ACTIVITY` \"replace\". Likewise for the following:\n",
    "\n",
    "    fix air conditioner , not working\n",
    "    \n",
    "We can say that \"air conditioner\" `HAS_ACTIVITY` \"fix\", and `HAS_OBSERVATION` \"not working\".\n",
    "\n",
    "This is not a foolproof method, though - it is a heuristic, i.e. a rule-based method designed to exploit a pattern in the data. For creating this specific type of knowledge graph, though, it works quite well, and thus we can define a model to use this heuristic as a weaker alternative to a deep learning model.\n",
    "\n",
    "Just like the dictionary-based NER model, the model is super simple, so we won't show the code here, but feel free to have a look under `helpers/SimpleMWOREModel.py` if you are interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import SimpleMWOREModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example output from the model. Note we have set the last column (i.e. the relation type) to `None`, as it is our model's job to predict that column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HAS_OBSERVATION'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = SimpleMWOREModel()\n",
    "\n",
    "r.inference([\n",
    "  \"rod support\",\n",
    "  \"broken\",\n",
    "  \"Item\",\n",
    "  \"Observation\",\n",
    "  \"rod support broken\",\n",
    "  \"1\",\n",
    "  \"0\",\n",
    "  None\n",
    " ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Train the model/load the pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the pretrained Flair RE model from Huggingface.\n",
    "\n",
    "(or alternatively you can train it yourself by uncommenting the train line, and commenting the load line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-21 20:37:32,560 Reading data from data\\re_dataset\n",
      "2022-11-21 20:37:32,562 Train: data\\re_dataset\\train.csv\n",
      "2022-11-21 20:37:32,562 Dev: data\\re_dataset\\dev.csv\n",
      "2022-11-21 20:37:32,563 Test: data\\re_dataset\\test.csv\n",
      "2022-11-21 20:37:32,653 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24804it [00:04, 6156.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-21 20:37:36,688 Dictionary created for label 'relation' with 13 values: O (seen 15398 times), HAS_ACTIVITY (seen 2825 times), HAS_OBSERVATION (seen 2174 times), APPEARS_WITH (seen 1982 times), HAS_LOCATION (seen 1556 times), HAS_CONSUMABLE (seen 334 times), HAS_SPECIFIER (seen 173 times), HAS_AGENT (seen 143 times), HAS_CARDINALITY (seen 114 times), HAS_ATTRIBUTE (seen 76 times), HAS_TIME (seen 25 times), HAS_EVENT (seen 4 times)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flair\\trainers\\trainer.py:65: UserWarning: There should be no best model saved at epoch 1 except there is a model from previous trainings in your training folder. All previous best models will be deleted.\n",
      "  \"There should be no best model saved at epoch 1 except there \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-21 20:37:39,123 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-21 20:37:39,125 Model: \"TextClassifier(\n",
      "  (decoder): Linear(in_features=256, out_features=13, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (locked_dropout): LockedDropout(p=0.0)\n",
      "  (word_dropout): WordDropout(p=0.0)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): PooledFlairEmbeddings(\n",
      "        (context_embeddings): FlairEmbeddings(\n",
      "          (lm): LanguageModel(\n",
      "            (drop): Dropout(p=0.25, inplace=False)\n",
      "            (encoder): Embedding(275, 100)\n",
      "            (rnn): LSTM(100, 2048)\n",
      "            (decoder): Linear(in_features=2048, out_features=275, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_1): PooledFlairEmbeddings(\n",
      "        (context_embeddings): FlairEmbeddings(\n",
      "          (lm): LanguageModel(\n",
      "            (drop): Dropout(p=0.25, inplace=False)\n",
      "            (encoder): Embedding(275, 100)\n",
      "            (rnn): LSTM(100, 2048)\n",
      "            (decoder): Linear(in_features=2048, out_features=275, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=8192, out_features=8192, bias=True)\n",
      "    (rnn): GRU(8192, 256, batch_first=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2022-11-21 20:37:39,126 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-21 20:37:39,127 Corpus: \"Corpus: 24804 train + 3310 dev + 3234 test sentences\"\n",
      "2022-11-21 20:37:39,128 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-21 20:37:39,128 Parameters:\n",
      "2022-11-21 20:37:39,129  - learning_rate: \"0.100000\"\n",
      "2022-11-21 20:37:39,130  - mini_batch_size: \"32\"\n",
      "2022-11-21 20:37:39,131  - patience: \"3\"\n",
      "2022-11-21 20:37:39,132  - anneal_factor: \"0.5\"\n",
      "2022-11-21 20:37:39,133  - max_epochs: \"1\"\n",
      "2022-11-21 20:37:39,134  - shuffle: \"True\"\n",
      "2022-11-21 20:37:39,135  - train_with_dev: \"False\"\n",
      "2022-11-21 20:37:39,135  - batch_growth_annealing: \"False\"\n",
      "2022-11-21 20:37:39,136 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-21 20:37:39,137 Model training base path: \"models\\re_models\\flair\"\n",
      "2022-11-21 20:37:39,138 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-21 20:37:39,139 Device: cuda:0\n",
      "2022-11-21 20:37:39,140 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-21 20:37:39,140 Embeddings storage mode: gpu\n",
      "2022-11-21 20:37:39,141 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-21 20:37:39,143 train mode resetting embeddings\n",
      "2022-11-21 20:37:39,143 train mode resetting embeddings\n",
      "2022-11-21 20:37:53,636 epoch 1 - iter 77/776 - loss 0.05875709 - samples/sec: 174.18 - lr: 0.100000\n",
      "2022-11-21 20:38:09,078 epoch 1 - iter 154/776 - loss 0.04883092 - samples/sec: 164.38 - lr: 0.100000\n",
      "2022-11-21 20:38:24,116 epoch 1 - iter 231/776 - loss 0.04067766 - samples/sec: 168.98 - lr: 0.100000\n",
      "2022-11-21 20:38:39,196 epoch 1 - iter 308/776 - loss 0.03364710 - samples/sec: 167.23 - lr: 0.100000\n",
      "2022-11-21 20:38:54,197 epoch 1 - iter 385/776 - loss 0.02853559 - samples/sec: 169.50 - lr: 0.100000\n",
      "2022-11-21 20:39:09,990 epoch 1 - iter 462/776 - loss 0.02485532 - samples/sec: 159.51 - lr: 0.100000\n",
      "2022-11-21 20:39:24,857 epoch 1 - iter 539/776 - loss 0.02212722 - samples/sec: 171.02 - lr: 0.100000\n",
      "2022-11-21 20:39:39,629 epoch 1 - iter 616/776 - loss 0.02006071 - samples/sec: 170.70 - lr: 0.100000\n",
      "2022-11-21 20:39:55,776 epoch 1 - iter 693/776 - loss 0.01834604 - samples/sec: 155.97 - lr: 0.100000\n",
      "2022-11-21 20:40:10,464 epoch 1 - iter 770/776 - loss 0.01691758 - samples/sec: 171.81 - lr: 0.100000\n",
      "2022-11-21 20:40:11,521 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-21 20:40:11,523 EPOCH 1 done: loss 0.0168 - lr 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 104/104 [00:16<00:00,  6.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-21 20:40:28,143 Evaluating as a multi-label problem: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-21 20:40:28,163 DEV : loss 0.0031255579087883234 - f1-score (micro avg)  0.9164\n",
      "2022-11-21 20:40:28,894 BAD EPOCHS (no improvement): 0\n",
      "2022-11-21 20:40:28,896 saving best model\n",
      "2022-11-21 20:40:31,928 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-21 20:40:31,929 loading file models\\re_models\\flair\\best-model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 102/102 [00:15<00:00,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-21 20:40:49,408 Evaluating as a multi-label problem: False\n",
      "2022-11-21 20:40:49,427 0.9587\t0.8521\t0.9023\t0.9332\n",
      "2022-11-21 20:40:49,428 \n",
      "Results:\n",
      "- F-score (micro) 0.9023\n",
      "- F-score (macro) 0.9294\n",
      "- Accuracy 0.9332\n",
      "\n",
      "By class:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "HAS_OBSERVATION     1.0000    1.0000    1.0000       313\n",
      "   HAS_ACTIVITY     1.0000    1.0000    1.0000       279\n",
      "   HAS_LOCATION     1.0000    1.0000    1.0000       238\n",
      "   APPEARS_WITH     0.5114    0.2064    0.2941       218\n",
      " HAS_CONSUMABLE     1.0000    1.0000    1.0000        59\n",
      "      HAS_AGENT     1.0000    1.0000    1.0000        21\n",
      "  HAS_SPECIFIER     1.0000    1.0000    1.0000        15\n",
      "  HAS_ATTRIBUTE     1.0000    1.0000    1.0000        12\n",
      "HAS_CARDINALITY     1.0000    1.0000    1.0000        10\n",
      "       HAS_TIME     1.0000    1.0000    1.0000         5\n",
      "\n",
      "      micro avg     0.9587    0.8521    0.9023      1170\n",
      "      macro avg     0.9511    0.9206    0.9294      1170\n",
      "   weighted avg     0.9090    0.8521    0.8685      1170\n",
      "\n",
      "2022-11-21 20:40:49,429 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-21 20:40:49,430 loading file models/re_models/flair\\final-model.pt\n"
     ]
    }
   ],
   "source": [
    "re_model = FlairREModel()\n",
    "re_model.train(RE_DATASET_PATH, \"models/re_models/flair\") # Uncomment to train manually\n",
    "\n",
    "# TODO: Load from huggingface\n",
    "# re_model.load('nlp-tlp/mwo-re')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. Inference\n",
    "\n",
    "Now we have our RE model, the next step is to run inference on the MWO dataset to extract the relationships between the entities.\n",
    "\n",
    "We need our data to be in the same format as required by the model, i.e. a list of rows where each row has five columns (entity 1, entity 2, etc), just like the training data used to train the model.\n",
    "\n",
    "So before we can run RE, we need to 'wrangle' our data again to get it into the right format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1. Converting the BIO format to the \"Mention\"-based format\n",
    "\n",
    "The BIO-based format from the NER model has one key downside - it is not good for representing 'phrases' of more than one token in length. This makes it difficult to work with for future steps, such as constructing nodes from the entities and running relation extraction. In light of this, we will now convert the BIO-formatted predictions into Mention format, i.e. go from this:\n",
    "\n",
    "    {'tokens': ['a/c', 'not', 'working'],\n",
    "     'labels': ['B-Item', 'B-Observation', 'I-Observation']}\n",
    "    \n",
    "To this:\n",
    "\n",
    "    {'tokens': ['a/c', 'not', 'working'],\n",
    "     'mentions': [\n",
    "         {'start': 0, 'labels': ['Item'], 'end': 1},\n",
    "         {'start': 1, 'labels': ['Observation'], 'end': 3}]}\n",
    "    \n",
    "Note that this format is also able to now support multiple labels per mention (though we will only be using single labels for simplicity). Researchers use this format for **entity typing**, which is similar to NER but with >= 1 label per mention.\n",
    "\n",
    "This step is just a bit of data wrangling - here we have defined a helper function to convert a BIO-tagged sentence into a Mention-tagged sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"tokens\": [\n",
      "  \"a/c\",\n",
      "  \"not\",\n",
      "  \"working\"\n",
      " ],\n",
      " \"mentions\": [\n",
      "  {\n",
      "   \"start\": 0,\n",
      "   \"labels\": [\n",
      "    \"Item\"\n",
      "   ],\n",
      "   \"end\": 1,\n",
      "   \"phrase\": \"a/c\"\n",
      "  },\n",
      "  {\n",
      "   \"start\": 1,\n",
      "   \"labels\": [\n",
      "    \"Observation\"\n",
      "   ],\n",
      "   \"end\": 3,\n",
      "   \"phrase\": \"not working\"\n",
      "  }\n",
      " ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def bio_to_mention(bio_doc: dict):\n",
    "    \"\"\"Return a Mention-format representation of a BIO-formatted\n",
    "    tagged sentence.\n",
    "\n",
    "    Args:\n",
    "        bio_doc (dict): The BIO doc to convert to the Mention-based doc.\n",
    "\n",
    "    Returns:\n",
    "        dict: A mention-formatted dict created from the bio_doc.\n",
    "    \"\"\"\n",
    "    tokens = bio_doc[\"tokens\"]\n",
    "    labels = bio_doc[\"labels\"]\n",
    "    mentions_list = []\n",
    "\n",
    "    start = 0\n",
    "    end = 0\n",
    "    label = None\n",
    "    for i, (token, label) in enumerate(\n",
    "        zip(tokens, labels)\n",
    "    ):\n",
    "        if label.startswith(\"B-\"):\n",
    "            if len(mentions_list) > 0:\n",
    "                mentions_list[-1][\"end\"] = i\n",
    "            mentions_list.append({\"start\": i, \"labels\": [label[2:]]})\n",
    "        elif label == \"O\" and len(mentions_list) > 0:\n",
    "            mentions_list[-1][\"end\"] = i\n",
    "        if len(mentions_list) == 0:\n",
    "            continue\n",
    "        if i == (len(tokens) - 1) and \"end\" not in mentions_list[-1]:\n",
    "            mentions_list[-1][\"end\"] = i + 1\n",
    "            \n",
    "    for m in mentions_list:\n",
    "        m['phrase'] = \" \".join(tokens[m['start']:m['end']])\n",
    "    return {'tokens': tokens, 'mentions': mentions_list}\n",
    "\n",
    "\n",
    "# For each BIO tagged sentence in tagged_sents, convert it to the mention-based\n",
    "# representation\n",
    "tagged_sents = []\n",
    "for doc in tagged_bio_sents:\n",
    "    mention_doc = bio_to_mention(doc)\n",
    "    tagged_sents.append(mention_doc)\n",
    "\n",
    "# Let's print our example sentence again, this time with the mention-based\n",
    "# representation.\n",
    "# We'll use json.dumps to make it a bit easier to read.\n",
    "print(json.dumps(tagged_sents[12],indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we have added a \"phrase\" to each mention. We technically could get this phrase by looking at the list of tokens from the `start` to the `end` of the mention, but storing it inside `mentions` directly makes things easier later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2. Building a list of potential relations between entities\n",
    "\n",
    "Now we have our data in a more amenable format, but we still need tabular data as required by the RE model. To refresh your memory, this is the required format:\n",
    "\n",
    " - entity 1\n",
    " - entity 2\n",
    " - label of entity 1\n",
    " - label of entity 2\n",
    " - The text between 'broken' and 'rod support', inclusive\n",
    " - The position of entity 1\n",
    " - The position of entity 2\n",
    " - The relation type. \"O\" means no relation.\n",
    " \n",
    "We don't need that last column here as this is what we want our model to predict. We will set it to `None` to denote that no relation has been assigned yet.\n",
    "\n",
    "We also need to add a new column to represent the document index - we will see why later.\n",
    "\n",
    "Here is a helper function to transform our mention-based entity format of a single document into a list of potential relationships between each entity and each other entity in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['repair', 'cracked', 'Activity', 'Observation', 'repair cracked', 0, 1, None, 0]\n"
     ]
    }
   ],
   "source": [
    "def build_potential_relations(tagged_sents) -> list:\n",
    "    \"\"\"Build a list of potential relations, i.e. all possible relationships\n",
    "    between each entity in each document. The 8th column (which denotes the\n",
    "    relationship type) will be set to None. The 9th column is the document index.\n",
    "    \n",
    "    Args:\n",
    "        tagged_sents(list): The list of tagged sentences, where each sentence is a\n",
    "            dict of tokens: [list of tokens] and mentions: [list of mentions].\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of rows, where each row is a potential relationship.\n",
    "    \"\"\"\n",
    "\n",
    "    relations = []\n",
    "    for doc_idx, doc in enumerate(tagged_sents):\n",
    "        for m1_idx, mention_1 in enumerate(doc['mentions']):\n",
    "            entity_1 = \" \".join(doc['tokens'][mention_1['start']: mention_1['end']])\n",
    "            label_1 = mention_1['labels'][0]\n",
    "\n",
    "            for m2_idx, mention_2 in enumerate(doc['mentions']):\n",
    "                if m1_idx == m2_idx:\n",
    "                    continue\n",
    "                entity_2 = \" \".join(doc['tokens'][mention_2['start']: mention_2['end']])\n",
    "                label_2 = mention_2['labels'][0]\n",
    "                mention_text = \" \".join(doc['tokens'][mention_1['start']:mention_2['end']]   )         \n",
    "\n",
    "                relations.append(\n",
    "                    [entity_1, entity_2, label_1, label_2, mention_text, m1_idx, m2_idx, None, doc_idx]         \n",
    "                )\n",
    "    return relations\n",
    "            \n",
    "relations = build_potential_relations(tagged_sents)\n",
    "print(relations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3 Running inference over every row\n",
    "\n",
    "Now that our data is in the same format that we used to train the RE model, we can run the inference on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['repair', 'cracked', 'Activity', 'Observation', 'repair cracked', 0, 1, 'O', 0]\n",
      "['repair', 'hyd', 'Activity', 'Item', 'repair cracked hyd', 0, 2, 'O', 0]\n",
      "['repair', 'tank', 'Activity', 'Item', 'repair cracked hyd tank', 0, 3, 'O', 0]\n",
      "['cracked', 'repair', 'Observation', 'Activity', '', 1, 0, 'O', 0]\n",
      "['cracked', 'hyd', 'Observation', 'Item', 'cracked hyd', 1, 2, 'O', 0]\n",
      "['cracked', 'tank', 'Observation', 'Item', 'cracked hyd tank', 1, 3, 'O', 0]\n",
      "['hyd', 'repair', 'Item', 'Activity', '', 2, 0, 'HAS_ACTIVITY', 0]\n",
      "['hyd', 'cracked', 'Item', 'Observation', '', 2, 1, 'HAS_OBSERVATION', 0]\n",
      "['hyd', 'tank', 'Item', 'Item', 'hyd tank', 2, 3, 'HAS_ITEM', 0]\n",
      "['tank', 'repair', 'Item', 'Activity', '', 3, 0, 'HAS_ACTIVITY', 0]\n"
     ]
    }
   ],
   "source": [
    "def tag_all_relations(relations: list):\n",
    "    \"\"\"Run model inference over every potential relation in the list of\n",
    "    relations.\n",
    "    \n",
    "    Args:\n",
    "        relations(list): The list of (untagged) relations.\n",
    "        \n",
    "    Returns:\n",
    "        tagged_relations(list): The same list, but with the rel_type in the\n",
    "           8th column.\n",
    "    \n",
    "    \"\"\"\n",
    "    tagged_relations = []\n",
    "\n",
    "    for rel in relations:\n",
    "        tagged_rel = rel[:]\n",
    "        rel_type = rel_model.inference(rel)\n",
    "        tagged_rel[7] = rel_type\n",
    "        tagged_relations.append(tagged_rel)\n",
    "    return tagged_relations\n",
    "        \n",
    "rel_model = SimpleMWOREModel() # or FlairREModel()\n",
    "tagged_relations = tag_all_relations(relations)\n",
    "\n",
    "# Print the first 10 rows\n",
    "for row in tagged_relations[:10]:\n",
    "    print(row)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Combining NER+RE\n",
    "\n",
    "Now we have outputs from both the NER model and the RE model. The NER model's output looks like this:\n",
    "\n",
    "    {'tokens': ['a/c', 'not', 'working'],\n",
    "     'mentions': [\n",
    "         {'start': 0, 'labels': ['Item'], 'end': 1},\n",
    "         {'start': 1, 'labels': ['Observation'], 'end': 3}]}         \n",
    "While the RE model's output is shown in the cell above.\n",
    "\n",
    "The next step is to combine the two outputs. Fortunately we stored the document index in the relations, so we can easily join them up.\n",
    "\n",
    "Let's add a 'relations' key to this dictionary. It will capture the relationships between mentions, e.g.\n",
    "\n",
    "    'relations': {'start': 0, 'end': 1, 'type': 'HAS_OBSERVATION'}\n",
    "    \n",
    "... which denotes that mention 0 ('a/c') has the observation of mention 1 ('not working')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"tokens\": [\n",
      "  \"pump\",\n",
      "  \"fault\"\n",
      " ],\n",
      " \"mentions\": [\n",
      "  {\n",
      "   \"start\": 0,\n",
      "   \"labels\": [\n",
      "    \"Item\"\n",
      "   ],\n",
      "   \"end\": 1,\n",
      "   \"phrase\": \"pump\"\n",
      "  },\n",
      "  {\n",
      "   \"start\": 1,\n",
      "   \"labels\": [\n",
      "    \"Observation\"\n",
      "   ],\n",
      "   \"end\": 2,\n",
      "   \"phrase\": \"fault\"\n",
      "  }\n",
      " ],\n",
      " \"relations\": [\n",
      "  {\n",
      "   \"start\": 0,\n",
      "   \"end\": 1,\n",
      "   \"type\": \"HAS_OBSERVATION\"\n",
      "  }\n",
      " ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for i, sent in enumerate(tagged_sents):\n",
    "    \n",
    "    # Note we only care about the relations that do not have the class \"O\".\n",
    "    doc_relations = [row for row in tagged_relations if row[7] != \"O\" and row[8] == i]\n",
    "    \n",
    "    sent['relations'] = []    \n",
    "    for row in doc_relations:\n",
    "        rel = {'start': row[5], 'end': row[6], 'type': row[7]}     \n",
    "        sent['relations'].append(rel)\n",
    "\n",
    "# Let's print an example...\n",
    "print(json.dumps(tagged_sents[10], indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Creating the graph\n",
    "\n",
    "We now have a data structure that stores the tokens, entity mentions, and relationships between those mentions, for each document. The last step is to put it all into a Neo4j graph so that we can query this information.\n",
    "\n",
    "There are two popular methods for doing this:\n",
    "\n",
    "- Using `py2neo` to programatically insert data into Neo4j\n",
    "- Saving CSVs of your entities and relations, then reading them in via a `LOAD CSV` query in Neo4j\n",
    "\n",
    "The first option is simple but a bit slow, and the second option is a little more complex but much faster. We will go with the first option here in this notebook for simplicity.\n",
    "\n",
    "> Before proceeding, make sure you have created a new graph in Neo4j and that your new Neo4j graph is running.\n",
    "\n",
    "You can download and install Neo4j from here if you haven't already: https://neo4j.com/download/. I will be demonstrating the graph during the class so there's no need to have it installed unless you are also interested in trying out some graph queries yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "from py2neo.data import Node, Relationship\n",
    "\n",
    "GRAPH_PASSWORD = \"password\" # Set this to the password of your Neo4J graph\n",
    "\n",
    "\n",
    "def get_node_id(phrase, entity_class):\n",
    "    \"\"\"A simple function to generate an id.\n",
    "    This ensures an entity that can be different classes (pump for example) can have\n",
    "    a unique node for each class type.\n",
    "    \"\"\"\n",
    "    return f\"{phrase}__{entity_class}\"\n",
    "    \n",
    "def create_graph(tagged_sents):\n",
    "    \"\"\"Build the Neo4j graph.\n",
    "    We do this by iterating over each tagged_sentence, and constructing the\n",
    "    graph as follows:\n",
    "     - Create a node to represent the document itself.\n",
    "     - Create nodes for each entity appearing in that document, if they have not\n",
    "       already been created. Each unique combination of entity + class will be added, so\n",
    "       pump (the Item) is different from pump (the Activity).\n",
    "     - Create a relationship between each entity and each document in which it appears.\n",
    "     - Create a relationship between each entity and each other entity it is related to,\n",
    "       via the list of relations.\n",
    "     \n",
    "    Args:\n",
    "        tagged_sents(list): The list of tagged sentences.\n",
    "    \"\"\"\n",
    "    graph = Graph(password = GRAPH_PASSWORD)\n",
    "\n",
    "    # We will start by deleting all nodes and edges in the current graph.\n",
    "    # If we don't do this, we will end up with duplicate nodes and edges when running this script again.\n",
    "    graph.delete_all() \n",
    "\n",
    "    tx = graph.begin()\n",
    "    \n",
    "    # Keep track of the created entity nodes.\n",
    "    # We need a way to map the id of the nodes to the py2neo Node objects so that we can\n",
    "    # easily create relationships between these nodes.\n",
    "    created_entity_nodes = {}\n",
    "    \n",
    "    # Iterate over the list of tagged sentences and programmatically create the graph.\n",
    "    for sent in tagged_sents:\n",
    "        \n",
    "        # Create a node to represent the document.\n",
    "        # Note that if you had additional properties in tagged_sents (such as dates, costs, etc)\n",
    "        # you could add them as properties of the Document nodes here.\n",
    "        document_node = Node(\"Document\", name=\" \".join(sent['tokens']))\n",
    "        tx.create(document_node)\n",
    "        \n",
    "        tokens = sent['tokens']\n",
    "        mentions = sent['mentions']\n",
    "        relations = sent['relations']\n",
    "        \n",
    "        for m in mentions:\n",
    "            start = m['start']\n",
    "            end = m['end']\n",
    "            entity_class = m['labels'][0]        \n",
    "            phrase = \" \".join(tokens[start: end])     \n",
    "                    \n",
    "            # Create a node for this entity mention.\n",
    "            # If the node has already been created (i.e. it exists in created_nodes), \n",
    "            # simply retrieve that Node from created_entity_nodes.\n",
    "            # Otherwise, create it, and add it to created_entity_nodes.\n",
    "            entity_node_id = get_node_id(phrase, entity_class)\n",
    "\n",
    "            if entity_node_id in created_entity_nodes:\n",
    "                entity_node = created_entity_nodes[entity_node_id]\n",
    "            else:\n",
    "                entity_node = Node(\"Entity\", entity_class, _id=entity_node_id, name=phrase)\n",
    "                created_entity_nodes[entity_node_id] = entity_node\n",
    "                tx.create(entity_node)            \n",
    "                        \n",
    "                \n",
    "            # Create a relationship between that node and the document\n",
    "            # in which it appears.               \n",
    "            r = Relationship(entity_node, \"APPEARS_IN\", document_node)\n",
    "            tx.create(r)\n",
    "            \n",
    "        # Create relationships between each (entity_1, entity_2) in the\n",
    "        # list of relations for this document.\n",
    "        for rel in relations:\n",
    "            start = rel['start']\n",
    "            end = rel['end']\n",
    "            \n",
    "            phrase_1 = mentions[start]['phrase']\n",
    "            entity_class_1 = mentions[start]['labels'][0]\n",
    "            \n",
    "            phrase_2 = mentions[end]['phrase']\n",
    "            entity_class_2 = mentions[end]['labels'][0]\n",
    "                       \n",
    "            node_1 = created_entity_nodes[get_node_id(phrase_1, entity_class_1)]\n",
    "            node_2 = created_entity_nodes[get_node_id(phrase_2, entity_class_2)]\n",
    "            \n",
    "            r = Relationship(node_1, rel['type'], node_2)\n",
    "            tx.create(r)\n",
    "    tx.commit()\n",
    "\n",
    "create_graph(tagged_sents)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Querying the graph\n",
    "\n",
    "\n",
    "Now that the graph has been created, we can query it in Neo4j. This section lists some example queries that we can run on our graph. Feel free to try your own queries!\n",
    "\n",
    "Note we are using `gqvis` to visualise these in Jupyter Notebook. The results will look very similar if you run these queries directly in the Neo4j browser.\n",
    "\n",
    "*Note about gqvis: gqvis works out of the box in Jupyter Notebook, but to get it working in Jupyter Lab you'll need to install the jupyter_requirejs plugin. See the Appendix section at the bottom of this notebook for more details.*\n",
    "\n",
    "First, let's try a simple query. Here is a query that searches for __all failure modes observed on pumps__:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "<meta charset=\"utf-8\" />\n",
       "<link\n",
       "  href=\"https://fonts.googleapis.com/css2?family=Open+Sans&display=swap\"\n",
       "  rel=\"stylesheet\"\n",
       "/>\n",
       "<style>\n",
       "  circle {\n",
       "    cursor: pointer;\n",
       "\n",
       "  }\n",
       "  .not-selected {\n",
       "    opacity: 0.2;\n",
       "  }\n",
       "  text {\n",
       "    pointer-events: none;\n",
       "    font-family: \"Open Sans\", sans-serif;\n",
       "    font-size: 12;\n",
       "  }\n",
       "\n",
       "  g#links {\n",
       "    opacity: 0.3;\n",
       "  }\n",
       "\n",
       "  .tooltip {\n",
       "    background: #f5f5f5;\n",
       "\n",
       "    padding: 10px 10px;\n",
       "    font-family: \"Open Sans\", sans-serif;\n",
       "    box-sizing: border-box;\n",
       "    font-size: 0.8em;\n",
       "    position: absolute;\n",
       "    top: 1em;\n",
       "    left: 1em;\n",
       "    opacity: 0;\n",
       "    transition: opacity 0.3s ease;\n",
       "    pointer-events: none;\n",
       "    border: 2px solid #ddd;\n",
       "  }\n",
       "  .tooltip:after {\n",
       "    content: \" \";\n",
       "    position: absolute;\n",
       "    top: calc(50% - 6px);\n",
       "    left: -12px;\n",
       "    width: 0;\n",
       "    height: 0;\n",
       "    border-style: solid;\n",
       "    border-width: 6px 12px 6px 0;\n",
       "    border-color: transparent #f5f5f5 transparent transparent;\n",
       "  }\n",
       "  .tooltip:before {\n",
       "    content: \" \";\n",
       "    position: absolute;\n",
       "    top: calc(50% - 8px);\n",
       "    left: -16px;\n",
       "    width: 0;\n",
       "    height: 0;\n",
       "    border-style: solid;\n",
       "    border-width: 8px 16px 8px 0;\n",
       "    border-color: transparent #ddd transparent transparent;\n",
       "  }\n",
       "  .tooltip-table td {\n",
       "    padding: 0em 0.5em;\n",
       "  }\n",
       "  .tooltip-table tr td:first-child {\n",
       "    font-weight: bold;\n",
       "  }\n",
       "  .tooltip-table tr td:last-child {\n",
       "    text-align: left;\n",
       "  }\n",
       "\n",
       "  .d3-graph-wrapper {\n",
       "    width: 960px;\n",
       "    display: flex;\n",
       "    flex-direction: row;\n",
       "    border: 2px solid #ddd;\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"d3-graph-wrapper\">\n",
       "  <svg id=\"svg-chart\" width=\"960px\" height=\"600\"></svg>\n",
       "  <div id=\"tooltip\" class=\"tooltip\">\n",
       "    <table class=\"tooltip-table\"></table>\n",
       "  </div>\n",
       "</div>\n",
       "<!-- Note d3 is loaded via require.js -->\n",
       "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\"></script>\n",
       "<script>\n",
       "  // This bit is a bit of a pain.\n",
       "  // I wanted a way to run this both directly in the browser (for developing),\n",
       "  // and in Jupyter.\n",
       "  // When developing, I needed some 'dummy data' to populate the graph.\n",
       "  // The following script tag attempts to load the file 'dummyData.js' (which\n",
       "  // contains the dummy data). It will fail when running on jupyter.\n",
       "  //\n",
       "  // Whether or not the dummyData file loads (and dummyData is not null) determines\n",
       "  // whether the CHART_SELECTOR (the id of the svg chart) will be fixed or\n",
       "  // dynamic based on template injection, and also whether the nodes and links are from\n",
       "  // the dummyData or dynamic via template injection.\n",
       "  dummyData = null;\n",
       "  console.log(\n",
       "    \"Attempting to load dummy data. This will fail if running via \" +\n",
       "      \"jupyter notebook, and may throw console errors - this is OK.\"\n",
       "  );\n",
       "</script>\n",
       "<script src=\"dummyData.js\"></script>\n",
       "<script>\n",
       "  require.config({\n",
       "    paths: {\n",
       "      d3: \"https://d3js.org/d3.v7.min\",\n",
       "    },\n",
       "  });\n",
       "\n",
       "  // Everything is wrapped in this require.js function - this way the variables don't\n",
       "  // go into the global scope and everything works even when multiple graphs are\n",
       "  // rendered.\n",
       "  require([\"d3\"], function (d3) {\n",
       "    const CHART_SELECTOR = dummyData ? \"#chart\" : \"#chart-463749\";\n",
       "    const TOOLTIP_SELECTOR = dummyData ? \"#tooltip\" : \"#tooltip-463749\";\n",
       "    const svg_chart = document.getElementById(\"svg-chart\");\n",
       "    const tooltip = document.getElementById(\"tooltip\");\n",
       "\n",
       "    // Maintain the svg element the tooltip is currently bound to.\n",
       "    let tooltipNode = null;\n",
       "    const tooltipTable = tooltip.querySelector(\".tooltip-table\");\n",
       "\n",
       "    // Maintain the currently selected node, if any, as well as all nodes/links\n",
       "    // related to it.\n",
       "    // When not null, it will have the following:\n",
       "    // {\n",
       "    //  baseNodeId: (the index of the base node being selected),\n",
       "    //  connectedNodeIds: (an array of all connected node ids),\n",
       "    //  connectedLinkIds: (an array of all connected link ids),\n",
       "    // };\n",
       "    let selectedNode = null;\n",
       "\n",
       "    // Dynamically set the id of the svg and tooltip.\n",
       "    // If in development mode (i.e. dummyData is not null), the id will just be\n",
       "    // \"#chart\" or \"#tooltip\" respectively.\n",
       "    svg_chart.id = CHART_SELECTOR.substring(1, CHART_SELECTOR.length);\n",
       "    tooltip.id = TOOLTIP_SELECTOR.substring(1, TOOLTIP_SELECTOR.length);\n",
       "    let data = {};\n",
       "    if (dummyData) {\n",
       "      data = dummyData;\n",
       "    } else {\n",
       "      data = {\n",
       "        nodes: JSON.parse(\"[{'id': 244, 'category': 'Entity', 'name': 'pump', '_id': 'pump__Item'}, {'id': 247, 'category': 'Observation', 'name': 'leak', '_id': 'leak__Observation'}, {'id': 257, 'category': 'Observation', 'name': 'fault', '_id': 'fault__Observation'}, {'id': 259, 'category': 'Observation', 'name': 'leaking', '_id': 'leaking__Observation'}]\".replace(/'/g, '\"')),\n",
       "        links: JSON.parse(\"[{'source': 244, 'target': 247, 'type': 'HAS_OBSERVATION'}, {'source': 244, 'target': 257, 'type': 'HAS_OBSERVATION'}, {'source': 244, 'target': 259, 'type': 'HAS_OBSERVATION'}]\".replace(/'/g, '\"')),\n",
       "      };\n",
       "    }\n",
       "\n",
       "    /* Code found in a range of places, spliced together from the following:\n",
       "    https://observablehq.com/@brunolaranjeira/d3-v6-force-directed-graph-with-directional-straight-arrow\n",
       "    https://bl.ocks.org/mbostock/4062045\n",
       "    https://github.com/nlp-tlp/aquila/blob/master/views/visualisations/entity_linking_graph.jade\n",
       "    https://observablehq.com/@harrylove/draw-an-arrowhead-marker-connected-to-a-line-in-d3\n",
       "    */\n",
       "\n",
       "    // https://stackoverflow.com/questions/5560248/programmatically-lighten-or-darken-a-hex-color-or-rgb-and-blend-colors\n",
       "    function lightenDarkenColor(col, amt) {\n",
       "      let usePound = false;\n",
       "      if (col[0] == \"#\") {\n",
       "        col = col.slice(1);\n",
       "        usePound = true;\n",
       "      }\n",
       "\n",
       "      let num = parseInt(col, 16);\n",
       "\n",
       "      let r = (num >> 16) + amt;\n",
       "\n",
       "      if (r > 255) r = 255;\n",
       "      else if (r < 0) r = 0;\n",
       "\n",
       "      let b = ((num >> 8) & 0x00ff) + amt;\n",
       "\n",
       "      if (b > 255) b = 255;\n",
       "      else if (b < 0) b = 0;\n",
       "\n",
       "      let g = (num & 0x0000ff) + amt;\n",
       "\n",
       "      if (g > 255) g = 255;\n",
       "      else if (g < 0) g = 0;\n",
       "\n",
       "      return (usePound ? \"#\" : \"\") + (g | (b << 8) | (r << 16)).toString(16);\n",
       "    }\n",
       "\n",
       "    /**\n",
       "     * Set the tooltip (after hovering over a specific node). The tooltip, which appears\n",
       "     * on the right of the graph, will show the property (key, value) pairs of the\n",
       "     * selected node.\n",
       "     * @param {[type]} d The d3 data point.\n",
       "     * @param {[type]} i The object.\n",
       "     */\n",
       "    function setTooltip(d, i) {\n",
       "      var p = Object.getPrototypeOf(i);\n",
       "      const keys = Object.keys(p);\n",
       "      const vals = keys.map((key) => p[key]);\n",
       "\n",
       "      let table = \"\";\n",
       "      for (var j = 0; j < keys.length; j++) {\n",
       "        table += \"<tr>\";\n",
       "        table += `<td>${keys[j]}:</td>`;\n",
       "        table += `<td>${vals[j]}</td>`;\n",
       "        table += \"</tr>\";\n",
       "      }\n",
       "\n",
       "      tooltipTable.innerHTML = table;\n",
       "      tooltip.style.opacity = \"0.95\";\n",
       "\n",
       "      tooltipNode = d.target;\n",
       "      updateTooltipPosition();\n",
       "\n",
       "      return d;\n",
       "    }\n",
       "\n",
       "    /**\n",
       "     * Update the position (x and y coords) of the tooltip based on the location\n",
       "     * of the element it is bound to.\n",
       "     * @return {[type]} [description]\n",
       "     */\n",
       "    function updateTooltipPosition() {\n",
       "      if (!tooltipNode) return;\n",
       "      const rect = tooltipNode.getBoundingClientRect();\n",
       "      const tooltipRect = tooltip.getBoundingClientRect();\n",
       "      const svgRect = svg_chart.getBoundingClientRect();\n",
       "\n",
       "      tooltip.style.left = `${rect.right + 25 - svgRect.left}px`;\n",
       "      tooltip.style.top = `${\n",
       "        (rect.top + rect.bottom) / 2 - tooltipRect.height / 2 - svgRect.top\n",
       "      }px`;\n",
       "    }\n",
       "\n",
       "    /**\n",
       "     * Clear the tooltip by settings its opacity to 0.\n",
       "     * @return {[type]} [description]\n",
       "     */\n",
       "    function clearTooltip() {\n",
       "      tooltip.style.opacity = \"0\";\n",
       "    }\n",
       "\n",
       "    /**\n",
       "     * Allow the nodes to be moved around when dragged.\n",
       "     * @param  {[type]} simulation [description]\n",
       "     * @return {[type]}            [description]\n",
       "     */\n",
       "    let drag = (simulation) => {\n",
       "      function dragstarted(event) {\n",
       "        if (!event.active) simulation.alphaTarget(0.05).restart();\n",
       "        event.subject.fx = event.subject.x;\n",
       "        event.subject.fy = event.subject.y;\n",
       "      }\n",
       "\n",
       "      function dragged(event) {\n",
       "        event.subject.fx = event.x;\n",
       "        event.subject.fy = event.y;\n",
       "      }\n",
       "\n",
       "      function dragended(event) {\n",
       "        if (!event.active) simulation.alphaTarget(0);\n",
       "        event.subject.fx = null;\n",
       "        event.subject.fy = null;\n",
       "      }\n",
       "\n",
       "      return d3\n",
       "        .drag()\n",
       "        .on(\"start\", dragstarted)\n",
       "        .on(\"drag\", dragged)\n",
       "        .on(\"end\", dragended);\n",
       "    };\n",
       "\n",
       "    const scale = d3.scaleOrdinal(d3.schemeCategory10);\n",
       "\n",
       "    const nodeSize = 40;\n",
       "    let height = 600;\n",
       "    let width = 600;\n",
       "\n",
       "    const links = data.links.map((d) => Object.create(d));\n",
       "    const nodes = data.nodes.map((d) => Object.create(d));\n",
       "\n",
       "    // A list of colours that the colour map will be generated from.\n",
       "    const colours = [\n",
       "      \"#99ffcc\",\n",
       "      \"#ffcccc\",\n",
       "      \"#ccccff\",\n",
       "      \"#ccff99\",\n",
       "      \"#ccffcc\",\n",
       "      \"#ccffff\",\n",
       "      \"#ffcc99\",\n",
       "      \"#ffccff\",\n",
       "      \"#ffff99\",\n",
       "      \"#ffffcc\",\n",
       "      \"#cccc99\",\n",
       "      \"#fbafff\",\n",
       "    ];\n",
       "\n",
       "    /**\n",
       "     * Load the 'colour map' (i.e. mapping of category to colour).\n",
       "     * @param  {Array} nodes The node data.\n",
       "     * @return {Object}       The colour map.\n",
       "     */\n",
       "    function loadColourMap(nodes) {\n",
       "      let colourMap = {};\n",
       "      // Load the colour map\n",
       "      for (let i = 0; i < nodes.length; i++) {\n",
       "        const category = nodes[i].category;\n",
       "        if (!(category in colourMap)) {\n",
       "          colourMap[category] =\n",
       "            colours[Object.keys(colourMap).length % colours.length];\n",
       "        }\n",
       "      }\n",
       "      return colourMap;\n",
       "    }\n",
       "    colourMap = loadColourMap(nodes);\n",
       "    const getColour = (d) => {\n",
       "      return colourMap[d.category];\n",
       "    };\n",
       "\n",
       "    // Initialise some d3 forces etc to make the graph behave properly.\n",
       "    const simulation = d3\n",
       "      .forceSimulation(nodes)\n",
       "      .force(\n",
       "        \"link\",\n",
       "        d3.forceLink(links).id((d) => d.id)\n",
       "      )\n",
       "      // .force(\"charge\", d3.forceManyBody().strength(-200))\n",
       "      .force(\n",
       "        \"collide\",\n",
       "        d3.forceCollide((d) => nodeSize * 1.5)\n",
       "      )\n",
       "      .force(\"center\", d3.forceCenter(width / 2, height / 2));\n",
       "\n",
       "    // Svg: the main container for rendering the graph.\n",
       "    const svg = d3\n",
       "      .select(CHART_SELECTOR)\n",
       "      .attr(\"viewBox\", [0, 0, width, height]);\n",
       "\n",
       "    // svg_g: The 'g' within svg, which seems to make zooming/panning smoother\n",
       "    const svg_g = svg.append(\"g\");\n",
       "    const defs = svg_g.append(\"defs\");\n",
       "\n",
       "    // Marker: a definition for the triangles appearing at the end of each link.\n",
       "    const marker = defs\n",
       "      .selectAll(\"marker\")\n",
       "      .data([\"type_1\"])\n",
       "      .enter()\n",
       "      .append(\"svg:marker\")\n",
       "      .attr(\"id\", function (d, i) {\n",
       "        return `marker_${i}`;\n",
       "      })\n",
       "      .attr(\"viewBox\", \"0 -5 10 10\")\n",
       "      .attr(\"refX\", 10)\n",
       "      .attr(\"refY\", 0)\n",
       "      .attr(\"markerWidth\", 6)\n",
       "      .attr(\"markerHeight\", 6)\n",
       "      .attr(\"orient\", \"auto\")\n",
       "      .append(\"svg:path\")\n",
       "      .attr(\"fill\", \"#444\")\n",
       "      .attr(\"d\", \"M0,-5L10,0L0,5\");\n",
       "\n",
       "    // Link: The links between nodes.\n",
       "    const link = svg_g\n",
       "      .append(\"g\")\n",
       "      .attr(\"id\", \"links\")\n",
       "      .attr(\"fill\", \"none\")\n",
       "      .attr(\"stroke-width\", 1.5)\n",
       "      .selectAll(\"path\")\n",
       "      .data(links)\n",
       "      .enter()\n",
       "      .append(\"path\")\n",
       "      .attr(\"stroke\", \"#444\")\n",
       "      .attr(\"stroke-width\", 2)\n",
       "      .attr(\"marker-end\", function (d, i) {\n",
       "        return `url(\"#marker_0\")`;\n",
       "      })\n",
       "      // 'd' attr is calculated on tick, not here.\n",
       "\n",
       "    /**\n",
       "     * When the mouse is unclicked, clear the selectedNode.\n",
       "     * This will remove the 'hide' class from all nodes in the tick function.\n",
       "     */\n",
       "    function deselectNode() {\n",
       "      selectedNode = null;\n",
       "    }\n",
       "\n",
       "    /**\n",
       "     * When a node is clicked, update selectedNode to reflect the id of that node,\n",
       "     * as well as the id of all nodes and links connected to it.\n",
       "     * @param  {[type]} d [description]\n",
       "     * @param  {[type]} i [description]\n",
       "     * @return {Object}   The selectedNode\n",
       "     *                    (baseNodeId, connectedNodeIds, connectedLinkIds).\n",
       "     */\n",
       "    function selectNode(d, i) {\n",
       "      let connectedNodeIds = new Set();\n",
       "      let connectedLinkIds = new Set();\n",
       "      for (let link of links) {\n",
       "        if (link.source.index === i.index || link.target.index === i.index) {\n",
       "          connectedLinkIds.add(link.index);\n",
       "          if (link.source.index === i.index) {\n",
       "            connectedNodeIds.add(link.target.index);\n",
       "          } else if (link.target.index === i.index) {\n",
       "            connectedNodeIds.add(link.source.index);\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      selectedNode = {\n",
       "        baseNodeId: i.index,\n",
       "        connectedNodeIds: connectedNodeIds,\n",
       "        connectedLinkIds: connectedLinkIds,\n",
       "      };\n",
       "    }\n",
       "\n",
       "    // Node: The nodes.\n",
       "    const node = svg_g\n",
       "      .append(\"g\")\n",
       "      .attr(\"stroke-width\", 3)\n",
       "      .selectAll(\"circle\")\n",
       "      .data(nodes)\n",
       "      .join(\"circle\")\n",
       "      .attr(\"class\", \"node\")\n",
       "      .attr(\"r\", nodeSize)\n",
       "      .attr(\"fill\", getColour)\n",
       "      .attr(\"stroke\", function (d) {\n",
       "        return lightenDarkenColor(getColour(d), -15);\n",
       "      })\n",
       "      .on(\"mousedown\", selectNode)\n",
       "      .call(drag(simulation));\n",
       "\n",
       "    // When a node is hovered over, set the tooltip accordingly.\n",
       "    // Clear the tooltip when the mouse leaves a node.\n",
       "    d3.selectAll(\".node\").on(\"mouseover\", setTooltip);\n",
       "    d3.selectAll(\".node\").on(\"mouseleave\", clearTooltip);\n",
       "\n",
       "    // When mouse up event (or pointer up event) fires, deselect the\n",
       "    // selected node if there is one.\n",
       "    document.addEventListener(\"mouseup\", deselectNode);\n",
       "    document.addEventListener(\"pointerup\", deselectNode);\n",
       "\n",
       "    // Text: The text appearing on the nodes.\n",
       "    const text = svg_g\n",
       "      .append(\"g\")\n",
       "      .selectAll(\"text\")\n",
       "      .data(nodes)\n",
       "      .enter()\n",
       "      .append(\"text\")\n",
       "      .text((d) => d.name)\n",
       "      .attr(\"font-size\", 12)\n",
       "      .attr(\"text-anchor\", \"middle\")\n",
       "      .attr(\"dominant-baseline\", \"central\");\n",
       "\n",
       "    // Links text: The text appearing on the links.\n",
       "    const link_text = svg_g\n",
       "      .append(\"g\")\n",
       "      .selectAll(\"text\")\n",
       "      .data(links)\n",
       "      .enter()\n",
       "      .append(\"text\")\n",
       "      .text((d) => d.type)\n",
       "      .attr(\"font-size\", 10)\n",
       "      .attr(\"text-anchor\", \"middle\")\n",
       "      .attr(\"dominant-baseline\", \"central\");\n",
       "\n",
       "    // Every time the simulation ticks, update the path of the links,\n",
       "    // the location of the nodes, the location of the text, and also the\n",
       "    // tooltip position.\n",
       "    simulation.on(\"tick\", () => {\n",
       "\n",
       "      // Only draw the links starting from the edge of node 1 to node 2.\n",
       "      // This way, when the nodes go transparent, the links won't be visible underneath\n",
       "      // the nodes (which looks bad).\n",
       "      // Code for doing this was found here:\n",
       "      // https://www.appsloveworld.com/d3js/100/17/\n",
       "      // create-links-from-node-border-to-node-border-not-center-to-center\n",
       "      link.attr(\n",
       "        \"d\",\n",
       "        (d) => {\n",
       "          x1 = d.source.x + (Math.cos(Math.atan2(d.target.y - d.source.y, d.target.x - d.source.x)) * nodeSize);\n",
       "\n",
       "          y1 = d.source.y + (Math.sin(Math.atan2(d.target.y - d.source.y, d.target.x - d.source.x)) * nodeSize);\n",
       "\n",
       "          x2 = d.target.x - (Math.cos(Math.atan2(d.target.y - d.source.y, d.target.x - d.source.x)) * nodeSize);\n",
       "\n",
       "          y2 = d.target.y - (Math.sin(Math.atan2(d.target.y - d.source.y, d.target.x - d.source.x)) * nodeSize);\n",
       "          return `M${x1},${y1}A0,0 0 0,1 ${x2},${y2}`\n",
       "        }\n",
       "      );\n",
       "\n",
       "      node.attr(\"cx\", (d) => d.x).attr(\"cy\", (d) => d.y);\n",
       "      text.attr(\"x\", (d) => Math.floor(d.x)).attr(\"y\", (d) => Math.floor(d.y));\n",
       "      link_text\n",
       "        .attr(\"x\", (d) => (d.source.x + d.target.x) / 2)\n",
       "        .attr(\"y\", (d) => (d.source.y + d.target.y) / 2);\n",
       "      updateTooltipPosition();\n",
       "\n",
       "      // If a node is currently selected, add a class to the nodes that are not\n",
       "      // related to that node.\n",
       "      //\n",
       "\n",
       "      node.classed(\"not-selected\", function (d, i) {\n",
       "        return (\n",
       "          selectedNode &&\n",
       "          selectedNode.baseNodeId !== i &&\n",
       "          !selectedNode.connectedNodeIds.has(i)\n",
       "        );\n",
       "      });\n",
       "\n",
       "      text.classed(\"not-selected\", function (d, i) {\n",
       "        return (\n",
       "          selectedNode &&\n",
       "          selectedNode.baseNodeId !== i &&\n",
       "          !selectedNode.connectedNodeIds.has(i)\n",
       "        );\n",
       "      });\n",
       "\n",
       "      link.classed(\"not-selected\", function (d, i) {\n",
       "        return selectedNode && !selectedNode.connectedLinkIds.has(i);\n",
       "      });\n",
       "\n",
       "      link_text.classed(\"not-selected\", function (d, i) {\n",
       "        return selectedNode && !selectedNode.connectedLinkIds.has(i);\n",
       "      });\n",
       "    });\n",
       "\n",
       "    // Register the zoom handler.\n",
       "    let zoom = d3.zoom().on(\"zoom\", handleZoom);\n",
       "\n",
       "    /**\n",
       "     * A function to handle zooming/panning. Sets a transform on the svg_g container\n",
       "     * according to the zoom/pan level.\n",
       "     * @param  {[type]} e [description]\n",
       "     * @return {[type]}   [description]\n",
       "     */\n",
       "    function handleZoom(e) {\n",
       "      transform = e.transform;\n",
       "      svg_g.attr(\"transform\", e.transform);\n",
       "      text.attr(\"font-size\", 12 / e.transform.k ** 0.7);\n",
       "      link_text.attr(\"font-size\", 10 / e.transform.k ** 0.7);\n",
       "    }\n",
       "\n",
       "    // Initialise the zooming.\n",
       "    function initZoom() {\n",
       "      svg.call(zoom);\n",
       "    }\n",
       "\n",
       "    initZoom();\n",
       "\n",
       "    // If dummyData is present (i.e. dev mode), display a message at the bottom.\n",
       "    if (dummyData) {\n",
       "      let div = document.createElement(\"div\");\n",
       "      div.innerHTML =\n",
       "        \"<h3>Note: currently in development mode using dummy data.</h3>\";\n",
       "      document.body.appendChild(div);\n",
       "    }\n",
       "  });\n",
       "</script>\n",
       "<!-- <script src=\"graphVisualisation.js\"></script> -->\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gqvis\n",
    "\n",
    "gqvis.visualise_cypher(\"MATCH (e:Entity {name: 'pump'})-[r:HAS_OBSERVATION]->(o:Observation) RETURN e, r, o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can also use our graph as a way to quickly search and access work orders for the entities appearing in those work orders. For example, searching for __all work orders containing a leak__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "<meta charset=\"utf-8\" />\n",
       "<link\n",
       "  href=\"https://fonts.googleapis.com/css2?family=Open+Sans&display=swap\"\n",
       "  rel=\"stylesheet\"\n",
       "/>\n",
       "<style>\n",
       "  circle {\n",
       "    cursor: pointer;\n",
       "\n",
       "  }\n",
       "  .not-selected {\n",
       "    opacity: 0.2;\n",
       "  }\n",
       "  text {\n",
       "    pointer-events: none;\n",
       "    font-family: \"Open Sans\", sans-serif;\n",
       "    font-size: 12;\n",
       "  }\n",
       "\n",
       "  g#links {\n",
       "    opacity: 0.3;\n",
       "  }\n",
       "\n",
       "  .tooltip {\n",
       "    background: #f5f5f5;\n",
       "\n",
       "    padding: 10px 10px;\n",
       "    font-family: \"Open Sans\", sans-serif;\n",
       "    box-sizing: border-box;\n",
       "    font-size: 0.8em;\n",
       "    position: absolute;\n",
       "    top: 1em;\n",
       "    left: 1em;\n",
       "    opacity: 0;\n",
       "    transition: opacity 0.3s ease;\n",
       "    pointer-events: none;\n",
       "    border: 2px solid #ddd;\n",
       "  }\n",
       "  .tooltip:after {\n",
       "    content: \" \";\n",
       "    position: absolute;\n",
       "    top: calc(50% - 6px);\n",
       "    left: -12px;\n",
       "    width: 0;\n",
       "    height: 0;\n",
       "    border-style: solid;\n",
       "    border-width: 6px 12px 6px 0;\n",
       "    border-color: transparent #f5f5f5 transparent transparent;\n",
       "  }\n",
       "  .tooltip:before {\n",
       "    content: \" \";\n",
       "    position: absolute;\n",
       "    top: calc(50% - 8px);\n",
       "    left: -16px;\n",
       "    width: 0;\n",
       "    height: 0;\n",
       "    border-style: solid;\n",
       "    border-width: 8px 16px 8px 0;\n",
       "    border-color: transparent #ddd transparent transparent;\n",
       "  }\n",
       "  .tooltip-table td {\n",
       "    padding: 0em 0.5em;\n",
       "  }\n",
       "  .tooltip-table tr td:first-child {\n",
       "    font-weight: bold;\n",
       "  }\n",
       "  .tooltip-table tr td:last-child {\n",
       "    text-align: left;\n",
       "  }\n",
       "\n",
       "  .d3-graph-wrapper {\n",
       "    width: 960px;\n",
       "    display: flex;\n",
       "    flex-direction: row;\n",
       "    border: 2px solid #ddd;\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"d3-graph-wrapper\">\n",
       "  <svg id=\"svg-chart\" width=\"960px\" height=\"600\"></svg>\n",
       "  <div id=\"tooltip\" class=\"tooltip\">\n",
       "    <table class=\"tooltip-table\"></table>\n",
       "  </div>\n",
       "</div>\n",
       "<!-- Note d3 is loaded via require.js -->\n",
       "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\"></script>\n",
       "<script>\n",
       "  // This bit is a bit of a pain.\n",
       "  // I wanted a way to run this both directly in the browser (for developing),\n",
       "  // and in Jupyter.\n",
       "  // When developing, I needed some 'dummy data' to populate the graph.\n",
       "  // The following script tag attempts to load the file 'dummyData.js' (which\n",
       "  // contains the dummy data). It will fail when running on jupyter.\n",
       "  //\n",
       "  // Whether or not the dummyData file loads (and dummyData is not null) determines\n",
       "  // whether the CHART_SELECTOR (the id of the svg chart) will be fixed or\n",
       "  // dynamic based on template injection, and also whether the nodes and links are from\n",
       "  // the dummyData or dynamic via template injection.\n",
       "  dummyData = null;\n",
       "  console.log(\n",
       "    \"Attempting to load dummy data. This will fail if running via \" +\n",
       "      \"jupyter notebook, and may throw console errors - this is OK.\"\n",
       "  );\n",
       "</script>\n",
       "<script src=\"dummyData.js\"></script>\n",
       "<script>\n",
       "  require.config({\n",
       "    paths: {\n",
       "      d3: \"https://d3js.org/d3.v7.min\",\n",
       "    },\n",
       "  });\n",
       "\n",
       "  // Everything is wrapped in this require.js function - this way the variables don't\n",
       "  // go into the global scope and everything works even when multiple graphs are\n",
       "  // rendered.\n",
       "  require([\"d3\"], function (d3) {\n",
       "    const CHART_SELECTOR = dummyData ? \"#chart\" : \"#chart-67564\";\n",
       "    const TOOLTIP_SELECTOR = dummyData ? \"#tooltip\" : \"#tooltip-67564\";\n",
       "    const svg_chart = document.getElementById(\"svg-chart\");\n",
       "    const tooltip = document.getElementById(\"tooltip\");\n",
       "\n",
       "    // Maintain the svg element the tooltip is currently bound to.\n",
       "    let tooltipNode = null;\n",
       "    const tooltipTable = tooltip.querySelector(\".tooltip-table\");\n",
       "\n",
       "    // Maintain the currently selected node, if any, as well as all nodes/links\n",
       "    // related to it.\n",
       "    // When not null, it will have the following:\n",
       "    // {\n",
       "    //  baseNodeId: (the index of the base node being selected),\n",
       "    //  connectedNodeIds: (an array of all connected node ids),\n",
       "    //  connectedLinkIds: (an array of all connected link ids),\n",
       "    // };\n",
       "    let selectedNode = null;\n",
       "\n",
       "    // Dynamically set the id of the svg and tooltip.\n",
       "    // If in development mode (i.e. dummyData is not null), the id will just be\n",
       "    // \"#chart\" or \"#tooltip\" respectively.\n",
       "    svg_chart.id = CHART_SELECTOR.substring(1, CHART_SELECTOR.length);\n",
       "    tooltip.id = TOOLTIP_SELECTOR.substring(1, TOOLTIP_SELECTOR.length);\n",
       "    let data = {};\n",
       "    if (dummyData) {\n",
       "      data = dummyData;\n",
       "    } else {\n",
       "      data = {\n",
       "        nodes: JSON.parse(\"[{'id': 248, 'category': 'Document', 'name': 'fix leak on pump'}, {'id': 247, 'category': 'Observation', 'name': 'leak', '_id': 'leak__Observation'}, {'id': 246, 'category': 'Document', 'name': 'pump leak'}]\".replace(/'/g, '\"')),\n",
       "        links: JSON.parse(\"[{'source': 247, 'target': 248, 'type': 'APPEARS_IN'}, {'source': 247, 'target': 246, 'type': 'APPEARS_IN'}]\".replace(/'/g, '\"')),\n",
       "      };\n",
       "    }\n",
       "\n",
       "    /* Code found in a range of places, spliced together from the following:\n",
       "    https://observablehq.com/@brunolaranjeira/d3-v6-force-directed-graph-with-directional-straight-arrow\n",
       "    https://bl.ocks.org/mbostock/4062045\n",
       "    https://github.com/nlp-tlp/aquila/blob/master/views/visualisations/entity_linking_graph.jade\n",
       "    https://observablehq.com/@harrylove/draw-an-arrowhead-marker-connected-to-a-line-in-d3\n",
       "    */\n",
       "\n",
       "    // https://stackoverflow.com/questions/5560248/programmatically-lighten-or-darken-a-hex-color-or-rgb-and-blend-colors\n",
       "    function lightenDarkenColor(col, amt) {\n",
       "      let usePound = false;\n",
       "      if (col[0] == \"#\") {\n",
       "        col = col.slice(1);\n",
       "        usePound = true;\n",
       "      }\n",
       "\n",
       "      let num = parseInt(col, 16);\n",
       "\n",
       "      let r = (num >> 16) + amt;\n",
       "\n",
       "      if (r > 255) r = 255;\n",
       "      else if (r < 0) r = 0;\n",
       "\n",
       "      let b = ((num >> 8) & 0x00ff) + amt;\n",
       "\n",
       "      if (b > 255) b = 255;\n",
       "      else if (b < 0) b = 0;\n",
       "\n",
       "      let g = (num & 0x0000ff) + amt;\n",
       "\n",
       "      if (g > 255) g = 255;\n",
       "      else if (g < 0) g = 0;\n",
       "\n",
       "      return (usePound ? \"#\" : \"\") + (g | (b << 8) | (r << 16)).toString(16);\n",
       "    }\n",
       "\n",
       "    /**\n",
       "     * Set the tooltip (after hovering over a specific node). The tooltip, which appears\n",
       "     * on the right of the graph, will show the property (key, value) pairs of the\n",
       "     * selected node.\n",
       "     * @param {[type]} d The d3 data point.\n",
       "     * @param {[type]} i The object.\n",
       "     */\n",
       "    function setTooltip(d, i) {\n",
       "      var p = Object.getPrototypeOf(i);\n",
       "      const keys = Object.keys(p);\n",
       "      const vals = keys.map((key) => p[key]);\n",
       "\n",
       "      let table = \"\";\n",
       "      for (var j = 0; j < keys.length; j++) {\n",
       "        table += \"<tr>\";\n",
       "        table += `<td>${keys[j]}:</td>`;\n",
       "        table += `<td>${vals[j]}</td>`;\n",
       "        table += \"</tr>\";\n",
       "      }\n",
       "\n",
       "      tooltipTable.innerHTML = table;\n",
       "      tooltip.style.opacity = \"0.95\";\n",
       "\n",
       "      tooltipNode = d.target;\n",
       "      updateTooltipPosition();\n",
       "\n",
       "      return d;\n",
       "    }\n",
       "\n",
       "    /**\n",
       "     * Update the position (x and y coords) of the tooltip based on the location\n",
       "     * of the element it is bound to.\n",
       "     * @return {[type]} [description]\n",
       "     */\n",
       "    function updateTooltipPosition() {\n",
       "      if (!tooltipNode) return;\n",
       "      const rect = tooltipNode.getBoundingClientRect();\n",
       "      const tooltipRect = tooltip.getBoundingClientRect();\n",
       "      const svgRect = svg_chart.getBoundingClientRect();\n",
       "\n",
       "      tooltip.style.left = `${rect.right + 25 - svgRect.left}px`;\n",
       "      tooltip.style.top = `${\n",
       "        (rect.top + rect.bottom) / 2 - tooltipRect.height / 2 - svgRect.top\n",
       "      }px`;\n",
       "    }\n",
       "\n",
       "    /**\n",
       "     * Clear the tooltip by settings its opacity to 0.\n",
       "     * @return {[type]} [description]\n",
       "     */\n",
       "    function clearTooltip() {\n",
       "      tooltip.style.opacity = \"0\";\n",
       "    }\n",
       "\n",
       "    /**\n",
       "     * Allow the nodes to be moved around when dragged.\n",
       "     * @param  {[type]} simulation [description]\n",
       "     * @return {[type]}            [description]\n",
       "     */\n",
       "    let drag = (simulation) => {\n",
       "      function dragstarted(event) {\n",
       "        if (!event.active) simulation.alphaTarget(0.05).restart();\n",
       "        event.subject.fx = event.subject.x;\n",
       "        event.subject.fy = event.subject.y;\n",
       "      }\n",
       "\n",
       "      function dragged(event) {\n",
       "        event.subject.fx = event.x;\n",
       "        event.subject.fy = event.y;\n",
       "      }\n",
       "\n",
       "      function dragended(event) {\n",
       "        if (!event.active) simulation.alphaTarget(0);\n",
       "        event.subject.fx = null;\n",
       "        event.subject.fy = null;\n",
       "      }\n",
       "\n",
       "      return d3\n",
       "        .drag()\n",
       "        .on(\"start\", dragstarted)\n",
       "        .on(\"drag\", dragged)\n",
       "        .on(\"end\", dragended);\n",
       "    };\n",
       "\n",
       "    const scale = d3.scaleOrdinal(d3.schemeCategory10);\n",
       "\n",
       "    const nodeSize = 40;\n",
       "    let height = 600;\n",
       "    let width = 600;\n",
       "\n",
       "    const links = data.links.map((d) => Object.create(d));\n",
       "    const nodes = data.nodes.map((d) => Object.create(d));\n",
       "\n",
       "    // A list of colours that the colour map will be generated from.\n",
       "    const colours = [\n",
       "      \"#99ffcc\",\n",
       "      \"#ffcccc\",\n",
       "      \"#ccccff\",\n",
       "      \"#ccff99\",\n",
       "      \"#ccffcc\",\n",
       "      \"#ccffff\",\n",
       "      \"#ffcc99\",\n",
       "      \"#ffccff\",\n",
       "      \"#ffff99\",\n",
       "      \"#ffffcc\",\n",
       "      \"#cccc99\",\n",
       "      \"#fbafff\",\n",
       "    ];\n",
       "\n",
       "    /**\n",
       "     * Load the 'colour map' (i.e. mapping of category to colour).\n",
       "     * @param  {Array} nodes The node data.\n",
       "     * @return {Object}       The colour map.\n",
       "     */\n",
       "    function loadColourMap(nodes) {\n",
       "      let colourMap = {};\n",
       "      // Load the colour map\n",
       "      for (let i = 0; i < nodes.length; i++) {\n",
       "        const category = nodes[i].category;\n",
       "        if (!(category in colourMap)) {\n",
       "          colourMap[category] =\n",
       "            colours[Object.keys(colourMap).length % colours.length];\n",
       "        }\n",
       "      }\n",
       "      return colourMap;\n",
       "    }\n",
       "    colourMap = loadColourMap(nodes);\n",
       "    const getColour = (d) => {\n",
       "      return colourMap[d.category];\n",
       "    };\n",
       "\n",
       "    // Initialise some d3 forces etc to make the graph behave properly.\n",
       "    const simulation = d3\n",
       "      .forceSimulation(nodes)\n",
       "      .force(\n",
       "        \"link\",\n",
       "        d3.forceLink(links).id((d) => d.id)\n",
       "      )\n",
       "      // .force(\"charge\", d3.forceManyBody().strength(-200))\n",
       "      .force(\n",
       "        \"collide\",\n",
       "        d3.forceCollide((d) => nodeSize * 1.5)\n",
       "      )\n",
       "      .force(\"center\", d3.forceCenter(width / 2, height / 2));\n",
       "\n",
       "    // Svg: the main container for rendering the graph.\n",
       "    const svg = d3\n",
       "      .select(CHART_SELECTOR)\n",
       "      .attr(\"viewBox\", [0, 0, width, height]);\n",
       "\n",
       "    // svg_g: The 'g' within svg, which seems to make zooming/panning smoother\n",
       "    const svg_g = svg.append(\"g\");\n",
       "    const defs = svg_g.append(\"defs\");\n",
       "\n",
       "    // Marker: a definition for the triangles appearing at the end of each link.\n",
       "    const marker = defs\n",
       "      .selectAll(\"marker\")\n",
       "      .data([\"type_1\"])\n",
       "      .enter()\n",
       "      .append(\"svg:marker\")\n",
       "      .attr(\"id\", function (d, i) {\n",
       "        return `marker_${i}`;\n",
       "      })\n",
       "      .attr(\"viewBox\", \"0 -5 10 10\")\n",
       "      .attr(\"refX\", 10)\n",
       "      .attr(\"refY\", 0)\n",
       "      .attr(\"markerWidth\", 6)\n",
       "      .attr(\"markerHeight\", 6)\n",
       "      .attr(\"orient\", \"auto\")\n",
       "      .append(\"svg:path\")\n",
       "      .attr(\"fill\", \"#444\")\n",
       "      .attr(\"d\", \"M0,-5L10,0L0,5\");\n",
       "\n",
       "    // Link: The links between nodes.\n",
       "    const link = svg_g\n",
       "      .append(\"g\")\n",
       "      .attr(\"id\", \"links\")\n",
       "      .attr(\"fill\", \"none\")\n",
       "      .attr(\"stroke-width\", 1.5)\n",
       "      .selectAll(\"path\")\n",
       "      .data(links)\n",
       "      .enter()\n",
       "      .append(\"path\")\n",
       "      .attr(\"stroke\", \"#444\")\n",
       "      .attr(\"stroke-width\", 2)\n",
       "      .attr(\"marker-end\", function (d, i) {\n",
       "        return `url(\"#marker_0\")`;\n",
       "      })\n",
       "      // 'd' attr is calculated on tick, not here.\n",
       "\n",
       "    /**\n",
       "     * When the mouse is unclicked, clear the selectedNode.\n",
       "     * This will remove the 'hide' class from all nodes in the tick function.\n",
       "     */\n",
       "    function deselectNode() {\n",
       "      selectedNode = null;\n",
       "    }\n",
       "\n",
       "    /**\n",
       "     * When a node is clicked, update selectedNode to reflect the id of that node,\n",
       "     * as well as the id of all nodes and links connected to it.\n",
       "     * @param  {[type]} d [description]\n",
       "     * @param  {[type]} i [description]\n",
       "     * @return {Object}   The selectedNode\n",
       "     *                    (baseNodeId, connectedNodeIds, connectedLinkIds).\n",
       "     */\n",
       "    function selectNode(d, i) {\n",
       "      let connectedNodeIds = new Set();\n",
       "      let connectedLinkIds = new Set();\n",
       "      for (let link of links) {\n",
       "        if (link.source.index === i.index || link.target.index === i.index) {\n",
       "          connectedLinkIds.add(link.index);\n",
       "          if (link.source.index === i.index) {\n",
       "            connectedNodeIds.add(link.target.index);\n",
       "          } else if (link.target.index === i.index) {\n",
       "            connectedNodeIds.add(link.source.index);\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      selectedNode = {\n",
       "        baseNodeId: i.index,\n",
       "        connectedNodeIds: connectedNodeIds,\n",
       "        connectedLinkIds: connectedLinkIds,\n",
       "      };\n",
       "    }\n",
       "\n",
       "    // Node: The nodes.\n",
       "    const node = svg_g\n",
       "      .append(\"g\")\n",
       "      .attr(\"stroke-width\", 3)\n",
       "      .selectAll(\"circle\")\n",
       "      .data(nodes)\n",
       "      .join(\"circle\")\n",
       "      .attr(\"class\", \"node\")\n",
       "      .attr(\"r\", nodeSize)\n",
       "      .attr(\"fill\", getColour)\n",
       "      .attr(\"stroke\", function (d) {\n",
       "        return lightenDarkenColor(getColour(d), -15);\n",
       "      })\n",
       "      .on(\"mousedown\", selectNode)\n",
       "      .call(drag(simulation));\n",
       "\n",
       "    // When a node is hovered over, set the tooltip accordingly.\n",
       "    // Clear the tooltip when the mouse leaves a node.\n",
       "    d3.selectAll(\".node\").on(\"mouseover\", setTooltip);\n",
       "    d3.selectAll(\".node\").on(\"mouseleave\", clearTooltip);\n",
       "\n",
       "    // When mouse up event (or pointer up event) fires, deselect the\n",
       "    // selected node if there is one.\n",
       "    document.addEventListener(\"mouseup\", deselectNode);\n",
       "    document.addEventListener(\"pointerup\", deselectNode);\n",
       "\n",
       "    // Text: The text appearing on the nodes.\n",
       "    const text = svg_g\n",
       "      .append(\"g\")\n",
       "      .selectAll(\"text\")\n",
       "      .data(nodes)\n",
       "      .enter()\n",
       "      .append(\"text\")\n",
       "      .text((d) => d.name)\n",
       "      .attr(\"font-size\", 12)\n",
       "      .attr(\"text-anchor\", \"middle\")\n",
       "      .attr(\"dominant-baseline\", \"central\");\n",
       "\n",
       "    // Links text: The text appearing on the links.\n",
       "    const link_text = svg_g\n",
       "      .append(\"g\")\n",
       "      .selectAll(\"text\")\n",
       "      .data(links)\n",
       "      .enter()\n",
       "      .append(\"text\")\n",
       "      .text((d) => d.type)\n",
       "      .attr(\"font-size\", 10)\n",
       "      .attr(\"text-anchor\", \"middle\")\n",
       "      .attr(\"dominant-baseline\", \"central\");\n",
       "\n",
       "    // Every time the simulation ticks, update the path of the links,\n",
       "    // the location of the nodes, the location of the text, and also the\n",
       "    // tooltip position.\n",
       "    simulation.on(\"tick\", () => {\n",
       "\n",
       "      // Only draw the links starting from the edge of node 1 to node 2.\n",
       "      // This way, when the nodes go transparent, the links won't be visible underneath\n",
       "      // the nodes (which looks bad).\n",
       "      // Code for doing this was found here:\n",
       "      // https://www.appsloveworld.com/d3js/100/17/\n",
       "      // create-links-from-node-border-to-node-border-not-center-to-center\n",
       "      link.attr(\n",
       "        \"d\",\n",
       "        (d) => {\n",
       "          x1 = d.source.x + (Math.cos(Math.atan2(d.target.y - d.source.y, d.target.x - d.source.x)) * nodeSize);\n",
       "\n",
       "          y1 = d.source.y + (Math.sin(Math.atan2(d.target.y - d.source.y, d.target.x - d.source.x)) * nodeSize);\n",
       "\n",
       "          x2 = d.target.x - (Math.cos(Math.atan2(d.target.y - d.source.y, d.target.x - d.source.x)) * nodeSize);\n",
       "\n",
       "          y2 = d.target.y - (Math.sin(Math.atan2(d.target.y - d.source.y, d.target.x - d.source.x)) * nodeSize);\n",
       "          return `M${x1},${y1}A0,0 0 0,1 ${x2},${y2}`\n",
       "        }\n",
       "      );\n",
       "\n",
       "      node.attr(\"cx\", (d) => d.x).attr(\"cy\", (d) => d.y);\n",
       "      text.attr(\"x\", (d) => Math.floor(d.x)).attr(\"y\", (d) => Math.floor(d.y));\n",
       "      link_text\n",
       "        .attr(\"x\", (d) => (d.source.x + d.target.x) / 2)\n",
       "        .attr(\"y\", (d) => (d.source.y + d.target.y) / 2);\n",
       "      updateTooltipPosition();\n",
       "\n",
       "      // If a node is currently selected, add a class to the nodes that are not\n",
       "      // related to that node.\n",
       "      //\n",
       "\n",
       "      node.classed(\"not-selected\", function (d, i) {\n",
       "        return (\n",
       "          selectedNode &&\n",
       "          selectedNode.baseNodeId !== i &&\n",
       "          !selectedNode.connectedNodeIds.has(i)\n",
       "        );\n",
       "      });\n",
       "\n",
       "      text.classed(\"not-selected\", function (d, i) {\n",
       "        return (\n",
       "          selectedNode &&\n",
       "          selectedNode.baseNodeId !== i &&\n",
       "          !selectedNode.connectedNodeIds.has(i)\n",
       "        );\n",
       "      });\n",
       "\n",
       "      link.classed(\"not-selected\", function (d, i) {\n",
       "        return selectedNode && !selectedNode.connectedLinkIds.has(i);\n",
       "      });\n",
       "\n",
       "      link_text.classed(\"not-selected\", function (d, i) {\n",
       "        return selectedNode && !selectedNode.connectedLinkIds.has(i);\n",
       "      });\n",
       "    });\n",
       "\n",
       "    // Register the zoom handler.\n",
       "    let zoom = d3.zoom().on(\"zoom\", handleZoom);\n",
       "\n",
       "    /**\n",
       "     * A function to handle zooming/panning. Sets a transform on the svg_g container\n",
       "     * according to the zoom/pan level.\n",
       "     * @param  {[type]} e [description]\n",
       "     * @return {[type]}   [description]\n",
       "     */\n",
       "    function handleZoom(e) {\n",
       "      transform = e.transform;\n",
       "      svg_g.attr(\"transform\", e.transform);\n",
       "      text.attr(\"font-size\", 12 / e.transform.k ** 0.7);\n",
       "      link_text.attr(\"font-size\", 10 / e.transform.k ** 0.7);\n",
       "    }\n",
       "\n",
       "    // Initialise the zooming.\n",
       "    function initZoom() {\n",
       "      svg.call(zoom);\n",
       "    }\n",
       "\n",
       "    initZoom();\n",
       "\n",
       "    // If dummyData is present (i.e. dev mode), display a message at the bottom.\n",
       "    if (dummyData) {\n",
       "      let div = document.createElement(\"div\");\n",
       "      div.innerHTML =\n",
       "        \"<h3>Note: currently in development mode using dummy data.</h3>\";\n",
       "      document.body.appendChild(div);\n",
       "    }\n",
       "  });\n",
       "</script>\n",
       "<!-- <script src=\"graphVisualisation.js\"></script> -->\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gqvis.visualise_cypher(\"MATCH (d:Document)<-[a:APPEARS_IN]-(o:Observation {name: 'leak'}) RETURN d, a, o\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could extend this to also show the items on which the leaks were present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "<meta charset=\"utf-8\" />\n",
       "<link\n",
       "  href=\"https://fonts.googleapis.com/css2?family=Open+Sans&display=swap\"\n",
       "  rel=\"stylesheet\"\n",
       "/>\n",
       "<style>\n",
       "  circle {\n",
       "    cursor: pointer;\n",
       "\n",
       "  }\n",
       "  .not-selected {\n",
       "    opacity: 0.2;\n",
       "  }\n",
       "  text {\n",
       "    pointer-events: none;\n",
       "    font-family: \"Open Sans\", sans-serif;\n",
       "    font-size: 12;\n",
       "  }\n",
       "\n",
       "  g#links {\n",
       "    opacity: 0.3;\n",
       "  }\n",
       "\n",
       "  .tooltip {\n",
       "    background: #f5f5f5;\n",
       "\n",
       "    padding: 10px 10px;\n",
       "    font-family: \"Open Sans\", sans-serif;\n",
       "    box-sizing: border-box;\n",
       "    font-size: 0.8em;\n",
       "    position: absolute;\n",
       "    top: 1em;\n",
       "    left: 1em;\n",
       "    opacity: 0;\n",
       "    transition: opacity 0.3s ease;\n",
       "    pointer-events: none;\n",
       "    border: 2px solid #ddd;\n",
       "  }\n",
       "  .tooltip:after {\n",
       "    content: \" \";\n",
       "    position: absolute;\n",
       "    top: calc(50% - 6px);\n",
       "    left: -12px;\n",
       "    width: 0;\n",
       "    height: 0;\n",
       "    border-style: solid;\n",
       "    border-width: 6px 12px 6px 0;\n",
       "    border-color: transparent #f5f5f5 transparent transparent;\n",
       "  }\n",
       "  .tooltip:before {\n",
       "    content: \" \";\n",
       "    position: absolute;\n",
       "    top: calc(50% - 8px);\n",
       "    left: -16px;\n",
       "    width: 0;\n",
       "    height: 0;\n",
       "    border-style: solid;\n",
       "    border-width: 8px 16px 8px 0;\n",
       "    border-color: transparent #ddd transparent transparent;\n",
       "  }\n",
       "  .tooltip-table td {\n",
       "    padding: 0em 0.5em;\n",
       "  }\n",
       "  .tooltip-table tr td:first-child {\n",
       "    font-weight: bold;\n",
       "  }\n",
       "  .tooltip-table tr td:last-child {\n",
       "    text-align: left;\n",
       "  }\n",
       "\n",
       "  .d3-graph-wrapper {\n",
       "    width: 960px;\n",
       "    display: flex;\n",
       "    flex-direction: row;\n",
       "    border: 2px solid #ddd;\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"d3-graph-wrapper\">\n",
       "  <svg id=\"svg-chart\" width=\"960px\" height=\"600\"></svg>\n",
       "  <div id=\"tooltip\" class=\"tooltip\">\n",
       "    <table class=\"tooltip-table\"></table>\n",
       "  </div>\n",
       "</div>\n",
       "<!-- Note d3 is loaded via require.js -->\n",
       "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\"></script>\n",
       "<script>\n",
       "  // This bit is a bit of a pain.\n",
       "  // I wanted a way to run this both directly in the browser (for developing),\n",
       "  // and in Jupyter.\n",
       "  // When developing, I needed some 'dummy data' to populate the graph.\n",
       "  // The following script tag attempts to load the file 'dummyData.js' (which\n",
       "  // contains the dummy data). It will fail when running on jupyter.\n",
       "  //\n",
       "  // Whether or not the dummyData file loads (and dummyData is not null) determines\n",
       "  // whether the CHART_SELECTOR (the id of the svg chart) will be fixed or\n",
       "  // dynamic based on template injection, and also whether the nodes and links are from\n",
       "  // the dummyData or dynamic via template injection.\n",
       "  dummyData = null;\n",
       "  console.log(\n",
       "    \"Attempting to load dummy data. This will fail if running via \" +\n",
       "      \"jupyter notebook, and may throw console errors - this is OK.\"\n",
       "  );\n",
       "</script>\n",
       "<script src=\"dummyData.js\"></script>\n",
       "<script>\n",
       "  require.config({\n",
       "    paths: {\n",
       "      d3: \"https://d3js.org/d3.v7.min\",\n",
       "    },\n",
       "  });\n",
       "\n",
       "  // Everything is wrapped in this require.js function - this way the variables don't\n",
       "  // go into the global scope and everything works even when multiple graphs are\n",
       "  // rendered.\n",
       "  require([\"d3\"], function (d3) {\n",
       "    const CHART_SELECTOR = dummyData ? \"#chart\" : \"#chart-521025\";\n",
       "    const TOOLTIP_SELECTOR = dummyData ? \"#tooltip\" : \"#tooltip-521025\";\n",
       "    const svg_chart = document.getElementById(\"svg-chart\");\n",
       "    const tooltip = document.getElementById(\"tooltip\");\n",
       "\n",
       "    // Maintain the svg element the tooltip is currently bound to.\n",
       "    let tooltipNode = null;\n",
       "    const tooltipTable = tooltip.querySelector(\".tooltip-table\");\n",
       "\n",
       "    // Maintain the currently selected node, if any, as well as all nodes/links\n",
       "    // related to it.\n",
       "    // When not null, it will have the following:\n",
       "    // {\n",
       "    //  baseNodeId: (the index of the base node being selected),\n",
       "    //  connectedNodeIds: (an array of all connected node ids),\n",
       "    //  connectedLinkIds: (an array of all connected link ids),\n",
       "    // };\n",
       "    let selectedNode = null;\n",
       "\n",
       "    // Dynamically set the id of the svg and tooltip.\n",
       "    // If in development mode (i.e. dummyData is not null), the id will just be\n",
       "    // \"#chart\" or \"#tooltip\" respectively.\n",
       "    svg_chart.id = CHART_SELECTOR.substring(1, CHART_SELECTOR.length);\n",
       "    tooltip.id = TOOLTIP_SELECTOR.substring(1, TOOLTIP_SELECTOR.length);\n",
       "    let data = {};\n",
       "    if (dummyData) {\n",
       "      data = dummyData;\n",
       "    } else {\n",
       "      data = {\n",
       "        nodes: JSON.parse(\"[{'id': 248, 'category': 'Document', 'name': 'fix leak on pump'}, {'id': 247, 'category': 'Observation', 'name': 'leak', '_id': 'leak__Observation'}, {'id': 244, 'category': 'Entity', 'name': 'pump', '_id': 'pump__Item'}, {'id': 246, 'category': 'Document', 'name': 'pump leak'}]\".replace(/'/g, '\"')),\n",
       "        links: JSON.parse(\"[{'source': 247, 'target': 248, 'type': 'APPEARS_IN'}, {'source': 244, 'target': 247, 'type': 'HAS_OBSERVATION'}, {'source': 247, 'target': 246, 'type': 'APPEARS_IN'}, {'source': 244, 'target': 247, 'type': 'HAS_OBSERVATION'}]\".replace(/'/g, '\"')),\n",
       "      };\n",
       "    }\n",
       "\n",
       "    /* Code found in a range of places, spliced together from the following:\n",
       "    https://observablehq.com/@brunolaranjeira/d3-v6-force-directed-graph-with-directional-straight-arrow\n",
       "    https://bl.ocks.org/mbostock/4062045\n",
       "    https://github.com/nlp-tlp/aquila/blob/master/views/visualisations/entity_linking_graph.jade\n",
       "    https://observablehq.com/@harrylove/draw-an-arrowhead-marker-connected-to-a-line-in-d3\n",
       "    */\n",
       "\n",
       "    // https://stackoverflow.com/questions/5560248/programmatically-lighten-or-darken-a-hex-color-or-rgb-and-blend-colors\n",
       "    function lightenDarkenColor(col, amt) {\n",
       "      let usePound = false;\n",
       "      if (col[0] == \"#\") {\n",
       "        col = col.slice(1);\n",
       "        usePound = true;\n",
       "      }\n",
       "\n",
       "      let num = parseInt(col, 16);\n",
       "\n",
       "      let r = (num >> 16) + amt;\n",
       "\n",
       "      if (r > 255) r = 255;\n",
       "      else if (r < 0) r = 0;\n",
       "\n",
       "      let b = ((num >> 8) & 0x00ff) + amt;\n",
       "\n",
       "      if (b > 255) b = 255;\n",
       "      else if (b < 0) b = 0;\n",
       "\n",
       "      let g = (num & 0x0000ff) + amt;\n",
       "\n",
       "      if (g > 255) g = 255;\n",
       "      else if (g < 0) g = 0;\n",
       "\n",
       "      return (usePound ? \"#\" : \"\") + (g | (b << 8) | (r << 16)).toString(16);\n",
       "    }\n",
       "\n",
       "    /**\n",
       "     * Set the tooltip (after hovering over a specific node). The tooltip, which appears\n",
       "     * on the right of the graph, will show the property (key, value) pairs of the\n",
       "     * selected node.\n",
       "     * @param {[type]} d The d3 data point.\n",
       "     * @param {[type]} i The object.\n",
       "     */\n",
       "    function setTooltip(d, i) {\n",
       "      var p = Object.getPrototypeOf(i);\n",
       "      const keys = Object.keys(p);\n",
       "      const vals = keys.map((key) => p[key]);\n",
       "\n",
       "      let table = \"\";\n",
       "      for (var j = 0; j < keys.length; j++) {\n",
       "        table += \"<tr>\";\n",
       "        table += `<td>${keys[j]}:</td>`;\n",
       "        table += `<td>${vals[j]}</td>`;\n",
       "        table += \"</tr>\";\n",
       "      }\n",
       "\n",
       "      tooltipTable.innerHTML = table;\n",
       "      tooltip.style.opacity = \"0.95\";\n",
       "\n",
       "      tooltipNode = d.target;\n",
       "      updateTooltipPosition();\n",
       "\n",
       "      return d;\n",
       "    }\n",
       "\n",
       "    /**\n",
       "     * Update the position (x and y coords) of the tooltip based on the location\n",
       "     * of the element it is bound to.\n",
       "     * @return {[type]} [description]\n",
       "     */\n",
       "    function updateTooltipPosition() {\n",
       "      if (!tooltipNode) return;\n",
       "      const rect = tooltipNode.getBoundingClientRect();\n",
       "      const tooltipRect = tooltip.getBoundingClientRect();\n",
       "      const svgRect = svg_chart.getBoundingClientRect();\n",
       "\n",
       "      tooltip.style.left = `${rect.right + 25 - svgRect.left}px`;\n",
       "      tooltip.style.top = `${\n",
       "        (rect.top + rect.bottom) / 2 - tooltipRect.height / 2 - svgRect.top\n",
       "      }px`;\n",
       "    }\n",
       "\n",
       "    /**\n",
       "     * Clear the tooltip by settings its opacity to 0.\n",
       "     * @return {[type]} [description]\n",
       "     */\n",
       "    function clearTooltip() {\n",
       "      tooltip.style.opacity = \"0\";\n",
       "    }\n",
       "\n",
       "    /**\n",
       "     * Allow the nodes to be moved around when dragged.\n",
       "     * @param  {[type]} simulation [description]\n",
       "     * @return {[type]}            [description]\n",
       "     */\n",
       "    let drag = (simulation) => {\n",
       "      function dragstarted(event) {\n",
       "        if (!event.active) simulation.alphaTarget(0.05).restart();\n",
       "        event.subject.fx = event.subject.x;\n",
       "        event.subject.fy = event.subject.y;\n",
       "      }\n",
       "\n",
       "      function dragged(event) {\n",
       "        event.subject.fx = event.x;\n",
       "        event.subject.fy = event.y;\n",
       "      }\n",
       "\n",
       "      function dragended(event) {\n",
       "        if (!event.active) simulation.alphaTarget(0);\n",
       "        event.subject.fx = null;\n",
       "        event.subject.fy = null;\n",
       "      }\n",
       "\n",
       "      return d3\n",
       "        .drag()\n",
       "        .on(\"start\", dragstarted)\n",
       "        .on(\"drag\", dragged)\n",
       "        .on(\"end\", dragended);\n",
       "    };\n",
       "\n",
       "    const scale = d3.scaleOrdinal(d3.schemeCategory10);\n",
       "\n",
       "    const nodeSize = 40;\n",
       "    let height = 600;\n",
       "    let width = 600;\n",
       "\n",
       "    const links = data.links.map((d) => Object.create(d));\n",
       "    const nodes = data.nodes.map((d) => Object.create(d));\n",
       "\n",
       "    // A list of colours that the colour map will be generated from.\n",
       "    const colours = [\n",
       "      \"#99ffcc\",\n",
       "      \"#ffcccc\",\n",
       "      \"#ccccff\",\n",
       "      \"#ccff99\",\n",
       "      \"#ccffcc\",\n",
       "      \"#ccffff\",\n",
       "      \"#ffcc99\",\n",
       "      \"#ffccff\",\n",
       "      \"#ffff99\",\n",
       "      \"#ffffcc\",\n",
       "      \"#cccc99\",\n",
       "      \"#fbafff\",\n",
       "    ];\n",
       "\n",
       "    /**\n",
       "     * Load the 'colour map' (i.e. mapping of category to colour).\n",
       "     * @param  {Array} nodes The node data.\n",
       "     * @return {Object}       The colour map.\n",
       "     */\n",
       "    function loadColourMap(nodes) {\n",
       "      let colourMap = {};\n",
       "      // Load the colour map\n",
       "      for (let i = 0; i < nodes.length; i++) {\n",
       "        const category = nodes[i].category;\n",
       "        if (!(category in colourMap)) {\n",
       "          colourMap[category] =\n",
       "            colours[Object.keys(colourMap).length % colours.length];\n",
       "        }\n",
       "      }\n",
       "      return colourMap;\n",
       "    }\n",
       "    colourMap = loadColourMap(nodes);\n",
       "    const getColour = (d) => {\n",
       "      return colourMap[d.category];\n",
       "    };\n",
       "\n",
       "    // Initialise some d3 forces etc to make the graph behave properly.\n",
       "    const simulation = d3\n",
       "      .forceSimulation(nodes)\n",
       "      .force(\n",
       "        \"link\",\n",
       "        d3.forceLink(links).id((d) => d.id)\n",
       "      )\n",
       "      // .force(\"charge\", d3.forceManyBody().strength(-200))\n",
       "      .force(\n",
       "        \"collide\",\n",
       "        d3.forceCollide((d) => nodeSize * 1.5)\n",
       "      )\n",
       "      .force(\"center\", d3.forceCenter(width / 2, height / 2));\n",
       "\n",
       "    // Svg: the main container for rendering the graph.\n",
       "    const svg = d3\n",
       "      .select(CHART_SELECTOR)\n",
       "      .attr(\"viewBox\", [0, 0, width, height]);\n",
       "\n",
       "    // svg_g: The 'g' within svg, which seems to make zooming/panning smoother\n",
       "    const svg_g = svg.append(\"g\");\n",
       "    const defs = svg_g.append(\"defs\");\n",
       "\n",
       "    // Marker: a definition for the triangles appearing at the end of each link.\n",
       "    const marker = defs\n",
       "      .selectAll(\"marker\")\n",
       "      .data([\"type_1\"])\n",
       "      .enter()\n",
       "      .append(\"svg:marker\")\n",
       "      .attr(\"id\", function (d, i) {\n",
       "        return `marker_${i}`;\n",
       "      })\n",
       "      .attr(\"viewBox\", \"0 -5 10 10\")\n",
       "      .attr(\"refX\", 10)\n",
       "      .attr(\"refY\", 0)\n",
       "      .attr(\"markerWidth\", 6)\n",
       "      .attr(\"markerHeight\", 6)\n",
       "      .attr(\"orient\", \"auto\")\n",
       "      .append(\"svg:path\")\n",
       "      .attr(\"fill\", \"#444\")\n",
       "      .attr(\"d\", \"M0,-5L10,0L0,5\");\n",
       "\n",
       "    // Link: The links between nodes.\n",
       "    const link = svg_g\n",
       "      .append(\"g\")\n",
       "      .attr(\"id\", \"links\")\n",
       "      .attr(\"fill\", \"none\")\n",
       "      .attr(\"stroke-width\", 1.5)\n",
       "      .selectAll(\"path\")\n",
       "      .data(links)\n",
       "      .enter()\n",
       "      .append(\"path\")\n",
       "      .attr(\"stroke\", \"#444\")\n",
       "      .attr(\"stroke-width\", 2)\n",
       "      .attr(\"marker-end\", function (d, i) {\n",
       "        return `url(\"#marker_0\")`;\n",
       "      })\n",
       "      // 'd' attr is calculated on tick, not here.\n",
       "\n",
       "    /**\n",
       "     * When the mouse is unclicked, clear the selectedNode.\n",
       "     * This will remove the 'hide' class from all nodes in the tick function.\n",
       "     */\n",
       "    function deselectNode() {\n",
       "      selectedNode = null;\n",
       "    }\n",
       "\n",
       "    /**\n",
       "     * When a node is clicked, update selectedNode to reflect the id of that node,\n",
       "     * as well as the id of all nodes and links connected to it.\n",
       "     * @param  {[type]} d [description]\n",
       "     * @param  {[type]} i [description]\n",
       "     * @return {Object}   The selectedNode\n",
       "     *                    (baseNodeId, connectedNodeIds, connectedLinkIds).\n",
       "     */\n",
       "    function selectNode(d, i) {\n",
       "      let connectedNodeIds = new Set();\n",
       "      let connectedLinkIds = new Set();\n",
       "      for (let link of links) {\n",
       "        if (link.source.index === i.index || link.target.index === i.index) {\n",
       "          connectedLinkIds.add(link.index);\n",
       "          if (link.source.index === i.index) {\n",
       "            connectedNodeIds.add(link.target.index);\n",
       "          } else if (link.target.index === i.index) {\n",
       "            connectedNodeIds.add(link.source.index);\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      selectedNode = {\n",
       "        baseNodeId: i.index,\n",
       "        connectedNodeIds: connectedNodeIds,\n",
       "        connectedLinkIds: connectedLinkIds,\n",
       "      };\n",
       "    }\n",
       "\n",
       "    // Node: The nodes.\n",
       "    const node = svg_g\n",
       "      .append(\"g\")\n",
       "      .attr(\"stroke-width\", 3)\n",
       "      .selectAll(\"circle\")\n",
       "      .data(nodes)\n",
       "      .join(\"circle\")\n",
       "      .attr(\"class\", \"node\")\n",
       "      .attr(\"r\", nodeSize)\n",
       "      .attr(\"fill\", getColour)\n",
       "      .attr(\"stroke\", function (d) {\n",
       "        return lightenDarkenColor(getColour(d), -15);\n",
       "      })\n",
       "      .on(\"mousedown\", selectNode)\n",
       "      .call(drag(simulation));\n",
       "\n",
       "    // When a node is hovered over, set the tooltip accordingly.\n",
       "    // Clear the tooltip when the mouse leaves a node.\n",
       "    d3.selectAll(\".node\").on(\"mouseover\", setTooltip);\n",
       "    d3.selectAll(\".node\").on(\"mouseleave\", clearTooltip);\n",
       "\n",
       "    // When mouse up event (or pointer up event) fires, deselect the\n",
       "    // selected node if there is one.\n",
       "    document.addEventListener(\"mouseup\", deselectNode);\n",
       "    document.addEventListener(\"pointerup\", deselectNode);\n",
       "\n",
       "    // Text: The text appearing on the nodes.\n",
       "    const text = svg_g\n",
       "      .append(\"g\")\n",
       "      .selectAll(\"text\")\n",
       "      .data(nodes)\n",
       "      .enter()\n",
       "      .append(\"text\")\n",
       "      .text((d) => d.name)\n",
       "      .attr(\"font-size\", 12)\n",
       "      .attr(\"text-anchor\", \"middle\")\n",
       "      .attr(\"dominant-baseline\", \"central\");\n",
       "\n",
       "    // Links text: The text appearing on the links.\n",
       "    const link_text = svg_g\n",
       "      .append(\"g\")\n",
       "      .selectAll(\"text\")\n",
       "      .data(links)\n",
       "      .enter()\n",
       "      .append(\"text\")\n",
       "      .text((d) => d.type)\n",
       "      .attr(\"font-size\", 10)\n",
       "      .attr(\"text-anchor\", \"middle\")\n",
       "      .attr(\"dominant-baseline\", \"central\");\n",
       "\n",
       "    // Every time the simulation ticks, update the path of the links,\n",
       "    // the location of the nodes, the location of the text, and also the\n",
       "    // tooltip position.\n",
       "    simulation.on(\"tick\", () => {\n",
       "\n",
       "      // Only draw the links starting from the edge of node 1 to node 2.\n",
       "      // This way, when the nodes go transparent, the links won't be visible underneath\n",
       "      // the nodes (which looks bad).\n",
       "      // Code for doing this was found here:\n",
       "      // https://www.appsloveworld.com/d3js/100/17/\n",
       "      // create-links-from-node-border-to-node-border-not-center-to-center\n",
       "      link.attr(\n",
       "        \"d\",\n",
       "        (d) => {\n",
       "          x1 = d.source.x + (Math.cos(Math.atan2(d.target.y - d.source.y, d.target.x - d.source.x)) * nodeSize);\n",
       "\n",
       "          y1 = d.source.y + (Math.sin(Math.atan2(d.target.y - d.source.y, d.target.x - d.source.x)) * nodeSize);\n",
       "\n",
       "          x2 = d.target.x - (Math.cos(Math.atan2(d.target.y - d.source.y, d.target.x - d.source.x)) * nodeSize);\n",
       "\n",
       "          y2 = d.target.y - (Math.sin(Math.atan2(d.target.y - d.source.y, d.target.x - d.source.x)) * nodeSize);\n",
       "          return `M${x1},${y1}A0,0 0 0,1 ${x2},${y2}`\n",
       "        }\n",
       "      );\n",
       "\n",
       "      node.attr(\"cx\", (d) => d.x).attr(\"cy\", (d) => d.y);\n",
       "      text.attr(\"x\", (d) => Math.floor(d.x)).attr(\"y\", (d) => Math.floor(d.y));\n",
       "      link_text\n",
       "        .attr(\"x\", (d) => (d.source.x + d.target.x) / 2)\n",
       "        .attr(\"y\", (d) => (d.source.y + d.target.y) / 2);\n",
       "      updateTooltipPosition();\n",
       "\n",
       "      // If a node is currently selected, add a class to the nodes that are not\n",
       "      // related to that node.\n",
       "      //\n",
       "\n",
       "      node.classed(\"not-selected\", function (d, i) {\n",
       "        return (\n",
       "          selectedNode &&\n",
       "          selectedNode.baseNodeId !== i &&\n",
       "          !selectedNode.connectedNodeIds.has(i)\n",
       "        );\n",
       "      });\n",
       "\n",
       "      text.classed(\"not-selected\", function (d, i) {\n",
       "        return (\n",
       "          selectedNode &&\n",
       "          selectedNode.baseNodeId !== i &&\n",
       "          !selectedNode.connectedNodeIds.has(i)\n",
       "        );\n",
       "      });\n",
       "\n",
       "      link.classed(\"not-selected\", function (d, i) {\n",
       "        return selectedNode && !selectedNode.connectedLinkIds.has(i);\n",
       "      });\n",
       "\n",
       "      link_text.classed(\"not-selected\", function (d, i) {\n",
       "        return selectedNode && !selectedNode.connectedLinkIds.has(i);\n",
       "      });\n",
       "    });\n",
       "\n",
       "    // Register the zoom handler.\n",
       "    let zoom = d3.zoom().on(\"zoom\", handleZoom);\n",
       "\n",
       "    /**\n",
       "     * A function to handle zooming/panning. Sets a transform on the svg_g container\n",
       "     * according to the zoom/pan level.\n",
       "     * @param  {[type]} e [description]\n",
       "     * @return {[type]}   [description]\n",
       "     */\n",
       "    function handleZoom(e) {\n",
       "      transform = e.transform;\n",
       "      svg_g.attr(\"transform\", e.transform);\n",
       "      text.attr(\"font-size\", 12 / e.transform.k ** 0.7);\n",
       "      link_text.attr(\"font-size\", 10 / e.transform.k ** 0.7);\n",
       "    }\n",
       "\n",
       "    // Initialise the zooming.\n",
       "    function initZoom() {\n",
       "      svg.call(zoom);\n",
       "    }\n",
       "\n",
       "    initZoom();\n",
       "\n",
       "    // If dummyData is present (i.e. dev mode), display a message at the bottom.\n",
       "    if (dummyData) {\n",
       "      let div = document.createElement(\"div\");\n",
       "      div.innerHTML =\n",
       "        \"<h3>Note: currently in development mode using dummy data.</h3>\";\n",
       "      document.body.appendChild(div);\n",
       "    }\n",
       "  });\n",
       "</script>\n",
       "<!-- <script src=\"graphVisualisation.js\"></script> -->\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gqvis.visualise_cypher(\"\"\"\n",
    "MATCH (d:Document)<-[a:APPEARS_IN]-(o:Observation {name: \"leak\"})<-[r:HAS_OBSERVATION]-(e:Entity)\n",
    "RETURN d, a, o, r, e\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our queries can also incorporate structured data, such as the start dates of the work orders. We have not added structured data to our graph for simplicity, but if we stored dates as properties on the `Document` nodes, we could run this type of query.\n",
    "\n",
    "Here is an example query for __all assets that had leaks from 25 to 28 July__:\n",
    "\n",
    "    MATCH (d:Document)<-[a:APPEARS_IN]-(e:Entity)-[r:HAS_OBSERVATION]->(o:observation {name: \"leak\"})-[:APPEARS_IN]->(d)\n",
    "    WHERE d.StartDate >= 20050725\n",
    "    AND d.StartDate <= 20050728\n",
    "    RETURN e, r, o\n",
    "\n",
    "On a larger graph this would also work well with other forms of structured data such as costs. We could query based on specific asset costs, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Where to go from here\n",
    "\n",
    "Feel free to use/adapt any of this code to build your own knowledge graphs. You might like to try running it on your own datasets, or designing your own `NERModel` or `REModel`.\n",
    "\n",
    "## 8.1. Improving the lexical normalisation model\n",
    "\n",
    "We only briefly touched on the lexical normalisation component of Knowledge Graph Construction from Text. There are plenty of neural models for lexical normalisation available that yield much better performance than our lexicon-based tagger.\n",
    "\n",
    "We have also developed a tool to support the rapid creation of training data for lexical normalisation - you can learn about it [here](https://aclanthology.org/2021.emnlp-demo.25/).\n",
    "\n",
    "## 8.2. Incorporating other structured data into the graph\n",
    "\n",
    "Graph databases are excellent at bringing together data from a wide range of sources. In a maintenance setting, there are two particular types of structured data that can be easily added to this knowledge graph schema: Downtime events, and Functional Locations.\n",
    "\n",
    "### Downtime events\n",
    "\n",
    "A downtime event is a point in time in which an asset is not operational. These events typically have costs and dates associated with them, and can be associated with particular `Item` entities.\n",
    "\n",
    "By modelling both work orders and downtime events in one graph, we can make queries about downtime events. Here is an example query for the __downtime events associated with assets appearing in work orders from 25 to 28 July (where the downtime events occurred in July)__:\n",
    "\n",
    "    MATCH (d:WorkOrder)<-[a:APPEARS_IN]-(e:Entity)-[r:HAS_EVENT]->(x:DowntimeEvent)\n",
    "    WHERE d.StartDate > 20050725\n",
    "    AND d.StartDate < 20050728\n",
    "    AND 20050700 <= x.StartDate <= 20050731\n",
    "    RETURN e, r, x\n",
    "\n",
    "We can of course extend this to specific assets, such as pumps:\n",
    "\n",
    "    MATCH (d:WorkOrder)<-[a:APPEARS_IN]-(e:Entity {name: \"pump\"})-[r:HAS_EVENT]->(x:DowntimeEvent)\n",
    "    WHERE d.StartDate > 20050725\n",
    "    AND d.StartDate < 20050728\n",
    "    AND 20050700 <= x.StartDate <= 20050731\n",
    "    RETURN e, r, x\n",
    "\n",
    "In larger graphs the downtime events could even be further queried based on duration, cost, lost feed, or date ranges.\n",
    "\n",
    "### Functional Locations (FLOCs)\n",
    "\n",
    "You may have noticed that our original `work_order_data.csv` has a column called \"FLOC\". This is the functional location of the asset being maintained. In the maintenance domain, this is often of greater interest to reliability engineers than the individual `Item` entities, and thus it would be ideal to create nodes to represent these functional locations in the graph. This way, we could run queries on the failure modes associated with particular FLOCs.\n",
    "\n",
    "If you are interested in continuing work on this small graph, the next best step would be to create nodes for the functional location data (`floc_data`) and to link the downtime events to those nodes as opposed to the Item nodes.\n",
    "\n",
    "![alt text](images/adding-flocs.png \"Adding FLOCs\")\n",
    "\n",
    "## 8.3. Consolidating the training data for NER+RE\n",
    "\n",
    "You may have noticed that our training data is split into two parts for the NER and RE tasks, i.e. the NER is in CONLL format and the RE is in tabular format. It is possible to put both the NER and RE training data into a single file, using the mention format we showed previously. For example, each row of your training data could look like this:\n",
    "\n",
    "    { tokens: [<list of tokens>], mentions: [<list of mentions>], relations: [<list of relations>] }\n",
    "    \n",
    "... and you could have a script to 'wrangle' this into the CONLL and tabular format before feeding them into the NER and RE model respectively. If you use a tool like [QuickGraph](https://aclanthology.org/2022.acl-demo.27/) your data will be in a similar format to the above.\n",
    "\n",
    "## 8.4. From Pipeline to End-to-End Knowledge Graph Construction from Text\n",
    "\n",
    "We have presented a \"pipeline\" for KGC here in this notebook. We wrote the notebook this way in order to be able to discuss each of the components (Lexical Normalisation, Named Entity Recognition and Relation Extraction) in isolation, thus making them easier to understand.\n",
    "\n",
    "However, the current state of the art in NLP/TLP is moving away from pipeline-based KGC models and towards end-to-end neural models, i.e. a single neural model that performs all of these steps simultaneously. If you are interested in learning about this, you might like to read some of the following papers:\n",
    "\n",
    "> Stewart, M., & Liu, W. (2020, July). Seq2kg: an end-to-end neural model for domain agnostic knowledge graph (not text graph) construction from text. In   Proceedings of the International Conference on Principles of Knowledge Representation and Reasoning (Vol. 17, No. 1, pp. 748-757).\n",
    "\n",
    "> Eberts, M., & Ulges, A. (2019). Span-based joint entity and relation extraction with transformer pre-training. arXiv preprint arXiv:1909.07755.\n",
    "\n",
    "> Cabot, P. L. H., & Navigli, R. (2021, November). REBEL: Relation extraction by end-to-end language generation. In Findings of the Association for Computational Linguistics: EMNLP 2021 (pp. 2370-2381)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## GQVis on Jupyter Lab\n",
    "\n",
    "GQVis works out of the box on Jupyter Notebook, but to get it working in Jupyter Lab, you'll need to run the following command prior to starting Jupyter lab:\n",
    "\n",
    "    jupyter labextension install jupyterlab_requirejs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
