{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to construct a simple knowledge graph using Python, and run some queries on the graph in Neo4j.\n",
    "\n",
    "If you would like to run this code yourself, you will need to install the `py2neo` package in Python 3.\n",
    "\n",
    "To run part 3 onwards, you will need to install Neo4j, which can be downloaded at https://neo4j.com/download/.\n",
    "\n",
    "I will be running through the code during part 2 of the master class so there is no need to install anything unless you would also like to try the code out yourself and run some graph queries.\n",
    "\n",
    "# 1. Read in the data\n",
    "\n",
    "Before we can build a graph, we must first read in the example datasets:\n",
    "\n",
    "- `work_order_file`: A csv file containing a set of work orders.\n",
    "\n",
    "Here is an example of what the first few rows of each dataset look like:\n",
    "\n",
    "![alt text](images/example-data.png \"Example datasets\")\n",
    "\n",
    "TODO: Remove downtime events completely\n",
    "\n",
    "We are using the simple `csv` library to read in the data, though this can also be done using `pandas`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing required packages\n",
    "\n",
    "To run this notebook you will need to install the following via pip\n",
    ":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting the data\n",
    "\n",
    "Let's start by inspecting the CSV dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('StartDate', '10/07/2005'), ('FLOC', '1234.1.1'), ('ShortText', 'repair cracked hyd tank')])\n",
      "OrderedDict([('StartDate', '14/07/2005'), ('FLOC', '1234.1.2'), ('ShortText', 'engine wont start')])\n",
      "OrderedDict([('StartDate', '17/07/2005'), ('FLOC', '1234.1.3'), ('ShortText', 'a/c blowing hot air')])\n",
      "OrderedDict([('StartDate', '20/07/2005'), ('FLOC', '1234.1.2'), ('ShortText', 'engin u/s')])\n",
      "OrderedDict([('StartDate', '21/07/2005'), ('FLOC', '1234.1.2'), ('ShortText', 'fix engine')])\n",
      "OrderedDict([('StartDate', '22/07/2005'), ('FLOC', '1234.1.4'), ('ShortText', 'pump service')])\n",
      "OrderedDict([('StartDate', '23/07/2005'), ('FLOC', '1234.1.4'), ('ShortText', 'pump leak')])\n",
      "OrderedDict([('StartDate', '24/07/2005'), ('FLOC', '1234.1.4'), ('ShortText', 'fix leak on pump')])\n",
      "OrderedDict([('StartDate', '25/07/2005'), ('FLOC', '1234.1.2'), ('ShortText', 'engine not running')])\n",
      "OrderedDict([('StartDate', '26/07/2005'), ('FLOC', '1234.1.2'), ('ShortText', 'engine has problems starting')])\n",
      "OrderedDict([('StartDate', '27/07/2005'), ('FLOC', '1234.1.4'), ('ShortText', 'pump fault')])\n",
      "OrderedDict([('StartDate', '28/07/2005'), ('FLOC', '1234.1.4'), ('ShortText', 'pump leaking')])\n",
      "OrderedDict([('StartDate', '29/07/2005'), ('FLOC', '1234.1.3'), ('ShortText', 'a/c not working')])\n",
      "OrderedDict([('StartDate', '30/07/2005'), ('FLOC', '1234.1.3'), ('ShortText', 'a/c broken')])\n"
     ]
    }
   ],
   "source": [
    "from csv import DictReader\n",
    "\n",
    "work_order_file = \"data/sample_work_orders.csv\"\n",
    "\n",
    "# A simple function to read in a csv file and return a list,\n",
    "# where each element in the list is a dictionary of {heading : value}\n",
    "def load_csv(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = DictReader(f)\n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "    return data\n",
    "\n",
    "        \n",
    "work_order_data = load_csv(work_order_file)\n",
    "\n",
    "for row in work_order_data:\n",
    "    print(row)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Named Entity Recognition\n",
    "\n",
    "Our first task is to extract the entities in the short text descriptions and construct nodes from those entities. This is how we are able to unlock the knowledge captured within the short text and combine it with the structured fields.\n",
    "\n",
    "![alt text](images/extracting-entities-v2.png \"Extracting entities\")\n",
    "\n",
    "## 2.1. Inspecting the data\n",
    "\n",
    "We can also inspect the dataset we will use to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3200 documents from data/ner_dataset\\train.txt.\n",
      "{'tokens': ['ram', 'on', 'cup', 'rod', 'support', 'broken'], 'labels': ['B-Item', 'B-Location', 'B-Item', 'B-Item', 'I-Item', 'B-Observation']}\n"
     ]
    }
   ],
   "source": [
    "NER_DATASETS_PATH = \"data/ner_dataset\"\n",
    "\n",
    "\n",
    "def to_conll_document(s: str):\n",
    "    \"\"\"Create a ConllDocument from a string as it appears\n",
    "    in a Conll-formatted file.\n",
    "\n",
    "    Args:\n",
    "        s (str): A string, separated by newlines, where each\n",
    "        line is a token, then a comma and space, then a label.\n",
    "\n",
    "    Returns:\n",
    "        ConllDocument: A ConllDocument created from s.\n",
    "    \"\"\"\n",
    "    tokens, labels = [], []\n",
    "    for line in s.split(\"\\n\"):\n",
    "        if len(line.strip()) == 0:\n",
    "            continue\n",
    "        token, label = line.split()\n",
    "\n",
    "        tokens.append(token)\n",
    "        labels.append(label)\n",
    "    return {'tokens': tokens, 'labels': labels}\n",
    "\n",
    "\n",
    "def load_conll_dataset(filename: str) -> list:\n",
    "    \"\"\"Load a list of documents from the given CONLL-formatted dataset.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The filename to load from.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of documents.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        docs = f.read().split(\"\\n\\n\")\n",
    "        for d in docs:\n",
    "            if len(d) == 0:\n",
    "                continue\n",
    "            document = to_conll_document(d)\n",
    "            documents.append(document)\n",
    "    print(f\"Loaded {len(documents)} documents from {filename}.\")\n",
    "    return documents\n",
    "\n",
    "train_dataset = load_conll_dataset(os.path.join(NER_DATASETS_PATH, 'train.txt'))\n",
    "\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write code to inspect the training data (CONLL format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Define an abstract base class for NER Models\n",
    "\n",
    "(explanation about ABCs and the four methods we are going to use in all our NER models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Abstract base class for the NER Model. \"\"\"\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class NERModel(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, conll_datasets_path: str):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def inference(self, dataset_filename: str):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load(self, model_path):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save(self, model_path):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Define a Flair-based NER Model class\n",
    "\n",
    "In this tutorial we will use [Flair](https://github.com/flairNLP/flair), which simplifies the process of building a deep learning model for a variety of NLP tasks.\n",
    "\n",
    "The code below is a class representing a `FlairNERModel`, which is based on the `NERModel` class above. It has the same four methods, i.e `train()`, `inference()`, `load()`, and `save()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flair'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-d93e81e0eb1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mflair\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mflair\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mflair\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mColumnCorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'flair'"
     ]
    }
   ],
   "source": [
    "\"\"\"A Flair-based Named Entity Recognition model. Learns to predict entity\n",
    "classes via deep learning.\"\"\"\n",
    "\n",
    "\n",
    "# TODO: Tidy up, fix this code as it does not work atm in this notebook\n",
    "\n",
    "\n",
    "import os\n",
    "import flair\n",
    "from flair.data import Corpus, Sentence\n",
    "from flair.datasets import ColumnCorpus\n",
    "from flair.embeddings import (\n",
    "    StackedEmbeddings,\n",
    "    FlairEmbeddings,\n",
    ")\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer\n",
    "from typing import List\n",
    "from flair.visual.training_curves import Plotter\n",
    "import torch\n",
    "\n",
    "\n",
    "from .NERModel import NERModel\n",
    "\n",
    "# TODO: Get rid of ConllDataset/ConllDocument and just use lists\n",
    "from mwo2kg_datasets import (\n",
    "    ConllDataset,\n",
    "    ConllDocument,\n",
    ")\n",
    "\n",
    "HIDDEN_SIZE = 256\n",
    "\n",
    "# Check whether CUDA is available and set the device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    flair.device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    flair.device = torch.device(\"cpu\")\n",
    "print(\"Device:\", flair.device)\n",
    "\n",
    "\n",
    "class FlairNERModel(NERModel):\n",
    "\n",
    "    model_name: str = \"Flair\"\n",
    "\n",
    "    \"\"\"A Flair-based Named Entity Recognition model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FlairNERModel, self).__init__()\n",
    "\n",
    "        self.model = None\n",
    "\n",
    "    def train(self, datasets_path: os.path, trained_model_path: os.path):\n",
    "        \"\"\" Train the Flair model on the given conll datasets.\n",
    "\n",
    "        Args:\n",
    "            datasets_path (os.path): The folder containing the\n",
    "              train, dev and text CONLL-formatted datasets.\n",
    "            trained_model_path (os.path): The folder to save the trained\n",
    "              model to.\n",
    "        \"\"\"\n",
    "\n",
    "        columns = {0: \"text\", 1: \"ner\"}\n",
    "        corpus: Corpus = ColumnCorpus(\n",
    "            datasets_path,\n",
    "            columns,\n",
    "            train_file=\"train.txt\",\n",
    "            dev_file=\"dev.txt\",\n",
    "            test_file=\"test.txt\",\n",
    "        )\n",
    "        label_dict = corpus.make_label_dictionary(label_type=\"ner\")\n",
    "\n",
    "        # Train the sequence tagger\n",
    "        embedding_types = [\n",
    "            FlairEmbeddings(\"mix-forward\"),\n",
    "            FlairEmbeddings(\"mix-backward\"),\n",
    "        ]\n",
    "\n",
    "        embeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "\n",
    "        tagger = SequenceTagger(\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            embeddings=embeddings,\n",
    "            tag_dictionary=label_dict,\n",
    "            tag_type=\"ner\",\n",
    "            use_crf=True,\n",
    "        )\n",
    "\n",
    "        trainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "        sm = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            sm = \"gpu\"\n",
    "        trainer.train(\n",
    "            trained_model_path,\n",
    "            learning_rate=0.1,\n",
    "            mini_batch_size=32,\n",
    "            max_epochs=10,\n",
    "            embeddings_storage_mode=sm,\n",
    "        )\n",
    "\n",
    "        plotter = Plotter()\n",
    "        plotter.plot_weights(os.path.join(trained_model_path, \"weights.txt\"))\n",
    "\n",
    "        self.load(trained_model_path)\n",
    "\n",
    "    def inference(self, raw_sents: list) -> ConllDataset:\n",
    "        \"\"\"Run the inference on a given list of short texts.\n",
    "\n",
    "        Args:\n",
    "            raw_sents (list): The list of raw sents to run the inference on.\n",
    "\n",
    "        Returns:\n",
    "            ConllDataset: The ConllDataset of preds.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the model has not yet been trained.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\n",
    "                \"The KGC Model has not yet been trained. \"\n",
    "                \"Please train this Flair model before proceeding.\"\n",
    "            )\n",
    "\n",
    "        preds_dataset = ConllDataset()\n",
    "\n",
    "        for i, tokens in enumerate(raw_sents):\n",
    "            labels = self._tag_sentence(tokens)\n",
    "            doc = ConllDocument(tokens, labels)\n",
    "            preds_dataset.add_document(doc)\n",
    "\n",
    "        return preds_dataset\n",
    "\n",
    "    def load(self, model_path: os.path):\n",
    "        \"\"\"Load the model from the specified path.\n",
    "\n",
    "        Args:\n",
    "            model_path (os.path): The path to load.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the path does not exist i.e. model not yet trained.\n",
    "        \"\"\"\n",
    "        model_file = os.path.join(model_path, \"final-model.pt\")\n",
    "        if not os.path.exists(model_file):\n",
    "            raise ValueError(\n",
    "                \"The NER Model has not yet been trained (the Flair resource \"\n",
    "                \"files are missing).\"\n",
    "            )\n",
    "        self.model = SequenceTagger.load(model_file)\n",
    "\n",
    "    def save(self, model_path):\n",
    "        \"\"\"Flair model does not need a save function -\n",
    "        it saves after training.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _tag_sentence(self, sentence: List[str]) -> List[str]:\n",
    "        \"\"\"Tag the given sentence (list of tokens) via the model.\n",
    "\n",
    "        Args:\n",
    "            sentence (List[str]): A list of tokens.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of labels.\n",
    "        \"\"\"\n",
    "        sentence_obj = Sentence(sentence, use_tokenizer=False)\n",
    "        self.model.predict(sentence_obj)\n",
    "        labels = [\"O\"] * len(sentence)\n",
    "\n",
    "        for entity in sentence_obj.get_spans(\"ner\"):\n",
    "            for i, token in enumerate(entity):\n",
    "                label = entity.get_label(\"ner\").value\n",
    "                prefix = \"B-\" if i == 0 else \"I-\"\n",
    "                \n",
    "                # Token idx starts from 1 in Flair.\n",
    "                labels[token.idx - 1] = prefix + label\n",
    "\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) Define a DictionaryNERModel class\n",
    "\n",
    "If for some reason you are not able to use the Flair library (not enough space, computer not powerful enough etc), here is a simple model you can use to extract the entities, albeit with a much weaker performance. This one scans the training data, builds a mapping between each phrase (one or more tokens in a row) and the most common entity type associated with that phrase, then uses that entity type as the prediction when seeing that token in the test data.\n",
    "\n",
    "#### TODO: Move this into a separate python script and just import it via `import DictionaryNERModel`. No point having it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" A dictionary-based NER model. Can be used as an alternative\n",
    "to Flair, which is cumbersome to run and install.\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle as pkl\n",
    "\n",
    "\n",
    "class DictionaryNERModel2183(NERModel):\n",
    "\n",
    "    \"\"\"The dictionary-based NER model. Labels sentences based on\n",
    "    the most frequent label assigned to each phrase as per the\n",
    "    training dataset.\n",
    "\n",
    "    Attributes:\n",
    "        _chunked_frequency_dict (dict): A dict to keep track of the\n",
    "          most frequent label for each phrase, for each phrase length.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name: str = \"Dictionary-based\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DictionaryNERModel2183, self).__init__()\n",
    "        self._chunked_frequency_dict = None\n",
    "\n",
    "    def train(self, datasets_path: os.path, trained_model_path: os.path):\n",
    "        \"\"\" \"Train\" the dictionary model on the given conll datasets.\n",
    "        Note it isn't actually training... just building a dictionary\n",
    "        from the training/dev datasets and using it as a means to\n",
    "        heuristically tag the test sents.\n",
    "\n",
    "        Args:\n",
    "            datasets_path (os.path): The folder containing the\n",
    "              train, dev and text CONLL-formatted datasets.\n",
    "            trained_model_path (os.path): The folder to store the\n",
    "              'trained model' i.e. the freq dict etc.\n",
    "        \"\"\"\n",
    "        print(\"Building dictionary...\")\n",
    "        conll_dataset = self._load_conll_data(datasets_path)\n",
    "        frequency_dict = self._build_frequency_dict(conll_dataset)\n",
    "\n",
    "        self._chunked_frequency_dict = self._chunk_frequency_dict(\n",
    "            frequency_dict\n",
    "        )\n",
    "\n",
    "        self.save(trained_model_path)\n",
    "\n",
    "    def load(self, model_path):\n",
    "        \"\"\"Load the chunked frequency dict from the given folder.\n",
    "\n",
    "        Args:\n",
    "            model_path (str): The filename containing the chunked\n",
    "              frequency dict (pickle file).\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the model.pkl file is missing, i.e.\n",
    "              model has not been trained.\n",
    "        \"\"\"\n",
    "        trained_model_path = os.path.join(model_path, \"model.pkl\")\n",
    "        if not os.path.exists(trained_model_path) or not os.path.exists(\n",
    "            model_path\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"The KGC Model has not yet been trained (the model.pkl\"\n",
    "                \" file is missing).\"\n",
    "            )\n",
    "\n",
    "        with open(trained_model_path, \"rb\") as f:\n",
    "            self._chunked_frequency_dict = pkl.load(f)\n",
    "\n",
    "    def save(self, model_path: os.path):\n",
    "        \"\"\"Save the chunked frequency dict inside the given folder.\n",
    "\n",
    "        Args:\n",
    "            model_path (os.path): The folder to save the chunked\n",
    "              frequency dict (pickle file).\n",
    "        \"\"\"\n",
    "        with open(os.path.join(model_path, \"model.pkl\"), \"wb\") as f:\n",
    "            pkl.dump(self._chunked_frequency_dict, f)\n",
    "\n",
    "    def inference(self, raw_sents: list) -> list:\n",
    "        \"\"\"Run the inference on a given list of short texts.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the model has not yet been trained.\n",
    "\n",
    "        Args:\n",
    "            raw_sents (list): The list of raw sents to run the inference on.\n",
    "\n",
    "        Returns:\n",
    "            list: The list of documents with predictions.\n",
    "        \"\"\"\n",
    "        if self._chunked_frequency_dict is None:\n",
    "            raise ValueError(\n",
    "                \"This Dictionary model is not trained yet. \"\n",
    "                \"Please run the train function before proceeding.\"\n",
    "            )\n",
    "\n",
    "        preds = []\n",
    "        \n",
    "        min_words = min(self._chunked_frequency_dict.keys())\n",
    "        max_words = max(self._chunked_frequency_dict.keys()) + 1\n",
    "        cfd = self._chunked_frequency_dict\n",
    "\n",
    "        for sent in raw_sents:\n",
    "            tokens = sent\n",
    "            labels = [\"O\"] * len(sent)\n",
    "\n",
    "            for i, t in enumerate(tokens):\n",
    "\n",
    "                # If label already predicted (by a larger term),\n",
    "                # move on.\n",
    "                if labels[i] != \"O\":\n",
    "                    continue\n",
    "\n",
    "                # Go through each number of words (in reverse order).\n",
    "                # Check each chunk of words to see whether they are in\n",
    "                # the cfd. If so, set the labels accordingly.\n",
    "                for j in reversed(range(min_words, max_words)):\n",
    "                    if j not in cfd:\n",
    "                        continue\n",
    "                    if (i + j) > len(tokens):\n",
    "                        continue\n",
    "                    token_str = \" \".join(tokens[i : i + j])\n",
    "\n",
    "                    if token_str in cfd[j]:\n",
    "                        base_class = cfd[j][token_str]\n",
    "                        labels[i] = \"B-\" + base_class\n",
    "                        for x in range((i + 1), (i + j)):\n",
    "                            labels[x] = \"I-\" + base_class                            \n",
    "                            i+= 1 # Skip ahead so the labels are not overwritten\n",
    "                        \n",
    "\n",
    "            # Create a ConllDocument from these tokens and labels and append.\n",
    "            doc = {\"tokens\": tokens, \"labels\": labels}\n",
    "            preds.append(doc)\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def _load_conll_data(self, datasets_path: str) -> dict:\n",
    "        \"\"\"Load the CONLL-formatted data from the given folder.\n",
    "        Only loads train and dev, as loading test would give it an unfair\n",
    "        advantage vs other models.\n",
    "\n",
    "        Args:\n",
    "            datasets_path (str): The folder containing the three\n",
    "              CONLL-formatted files (train.txt, dev.txt, test.txt)\n",
    "\n",
    "        Returns:\n",
    "            list: A list of all documents in the training and dev sets. Each doc is\n",
    "            represented as a dict, i.e.\n",
    "            {tokens: [list of tokens], labels: [list of labels])}.\n",
    "        \"\"\"\n",
    "        conll_dataset = []\n",
    "        for ds_name in [\"train\", \"dev\"]:\n",
    "            ds = load_conll_dataset(os.path.join(datasets_path, f\"{ds_name}.txt\"))\n",
    "            for doc in ds:\n",
    "                conll_dataset.append(doc)\n",
    "        return conll_dataset\n",
    "\n",
    "    def _chunk_frequency_dict(self, frequency_dict: dict) -> dict:\n",
    "        \"\"\"Chunk the frequency dict, i.e. split it into a dict where each\n",
    "        key is the number of words in each phrase, and then each item in that\n",
    "        key is the most commonly occurring label for that word, e.g.\n",
    "        1: {\n",
    "          \"pump\": \"Item\"\n",
    "        },\n",
    "        2: {\n",
    "          \"big pump\": \"Item\"\n",
    "        }\n",
    "\n",
    "        Args:\n",
    "            frequency_dict (dict): The non-chunked frequency dict.\n",
    "        \"\"\"\n",
    "        _chunked_frequency_dict = {}\n",
    "        for (phrase, label_freqs) in frequency_dict.items():\n",
    "            num_words = len(phrase.split(\" \"))\n",
    "            label = max(label_freqs, key=label_freqs.get)\n",
    "            if num_words not in _chunked_frequency_dict:\n",
    "                _chunked_frequency_dict[num_words] = {}\n",
    "            _chunked_frequency_dict[num_words].update({phrase: label})\n",
    "\n",
    "        return _chunked_frequency_dict\n",
    "\n",
    "    def _build_frequency_dict(self, conll_dataset: list):\n",
    "        \"\"\"Build a dictionary of the frequency of each token mapping to\n",
    "        each label in the given Redcoat dataset.\n",
    "\n",
    "        Args:\n",
    "            conll_dataset (list): The Conll dataset to build the\n",
    "              dict from.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dict mapping each entity mention to a dict of {type:\n",
    "              frequency}.\n",
    "        \"\"\"\n",
    "        frequency_dict = {}\n",
    "\n",
    "        for doc in conll_dataset:\n",
    "            phrase_labels = _get_phrase_labels(doc)\n",
    "            for (phrase, label) in phrase_labels:\n",
    "                phrase_str = \" \".join(phrase)\n",
    "\n",
    "                if phrase_str not in frequency_dict:\n",
    "                    frequency_dict[phrase_str] = {}\n",
    "                if label not in frequency_dict[phrase_str]:\n",
    "                    frequency_dict[phrase_str][label] = 0\n",
    "                frequency_dict[phrase_str][label] += 1\n",
    "\n",
    "        return frequency_dict\n",
    "\n",
    "    \n",
    "def _get_phrase_labels(doc: dict):\n",
    "    \"\"\"Return a list of (phrases, labels) for each mention\n",
    "    in a doc (which is a dict of {\"tokens\": [tokens in the doc], \"labels\": [labels of the doc]}.\n",
    "    Each phrase is a list of words of that label, i.e.\n",
    "    [['centrifugal', 'pump'], 'Item']\n",
    "\n",
    "    Returns:\n",
    "        list: A list of (phrase, labels).\n",
    "    \"\"\"\n",
    "    phrase_labels = []\n",
    "    current_phrase = []\n",
    "    for i, (token, label) in enumerate(zip(doc[\"tokens\"], doc[\"labels\"])):\n",
    "        \n",
    "        if label.startswith(\"B-\"):\n",
    "            if len(current_phrase) > 0:\n",
    "                phrase_labels.append((current_phrase, current_label))\n",
    "            current_phrase = [token]\n",
    "            current_label = label[2:]\n",
    "        elif label.startswith(\"I-\"):\n",
    "            current_phrase.append(token)\n",
    "        elif label == \"O\":\n",
    "            if len(current_phrase) > 0:\n",
    "                phrase_labels.append((current_phrase, current_label))\n",
    "            current_phrase = []\n",
    "            current_label = None\n",
    "        if (\n",
    "            i == len(doc[\"tokens\"]) - 1\n",
    "            and label != \"O\"\n",
    "            and len(current_phrase) > 0\n",
    "        ):\n",
    "            phrase_labels.append((current_phrase, current_label))\n",
    "            \n",
    "    return phrase_labels  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Running inference on unseen sentences\n",
    "\n",
    "The next step is to use our trained model to infer the entity type of each entity appearing in a list of previously unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n",
      "Loaded 3200 documents from data/ner_dataset\\train.txt.\n",
      "Loaded 401 documents from data/ner_dataset\\dev.txt.\n",
      "{'tokens': ['a/c', 'not', 'working'], 'labels': ['B-Item', 'B-Observation', 'I-Observation']}\n"
     ]
    }
   ],
   "source": [
    "NER_MODELS_PATH = \"models/ner_models\"\n",
    "\n",
    "\n",
    "# If you are using the DictionaryNERModel:\n",
    "m = DictionaryNERModel2183()\n",
    "\n",
    "# If you are using the FlairNERModel, uncomment this and comment the line above:\n",
    "#m = FlairNERModel()\n",
    "\n",
    "m.train(NER_DATASETS_PATH, os.path.join(NER_MODELS_PATH))\n",
    "\n",
    "tagged_bio_sents = []\n",
    "\n",
    "sentences = []\n",
    "for row in work_order_data:\n",
    "    sentences.append(row[\"ShortText\"].split()) # We must 'tokenise' the sentence first, i.e. split into words\n",
    "\n",
    "tagged_bio_sents = m.inference(sentences)\n",
    "    \n",
    "print(tagged_bio_sents[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Converting the BIO format to the \"Mention\"-based format\n",
    "\n",
    "The BIO-based format above has one key downside - it is not good for representing 'phrases' of more than one token in length. This makes it difficult to work with for future steps, such as constructing nodes from the entities and running relation extraction. In light of this, we will now convert the BIO-formatted predictions into Mention format, i.e. go from this:\n",
    "\n",
    "    {'tokens': ['a/c', 'not', 'working'],\n",
    "     'labels': ['B-Item', 'B-Observation', 'I-Observation']}\n",
    "    \n",
    "To this:\n",
    "\n",
    "    {'tokens': ['a/c', 'not', 'working'],\n",
    "     'mentions': [\n",
    "         {'start': 0, 'labels': ['Item'], 'end': 1},\n",
    "         {'start': 1, 'labels': ['Observation'], 'end': 3}]}\n",
    "    \n",
    "Note that this format is also able to now support multiple labels per mention (though we will only be using single labels for simplicity).\n",
    "\n",
    "This step is just a bit of data wrangling - here we have defined a helper function to convert a BIO-tagged sentence into a Mention-tagged sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['a/c', 'not', 'working'], 'mentions': [{'start': 0, 'labels': ['Item'], 'end': 1}, {'start': 1, 'labels': ['Observation'], 'end': 3}]}\n"
     ]
    }
   ],
   "source": [
    "def _bio_to_mention(conll_doc: dict):\n",
    "    \"\"\"Return a Mention-format representation of a BIO-formatted\n",
    "    tagged sentence.\n",
    "\n",
    "    Args:\n",
    "        conll_doc (ConllDocument): The doc to convert to redcoat.\n",
    "        doc_idx (int): The id of the doc, necessary to create a Redcoat doc.\n",
    "\n",
    "    Returns:\n",
    "        dict: A mention-formatted dict created from the conll_doc.\n",
    "    \"\"\"\n",
    "    tokens = conll_doc[\"tokens\"]\n",
    "    labels = conll_doc[\"labels\"]\n",
    "    mentions_list = []\n",
    "\n",
    "    start = 0\n",
    "    end = 0\n",
    "    label = None\n",
    "    for i, (token, label) in enumerate(\n",
    "        zip(tokens, labels)\n",
    "    ):\n",
    "        if label.startswith(\"B-\"):\n",
    "            if len(mentions_list) > 0:\n",
    "                mentions_list[-1][\"end\"] = i\n",
    "            mentions_list.append({\"start\": i, \"labels\": [label[2:]]})\n",
    "        elif label == \"O\" and len(mentions_list) > 0:\n",
    "            mentions_list[-1][\"end\"] = i\n",
    "        if len(mentions_list) == 0:\n",
    "            continue\n",
    "        if i == (len(tokens) - 1) and \"end\" not in mentions_list[-1]:\n",
    "            mentions_list[-1][\"end\"] = i + 1\n",
    "    return {'tokens': tokens, 'mentions': mentions_list}\n",
    "\n",
    "\n",
    "# For each BIO tagged sentence in tagged_sents, convert it to the mention-based\n",
    "# representation\n",
    "tagged_sents_m = []\n",
    "for doc in tagged_sents:\n",
    "    mention_doc = _bio_to_mention(doc)\n",
    "    tagged_sents_m.append(mention_doc)\n",
    " \n",
    "print(tagged_sents_m[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extracting relations between the entities via Relation Extraction\n",
    "\n",
    "We have extracted the entities appearing in each work order. The next step is to extract the relationships between those entities. We can do this using Relation Extraction.\n",
    "\n",
    "![alt text](images/building-relations.png \"Building relations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Flair-based RE model code from MWO2KG rebuild. Very similar to the section above, just with a different dataset and using text classification instead of sequence tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Combining NER+RE\n",
    "\n",
    "TODO: Do something like this but using RE instead of simple Item -> everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normalised_work_order_entities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-154-c67f1fc77ea1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtriples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnormalised_work_order_entities\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentity_class\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mentity_class\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"item\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'normalised_work_order_entities' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# This is old code from the master class\n",
    "\n",
    "triples = []\n",
    "\n",
    "for row in normalised_work_order_entities:\n",
    "    for (ngram, entity_class) in row:\n",
    "        if entity_class != \"item\": continue\n",
    "            \n",
    "        # If this entity is an item, link it to all other entities in the work order       \n",
    "             \n",
    "        for (other_ngram, other_entity_class) in row:   \n",
    "            if ngram == other_ngram: continue # Don't link items to themselves                \n",
    "\n",
    "            relation_type = other_entity_class.upper()                \n",
    "            triples.append(((ngram, entity_class), \"HAS_%s\" % relation_type, (other_ngram, other_entity_class)))\n",
    "        \n",
    "for triple in triples:\n",
    "    print(triple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Creating the graph\n",
    "\n",
    "Now that we have our nodes and relations we can go ahead and build the Neo4J graph.\n",
    "\n",
    "To do this we are going to use py2neo, a Python library for interacting with Neo4J.\n",
    "\n",
    "There are also a couple of other ways to do this - you can either use Neo4J and run Cypher queries to insert each node and relation, or use the APOC library to import a list of nodes from a CSV file. I find Python to be the simplest way, however.\n",
    "\n",
    "> Before proceeding, make sure you have created a new graph in Neo4j and that your new Neo4j graph is running.\n",
    "\n",
    "You can download and install Neo4j from here if you haven't already: https://neo4j.com/download/. I will be demonstrating the graph during the class so there's no need to have it installed unless you are also interested in trying out some graph queries yourself.\n",
    "\n",
    "> If you need to build your graph again, make sure to run this cell before running subsequent cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "from py2neo.data import Node, Relationship\n",
    "\n",
    "GRAPH_PASSWORD = \"password\" # Set this to the password of your Neo4J graph\n",
    "\n",
    "graph = Graph(password = GRAPH_PASSWORD)\n",
    "\n",
    "# We will start by deleting all nodes and edges in the current graph.\n",
    "# If we don't do this, we will end up with duplicate nodes and edges when running this script again.\n",
    "graph.delete_all() \n",
    "\n",
    "tx = graph.begin()\n",
    "\n",
    "# We will keep a dictionary of nodes that we have created so far.\n",
    "# This serves two purposes:\n",
    "#  - prevents duplicate nodes\n",
    "#  - provides us with a way to create edges between the nodes\n",
    "created_entity_nodes = {}\n",
    "\n",
    "# Creates a node for the specified ngram and entity_class.\n",
    "# If the node has already been created (i.e. it exists in created_nodes), return the node.\n",
    "# Otherwise, create a new one.\n",
    "def create_entity_node(ngram, entity_class):\n",
    "    if ngram in created_entity_nodes:\n",
    "        node = created_entity_nodes[ngram]\n",
    "    else:\n",
    "        node = Node(\"Entity\", entity_class, name=ngram)\n",
    "        created_entity_nodes[ngram] = node\n",
    "        tx.create(node)\n",
    "    return node\n",
    "\n",
    "\n",
    "# Create a node for each triple in the list of triples.\n",
    "# Set the class of each node to the entity_class (e.g. \"activity\", \"item\" or \"observation\").\n",
    "# Create a relationship between the nodes in the triple.\n",
    "for ((ngram_1, entity_class_1), relation, (ngram_2, entity_class_2)) in triples:\n",
    "    \n",
    "    node_1 = create_entity_node(ngram_1, entity_class_1)\n",
    "    node_2 = create_entity_node(ngram_2, entity_class_2)   \n",
    "    \n",
    "    \n",
    "    # Create a relationship between two nodes.\n",
    "    # This does not check for duplicate relationships unlike create_node,\n",
    "    # so this code will need to be adjusted on larger datasets.\n",
    "    relationship = Relationship( node_1, relation, node_2 )\n",
    "    tx.create(relationship)\n",
    "    \n",
    "    \n",
    "tx.commit()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Create nodes for the documents (i.e. the Work Orders)\n",
    "\n",
    "In order to query our graph, we need to create nodes for each work order in our dataset as well. We then need to link each Document node to every Entity node appearing in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse as parse_date\n",
    "\n",
    "# Our work_order_data and normalised_work_order entities allow us to do this quite easily,\n",
    "\n",
    "tx = graph.begin()\n",
    "\n",
    "# We will once again keep a mapping of created work order nodes, this time indexed by the row index.\n",
    "created_work_order_nodes = {}\n",
    "\n",
    "# Dates are a little awkward in Neo4j - we have to convert it to an integer representation in Python.\n",
    "# The APOC library has functions to handle this better.\n",
    "def date_to_int(date):\n",
    "    parsed_date = parse_date(str(date))\n",
    "    date = int(\"%s%s%s\" % (parsed_date.year, str(parsed_date.month).zfill(2), str(parsed_date.day).zfill(2)))\n",
    "    return date\n",
    "\n",
    "# The process of creating a work order node is a bit different to creating an entity,\n",
    "# as we also want to incorporate some of the structured fields onto the node.\n",
    "def create_structured_node(index, row, node_type, created_nodes):\n",
    "    if index in created_nodes:\n",
    "        return created_nodes[index]\n",
    "\n",
    "    if 'StartDate' in row:\n",
    "        row['StartDate'] = date_to_int(row['StartDate'])\n",
    "    if 'EndDate' in row:\n",
    "        row['EndDate'] = date_to_int(row['EndDate'])  \n",
    "\n",
    "    node = Node(node_type, **row)\n",
    "    created_nodes[index] = node\n",
    "    tx.create(node)\n",
    "    return node\n",
    "\n",
    "for i, row in enumerate(work_order_data):\n",
    "    node = create_structured_node(i, row, \"WorkOrder\", created_work_order_nodes)\n",
    "    \n",
    "tx.commit()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Link the entities to their corresponding work order nodes\n",
    "\n",
    "In order to properly query our graph, we need to link every entity node to the work order node in which it appears.\n",
    "\n",
    "This allows us to run queries such as \"pumps with electrical issues in the last 3 months\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = graph.begin()\n",
    "\n",
    "# We can use the normalised_work_order_entries list to do this.\n",
    "for i, row in enumerate(normalised_work_order_entities):\n",
    "    for (ngram, entity_class) in row:        \n",
    "        \n",
    "        node_1 = created_entity_nodes[ngram]\n",
    "        node_2 = created_work_order_nodes[i]\n",
    "        \n",
    "        relationship = Relationship( node_1, \"APPEARS_IN\", node_2 )\n",
    "        tx.create(relationship)\n",
    "       \n",
    "tx.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Querying the graph\n",
    "\n",
    "## TODO: Update with GQVis\n",
    "\n",
    "Now that the graph has been created, we can query it in Neo4j. This section lists some example queries that we can run on our graph. If you would like to try these yourself you can paste them directly into the Neo4j console.\n",
    "\n",
    "First, let's try a simple query. Here is a query that searches for __all failure modes observed on engines__:\n",
    "\n",
    "    MATCH (e:Entity {name: \"engine\"})-[r:HAS_OBSERVATION]->(o:observation)\n",
    "    RETURN e, r, o\n",
    "\n",
    "We can also use our graph as a way to quickly search and access work orders for the entities appearing in those work orders. For example, searching for __all work orders containing a leak__:\n",
    "\n",
    "    MATCH (d:WorkOrder)<-[a:APPEARS_IN]-(o:observation {name: \"leak\"})\n",
    "    RETURN d, a, o\n",
    "\n",
    "We could extend this to also show the items on which the leaks were present:\n",
    "\n",
    "    MATCH (d:WorkOrder)<-[a:APPEARS_IN]-(o:observation {name: \"leak\"})<-[r:HAS_OBSERVATION]-(e:Entity)\n",
    "    RETURN d, a, o, r, e\n",
    "\n",
    "Our queries can also incorporate structured data, such as the start dates of the work orders. Here is an example query for __all assets that had leaks from 25 to 28 July__:\n",
    "\n",
    "    MATCH (d:WorkOrder)<-[a:APPEARS_IN]-(e:Entity)-[r:HAS_OBSERVATION]->(o:observation {name: \"leak\"})-[:APPEARS_IN]->(d)\n",
    "    WHERE d.StartDate >= 20050725\n",
    "    AND d.StartDate <= 20050728\n",
    "    RETURN e, r, o\n",
    "\n",
    "On a larger graph this would also work well with other forms of structured data such as costs. We could query based on specific asset costs, for example.\n",
    "\n",
    "Now that our work orders and downtime events are in one graph, we can also make queries about downtime events. Here is an example query for the __downtime events associated with assets appearing in work orders from 25 to 28 July (where the downtime events occurred in July)__:\n",
    "\n",
    "    MATCH (d:WorkOrder)<-[a:APPEARS_IN]-(e:Entity)-[r:HAS_EVENT]->(x:DowntimeEvent)\n",
    "    WHERE d.StartDate > 20050725\n",
    "    AND d.StartDate < 20050728\n",
    "    AND 20050700 <= x.StartDate <= 20050731\n",
    "    RETURN e, r, x\n",
    "\n",
    "We can of course extend this to specific assets, such as pumps:\n",
    "\n",
    "    MATCH (d:WorkOrder)<-[a:APPEARS_IN]-(e:Entity {name: \"pump\"})-[r:HAS_EVENT]->(x:DowntimeEvent)\n",
    "    WHERE d.StartDate > 20050725\n",
    "    AND d.StartDate < 20050728\n",
    "    AND 20050700 <= x.StartDate <= 20050731\n",
    "    RETURN e, r, x\n",
    "\n",
    "In larger graphs the downtime events could even be further queried based on duration, cost, lost feed, or date ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Future improvements\n",
    "\n",
    "## Incorporating FLOCs\n",
    "\n",
    "Our downtime events are currently linked to Item nodes, but it would make more sense to link them to nodes representing the functional locations.\n",
    "\n",
    "If you are interested in continuing work on this small graph, the next best step would be to create nodes for the functional location data (`floc_data`) and to link the downtime events to those nodes as opposed to the Item nodes.\n",
    "\n",
    "![alt text](images/adding-flocs.png \"Adding FLOCs\")\n",
    "\n",
    "## Frequencies on edge properties\n",
    "\n",
    "We could also improve the graph by incorporating frequencies onto the edge properties. For example, if a \"leak\" occurred on a pump in two different work orders, our link between \"pump\" and \"leak\" could have a property called `frequency` with a value of `2`. This would allow us to query, for example, assets that had a particularly high number of leaks.\n",
    "\n",
    "\n",
    "## Constructing a graph from your own work order data\n",
    "\n",
    "If you have a work order dataset of your own, feel free to download this code and try it out on your dataset. I would be happy to chat if you would like to further discuss the code or if you run into any issues.\n",
    "\n",
    "If you need to extract entities not listed in the lexicon, you will need to update the lexicon file to include your new entities. Alternatively, the LexiconTagger can be substituted for a named entity recognition model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "floc_file = \"data/sample_flocs.csv\"\n",
    "floc_data = load_csv(floc_file)\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------\n",
    "\n",
    "## OLD Normalise the entities\n",
    "\n",
    "### MS: Probably going to take this out to save time, I can leave it as a future step for people. It should probably go before NER/RE anyway\n",
    "\n",
    "The next step is to normalise the ngrams, i.e. convert each ngram into a normalised form. This is important as we would prefer to have a single node for a single concept, e.g. one node for \"engine\" as opposed to two nodes for \"engin\" and \"engine\".\n",
    "\n",
    "We will once again be using a lexicon for this task, but it would typically be performed by machine learning.\n",
    "\n",
    "![alt text](images/normalising-entities.png \"Normalising entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('repair', 'activity'), ('cracked', 'observation'), ('hydraulic tank', 'item')]\n",
      "[('engine', 'item'), ('failure to start', 'observation')]\n",
      "[('air conditioner', 'item'), ('overheating', 'observation')]\n",
      "[('engine', 'item'), ('breakdown', 'observation')]\n",
      "[('fix', 'activity'), ('engine', 'item')]\n",
      "[('pump', 'item'), ('service', 'activity')]\n",
      "[('pump', 'item'), ('leak', 'observation')]\n",
      "[('fix', 'activity'), ('leak', 'observation'), ('pump', 'item')]\n",
      "[('engine', 'item'), ('breakdown', 'observation')]\n",
      "[('engine', 'item'), ('failure to start', 'observation')]\n",
      "[('pump', 'item'), ('electrical issue', 'observation')]\n",
      "[('pump', 'item'), ('leak', 'observation')]\n",
      "[('air conditioner', 'item'), ('breakdown', 'observation')]\n",
      "[('air conditioner', 'item'), ('breakdown', 'observation')]\n"
     ]
    }
   ],
   "source": [
    "lexicon_n_file = \"data/lexicon_normalisation.csv\"\n",
    "lexicon_normaliser = LexiconTagger(lexicon_n_file)\n",
    "\n",
    "normalised_work_order_entities = []\n",
    "\n",
    "# For every row in work_order_entities, replace each ngram with its normalised counterpart\n",
    "# as per the normalisation lexicon.\n",
    "# For example, \"engin\" will become \"engine\", \"leaking\" will become \"leak\", etc.\n",
    "for row in work_order_entities:\n",
    "    normalised_work_order_entities.append([(lexicon_normaliser.normalise_ngram(ngram), entity_class) \n",
    "                                           for (ngram, entity_class) in row])\n",
    "    \n",
    "    \n",
    "for row in normalised_work_order_entities:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD Extending the graph to incorporate Downtime events\n",
    "\n",
    "The next step is to incorporate the downtime events.\n",
    "\n",
    "For this exercise we are going to link the Downtime events to the first Item node appearing in the work orders with the same FLOC as the downtime event.\n",
    "\n",
    "\n",
    "![alt text](images/adding-downtime-events.png \"Adding downtime events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = graph.begin()\n",
    "\n",
    "created_downtime_nodes = {}\n",
    "\n",
    "# Create a DowntimeEvent node for each row\n",
    "for i, downtime_row in enumerate(downtime_data):\n",
    "    node = create_structured_node(i, downtime_row, \"DowntimeEvent\", created_downtime_nodes)\n",
    "    \n",
    "    # Get all work order nodes with the same FLOC and link the DowntimeEvent to the Items appearing\n",
    "    # in those work orders\n",
    "    for j, work_order_row in enumerate(work_order_data):\n",
    "        if work_order_row[\"FLOC\"] == downtime_row[\"FLOC\"]:\n",
    "            \n",
    "            work_order_entities = normalised_work_order_entities[j]\n",
    "            \n",
    "            for (ngram, entity_class) in work_order_entities:\n",
    "                if entity_class != \"item\": continue    # We don't need to link non-items to downtime events               \n",
    "                    \n",
    "                item_node = created_entity_nodes[ngram]\n",
    "                relationship = Relationship( item_node, \"HAS_EVENT\", node )\n",
    "                tx.create(relationship)\n",
    "                break\n",
    "\n",
    "    \n",
    "tx.commit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
