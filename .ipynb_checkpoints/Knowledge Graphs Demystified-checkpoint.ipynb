{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing a Knowledge Graph from Maintenance Work Order Data\n",
    "\n",
    "In this notebook we are going to construct a simple knowledge graph using Python, and run some queries on the graph in Neo4j. We have broken the notebook into several steps:\n",
    "\n",
    "- Reading in the data\n",
    "- Cleaning the data\n",
    "- Extracting entities via Named Entity Recognition (NER)\n",
    "- Creating relationships between entities via Relation Extraction (RE)\n",
    "- Putting it all together and building a Neo4j graph\n",
    "- Querying the graph in Neo4j\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Installing required packages\n",
    "\n",
    "To run this notebook you will need to install the following via pip:\n",
    "\n",
    "- `py2neo`: A library for working with Neo4j in Python.\n",
    "- `gqvis`: Our simple tool for visualising graph queries in Jupyter.\n",
    "- `flair`: A deep learning library for natural language processing. Note this library is quite large (a couple gb I believe). If you don't wish to install this, we have provided non deep-learning based alternatives so you can still follow along.\n",
    "\n",
    "You will also need to have Neo4j installed for the last part of the tutorial. You can download and install Neo4j Desktop [here](https://neo4j.com/).\n",
    "\n",
    "We will be running through the code during the tutorial so there is no need to install anything unless you would also like to try the code out yourself and run some graph queries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install py2neo\n",
    "!pip install gqvis\n",
    "!pip install flair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read in the data\n",
    "\n",
    "Here is a description of the datasets we are working with in this notebook.\n",
    "\n",
    "First of all, the datasets for the NER model:\n",
    "\n",
    "- `ner_dataset/train.txt`: The dataset we will use to *train* the NER model to predict the entities appearing in each work order.\n",
    "- `ner_dataset/dev.txt`: The dataset we will use to *validate* the quality of the model during training.\n",
    "- `ner_dataset/test.txt`: The dataset we will use to *evaluate* the final performance of the NER model after training.\n",
    "\n",
    "We also have three datasets for the Relation Extraction (RE) model:\n",
    "\n",
    "- `re_dataset/train.csv`\n",
    "- `re_dataset/dev.csv`\n",
    "- `re_dataset/test.csv`\n",
    "\n",
    "We are going to be building a knowledge graph on a small sample set of work orders. This will not be seen by the NER or RE models prior to constructing the graph - the idea is to get our models to run *inference* over this dataset to automatically predict the entities, and relationships between the entities, to build a graph.\n",
    "\n",
    "- `sample_work_orders.csv`: A csv file containing a set of work orders.\n",
    "\n",
    "Here is an example of what the first few rows of each dataset look like:\n",
    "\n",
    "![alt text](images/example-data.png \"Example datasets\")\n",
    "\n",
    "We are using the simple `csv` library to read in the data, though this can also be done using `pandas`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Inspecting the data\n",
    "\n",
    "Let's start by inspecting the `sample_work_orders.csv` CSV dataset. This is the dataset we will be building the graph from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('StartDate', '10/07/2005'), ('FLOC', '1234.1.1'), ('ShortText', 'repair cracked hyd tank')])\n",
      "OrderedDict([('StartDate', '14/07/2005'), ('FLOC', '1234.1.2'), ('ShortText', 'engine wont start')])\n",
      "OrderedDict([('StartDate', '17/07/2005'), ('FLOC', '1234.1.3'), ('ShortText', 'a/c blowing hot air')])\n",
      "OrderedDict([('StartDate', '20/07/2005'), ('FLOC', '1234.1.2'), ('ShortText', 'engin u/s')])\n",
      "OrderedDict([('StartDate', '21/07/2005'), ('FLOC', '1234.1.2'), ('ShortText', 'fix engine')])\n",
      "OrderedDict([('StartDate', '22/07/2005'), ('FLOC', '1234.1.4'), ('ShortText', 'pump service')])\n",
      "OrderedDict([('StartDate', '23/07/2005'), ('FLOC', '1234.1.4'), ('ShortText', 'pump leak')])\n",
      "OrderedDict([('StartDate', '24/07/2005'), ('FLOC', '1234.1.4'), ('ShortText', 'fix leak on pump')])\n",
      "OrderedDict([('StartDate', '25/07/2005'), ('FLOC', '1234.1.2'), ('ShortText', 'engine not running')])\n",
      "OrderedDict([('StartDate', '26/07/2005'), ('FLOC', '1234.1.2'), ('ShortText', 'engine has problems starting')])\n",
      "OrderedDict([('StartDate', '27/07/2005'), ('FLOC', '1234.1.4'), ('ShortText', 'pump fault')])\n",
      "OrderedDict([('StartDate', '28/07/2005'), ('FLOC', '1234.1.4'), ('ShortText', 'pump leaking')])\n",
      "OrderedDict([('StartDate', '29/07/2005'), ('FLOC', '1234.1.3'), ('ShortText', 'a/c not working')])\n",
      "OrderedDict([('StartDate', '30/07/2005'), ('FLOC', '1234.1.3'), ('ShortText', 'a/c broken')])\n"
     ]
    }
   ],
   "source": [
    "from csv import DictReader\n",
    "\n",
    "work_order_file = \"data/sample_work_orders.csv\"\n",
    "\n",
    "# A simple function to read in a csv file and return a list,\n",
    "# where each element in the list is a dictionary of {heading : value}\n",
    "def load_csv(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = DictReader(f)\n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "    return data\n",
    "\n",
    "        \n",
    "work_order_data = load_csv(work_order_file)\n",
    "\n",
    "for row in work_order_data:\n",
    "    print(row)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cleaning the data\n",
    "\n",
    "TODO: Probably best to have a simple model for cleaning the data here. I don't think we need to go into too much detail about it, but it would be nice to have this as the first step so things like 'hyd pump' get corrected before running NER/RE. We could mention Lexiclean here\n",
    "\n",
    "I could just chuck the lexicon cleaner thing from the masterclass here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Named Entity Recognition\n",
    "\n",
    "Our first task is to extract the entities in the short text descriptions and construct nodes from those entities. This is how we are able to unlock the knowledge captured within the short text and combine it with the structured fields.\n",
    "\n",
    "![alt text](images/extracting-entities-v2.png \"Extracting entities\")\n",
    "\n",
    "## 3.1. Loading and inspecting the data\n",
    "\n",
    "Let's start by defining some functions for loading the CONLL-formatted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "NER_DATASET_PATH = \"data/ner_dataset\"\n",
    "\n",
    "\n",
    "def to_conll_document(s: str):\n",
    "    \"\"\"Create a ConllDocument from a string as it appears\n",
    "    in a Conll-formatted file.\n",
    "\n",
    "    Args:\n",
    "        s (str): A string, separated by newlines, where each\n",
    "        line is a token, then a comma and space, then a label.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dict of tokens and labels.\n",
    "    \"\"\"\n",
    "    tokens, labels = [], []\n",
    "    for line in s.split(\"\\n\"):\n",
    "        if len(line.strip()) == 0:\n",
    "            continue\n",
    "        token, label = line.split()\n",
    "\n",
    "        tokens.append(token)\n",
    "        labels.append(label)\n",
    "    return {'tokens': tokens, 'labels': labels}\n",
    "\n",
    "\n",
    "def load_conll_dataset(filename: str) -> list:\n",
    "    \"\"\"Load a list of documents from the given CONLL-formatted dataset.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The filename to load from.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of documents, where each document is a dict of tokens and labels.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        docs = f.read().split(\"\\n\\n\")\n",
    "        for d in docs:\n",
    "            if len(d) == 0:\n",
    "                continue\n",
    "            document = to_conll_document(d)\n",
    "            documents.append(document)\n",
    "    print(f\"Loaded {len(documents)} documents from {filename}.\")\n",
    "    return documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the first row of our training dataset to make sure it loads OK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3200 documents from data/ner_dataset\\train.txt.\n",
      "{'tokens': ['ram', 'on', 'cup', 'rod', 'support', 'broken'], 'labels': ['B-Item', 'B-Location', 'B-Item', 'B-Item', 'I-Item', 'B-Observation']}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_conll_dataset(os.path.join(NER_DATASET_PATH, 'train.txt'))\n",
    "\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Define an abstract base class for NER Models\n",
    "\n",
    "Seeing as we would like to be able to work with a range of NER models, it's a good idea to create an 'abstract base class' to represent an NER model. This way, we can create classes for our NER models that inherit from this base class. Every model we create must have these three functions:\n",
    "\n",
    "- `train`: Train the model on the datasets in the given path.\n",
    "- `inference`: Run inference over the given sentence.\n",
    "- `load`: Load the model from the given path.\n",
    "\n",
    "If we try to create an NER model that does not have one of these functions, it will raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Abstract base class for the NER Model. \"\"\"\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class NERModel(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, datasets_path: str):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def inference(self, sent: list):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load(self, model_path):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Define our NER models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3.3.1. Flair-based NER Model\n",
    "\n",
    "In this tutorial we will use [Flair](https://github.com/flairNLP/flair), which simplifies the process of building a deep learning model for a variety of NLP tasks.\n",
    "\n",
    "The code below is a class representing a `FlairNERModel`, which is based on the `NERModel` class above. It has the same three methods, i.e `train()`, `inference()`, and `save()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mwo2kg_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-5a8ebaf04adf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# TODO: Get rid of ConllDataset/ConllDocument and just use lists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m from mwo2kg_datasets import (\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[0mConllDataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mConllDocument\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mwo2kg_datasets'"
     ]
    }
   ],
   "source": [
    "\"\"\"A Flair-based Named Entity Recognition model. Learns to predict entity\n",
    "classes via deep learning.\"\"\"\n",
    "\n",
    "\n",
    "# TODO: Tidy up, fix this code as it does not work atm in this notebook\n",
    "\n",
    "\n",
    "import os\n",
    "import flair\n",
    "from flair.data import Corpus, Sentence\n",
    "from flair.datasets import ColumnCorpus\n",
    "from flair.embeddings import (\n",
    "    StackedEmbeddings,\n",
    "    FlairEmbeddings,\n",
    ")\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer\n",
    "from typing import List\n",
    "from flair.visual.training_curves import Plotter\n",
    "import torch\n",
    "\n",
    "\n",
    "# TODO: Get rid of ConllDataset/ConllDocument and just use lists\n",
    "from mwo2kg_datasets import (\n",
    "    ConllDataset,\n",
    "    ConllDocument,\n",
    ")\n",
    "\n",
    "HIDDEN_SIZE = 256\n",
    "\n",
    "# Check whether CUDA is available and set the device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    flair.device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    flair.device = torch.device(\"cpu\")\n",
    "print(\"Device:\", flair.device)\n",
    "\n",
    "\n",
    "class FlairNERModel(NERModel):\n",
    "\n",
    "    model_name: str = \"Flair\"\n",
    "\n",
    "    \"\"\"A Flair-based Named Entity Recognition model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FlairNERModel, self).__init__()\n",
    "\n",
    "        self.model = None\n",
    "\n",
    "    def train(self, datasets_path: os.path, trained_model_path: os.path):\n",
    "        \"\"\" Train the Flair model on the given conll datasets.\n",
    "\n",
    "        Args:\n",
    "            datasets_path (os.path): The folder containing the\n",
    "              train, dev and text CONLL-formatted datasets.\n",
    "            trained_model_path (os.path): The folder to save the trained\n",
    "              model to.\n",
    "        \"\"\"\n",
    "\n",
    "        columns = {0: \"text\", 1: \"ner\"}\n",
    "        corpus: Corpus = ColumnCorpus(\n",
    "            datasets_path,\n",
    "            columns,\n",
    "            train_file=\"train.txt\",\n",
    "            dev_file=\"dev.txt\",\n",
    "            test_file=\"test.txt\",\n",
    "        )\n",
    "        label_dict = corpus.make_label_dictionary(label_type=\"ner\")\n",
    "\n",
    "        # Train the sequence tagger\n",
    "        embedding_types = [\n",
    "            FlairEmbeddings(\"mix-forward\"),\n",
    "            FlairEmbeddings(\"mix-backward\"),\n",
    "        ]\n",
    "\n",
    "        embeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "\n",
    "        tagger = SequenceTagger(\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            embeddings=embeddings,\n",
    "            tag_dictionary=label_dict,\n",
    "            tag_type=\"ner\",\n",
    "            use_crf=True,\n",
    "        )\n",
    "\n",
    "        trainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "        sm = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            sm = \"gpu\"\n",
    "        trainer.train(\n",
    "            trained_model_path,\n",
    "            learning_rate=0.1,\n",
    "            mini_batch_size=32,\n",
    "            max_epochs=10,\n",
    "            embeddings_storage_mode=sm,\n",
    "        )\n",
    "\n",
    "        plotter = Plotter()\n",
    "        plotter.plot_weights(os.path.join(trained_model_path, \"weights.txt\"))\n",
    "\n",
    "        self.load(os.path.join(trained_model_path, 'final-model.pt'))\n",
    "\n",
    "    def inference(self, raw_sents: list) -> ConllDataset:\n",
    "        \"\"\"Run the inference on a given list of short texts.\n",
    "\n",
    "        Args:\n",
    "            raw_sents (list): The list of raw sents to run the inference on.\n",
    "\n",
    "        Returns:\n",
    "            ConllDataset: The ConllDataset of preds.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the model has not yet been trained.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\n",
    "                \"The KGC Model has not yet been trained. \"\n",
    "                \"Please train this Flair model before proceeding.\"\n",
    "            )\n",
    "\n",
    "        preds_dataset = ConllDataset()\n",
    "\n",
    "        for i, tokens in enumerate(raw_sents):\n",
    "            labels = self._tag_sentence(tokens)\n",
    "            doc = ConllDocument(tokens, labels)\n",
    "            preds_dataset.add_document(doc)\n",
    "\n",
    "        return preds_dataset\n",
    "\n",
    "    def load(self, model_path: str):\n",
    "        \"\"\"Load the model from the specified path.\n",
    "\n",
    "        Args:\n",
    "            model_path (os.path): The path to load.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the path does not exist i.e. model not yet trained.\n",
    "        \"\"\"\n",
    "        self.model = SequenceTagger.load(model_path)\n",
    "\n",
    "    def _tag_sentence(self, sentence: List[str]) -> List[str]:\n",
    "        \"\"\"Tag the given sentence (list of tokens) via the model.\n",
    "\n",
    "        Args:\n",
    "            sentence (List[str]): A list of tokens.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of labels.\n",
    "        \"\"\"\n",
    "        sentence_obj = Sentence(sentence, use_tokenizer=False)\n",
    "        self.model.predict(sentence_obj)\n",
    "        labels = [\"O\"] * len(sentence)\n",
    "\n",
    "        for entity in sentence_obj.get_spans(\"ner\"):\n",
    "            for i, token in enumerate(entity):\n",
    "                label = entity.get_label(\"ner\").value\n",
    "                prefix = \"B-\" if i == 0 else \"I-\"\n",
    "                \n",
    "                # Token idx starts from 1 in Flair.\n",
    "                labels[token.idx - 1] = prefix + label\n",
    "\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. Dictionary-based NER model\n",
    "\n",
    "If you are not able to use the Flair library, here is a simple model you can use to extract the entities, albeit with a much weaker performance. This one scans the training data, builds a mapping between each phrase (one or more tokens in a row) and the most common entity type associated with that phrase, then uses that entity type as the prediction when seeing that token in the test data.\n",
    "\n",
    "The model is super simple, so we won't show the code here, but feel free to have a look under `helpers/DictionaryNERModel.py` if you are interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import DictionaryNERModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Training the model\n",
    "\n",
    "Depending on whether you are using Flair or the DictionaryNERModel, you can run one of the cells below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3.4.1. Using Flair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have trained the Flair-based model and have uploaded the model onto Huggingface. The following code will download that model and load the weights, so there is no need for you to train the model yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FlairNERModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-95be3cd0c64f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFlairNERModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#m.train(NER_DATASET_PATH, os.path.join('models/ner_models')) # Uncomment to train manually\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nlp-tlp/mwo-ner-test\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# TODO: Replace with load_pretrained\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'FlairNERModel' is not defined"
     ]
    }
   ],
   "source": [
    "m = FlairNERModel()\n",
    "#m.train(NER_DATASET_PATH, 'models/ner_models/flair') # Uncomment to train manually\n",
    "m.load(\"nlp-tlp/mwo-ner-test\") # TODO: Replace with load_pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2. Using the DictionaryNERModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n",
      "Loaded 3200 documents from data/ner_dataset\\train.txt.\n",
      "Loaded 401 documents from data/ner_dataset\\dev.txt.\n"
     ]
    }
   ],
   "source": [
    "m = DictionaryNERModel()\n",
    "m.train(NER_DATASET_PATH, 'models/ner_models/dictionary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Running inference on unseen sentences\n",
    "\n",
    "The next step is to use our trained model to infer the entity type of each entity appearing in a list of previously unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['a/c', 'not', 'working'], 'labels': ['B-Item', 'B-Observation', 'I-Observation']}\n"
     ]
    }
   ],
   "source": [
    "tagged_bio_sents = []\n",
    "\n",
    "sentences = []\n",
    "for row in work_order_data:\n",
    "    sentence = row[\"ShortText\"].split() # We must 'tokenise' the sentence first, i.e. split into words\n",
    "    tagged_sent = m.inference(sentence)        \n",
    "    tagged_bio_sents.append(tagged_sent)\n",
    "\n",
    "# Print an example tagged sentence\n",
    "print(tagged_bio_sents[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Extracting relations between the entities via Relation Extraction\n",
    "\n",
    "We have extracted the entities appearing in each work order. The next step is to extract the relationships between those entities. We can do this using Relation Extraction.\n",
    "\n",
    "![alt text](images/building-relations.png \"Building relations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Loading and inspecting the data\n",
    "\n",
    "Let's take a look again at the RE dataset we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['broken', 'rod support', 'Observation', 'Item', 'rod support broken', '0', '1', 'O']\n",
      "['rod support', 'broken', 'Item', 'Observation', 'rod support broken', '1', '0', 'HAS_OBSERVATION']\n",
      "['broken', 'cup', 'Observation', 'Item', 'cup rod support broken', '0', '2', 'O']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "RE_DATASET_PATH = \"data/re_dataset\"\n",
    "\n",
    "\n",
    "def load_re_dataset(filename: str) -> list:\n",
    "    \"\"\"Load the Relation Extraction dataset into a list.\n",
    "        \n",
    "    Args:\n",
    "        filename (str): The name of the file to load.\n",
    "    \"\"\"\n",
    "    re_data = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for row in f:\n",
    "            re_data.append(row.strip().split(','))\n",
    "    return re_data\n",
    "\n",
    "train_dataset = load_re_dataset(os.path.join(RE_DATASET_PATH, 'train.csv'))\n",
    "\n",
    "# Let's take a quick look just to make sure it loads as expected...\n",
    "for row in train_dataset[:3]:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interpret this as follows:\n",
    " - 'broken': entity 1\n",
    " - 'rod support': entity 2\n",
    " - 'Observation': label of entity 1\n",
    " - 'Item': label of entity 2\n",
    " - 'rod support broken': The text between 'broken' and 'rod support', inclusive\n",
    " - '0': The mention index of entity 1\n",
    " - '1': The mention index of entity 2\n",
    " - 'O': The relation type. \"O\" means no relation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Define the Abstract Base Class\n",
    "\n",
    "We are going to see two different RE models, so let's define an abstract base class again just like we did for the NER models. Just like the NER model, we have three functions:\n",
    "\n",
    "- `inference`: Given a row (as above, but without the last column), predict the given relation type (\"O\" if no relation).\n",
    "- `train`: Train the model on the files in the given dataset path.\n",
    "- `load`: Load the model from the specified path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class REModel(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def inference(self, row: list) -> str:\n",
    "        pass        \n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, re_datasets_path: str):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load(self, model_path: str):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Define our RE model(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1. Flair-based RE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "\"\"\" A Flair-based relation extraction model.\n",
    "This one uses Flair's TextClassifier model to classify the\n",
    "relation type of a given row.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "import flair\n",
    "from flair.trainers import ModelTrainer\n",
    "from flair.datasets import CSVClassificationCorpus\n",
    "from flair.embeddings import (\n",
    "    PooledFlairEmbeddings,\n",
    "    DocumentRNNEmbeddings,\n",
    ")\n",
    "from flair.data import Sentence\n",
    "from typing import List\n",
    "from flair.models import TextClassifier\n",
    "from flair.visual.training_curves import Plotter\n",
    "\n",
    "import torch\n",
    "\n",
    "MAX_EPOCHS = 1\n",
    "HIDDEN_SIZE = 256\n",
    "\n",
    "# Check whether CUDA is available and set the device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    flair.device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    flair.device = torch.device(\"cpu\")\n",
    "print(\"Device:\", flair.device)\n",
    "\n",
    "\n",
    "class FlairREModel(REModel):\n",
    "\n",
    "    \"\"\"The Flair-based RE model.\"\"\"\n",
    "\n",
    "    model_name: str = \"Flair\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FlairREModel, self).__init__()\n",
    "        self.model = None\n",
    "\n",
    "    def train(self, datasets_path: os.path, trained_model_path: os.path):\n",
    "        \"\"\"Train the Flair RE model on the given CSV datasets.\n",
    "\n",
    "        Args:\n",
    "            datasets_path (os.path): The path containing the train and dev\n",
    "               datasets.\n",
    "            trained_model_path (os.path): The path to save the trained model.\n",
    "        \"\"\"\n",
    "\n",
    "        column_name_map = {\n",
    "            0: \"text\",\n",
    "            1: \"text\",\n",
    "            2: \"text\",\n",
    "            3: \"text\",\n",
    "            4: \"text\",\n",
    "            7: \"label_relation\",\n",
    "        }\n",
    "\n",
    "        # Define corpus, labels, word embeddings, doc embeddings\n",
    "        corpus = CSVClassificationCorpus(\n",
    "            datasets_path,\n",
    "            column_name_map,\n",
    "            delimiter=\",\",\n",
    "            label_type=\"relation\",\n",
    "        )\n",
    "\n",
    "        label_dict = corpus.make_label_dictionary(label_type=\"relation\")\n",
    "\n",
    "        word_embeddings = [\n",
    "            PooledFlairEmbeddings(\"mix-forward\"),\n",
    "            PooledFlairEmbeddings(\"mix-backward\"),\n",
    "        ]\n",
    "\n",
    "        document_embeddings = DocumentRNNEmbeddings(\n",
    "            word_embeddings, hidden_size=HIDDEN_SIZE\n",
    "        )\n",
    "\n",
    "        # Initialise sequence tagger\n",
    "        tagger = TextClassifier(\n",
    "            document_embeddings,\n",
    "            label_dictionary=label_dict,\n",
    "            label_type=\"relation\",\n",
    "        )\n",
    "\n",
    "        # Initialize trainer\n",
    "        trainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "        sm = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            sm = \"gpu\"\n",
    "        \n",
    "        # Start training\n",
    "        trainer.train(\n",
    "            trained_model_path,\n",
    "            learning_rate=0.1,\n",
    "            mini_batch_size=32,\n",
    "            max_epochs=MAX_EPOCHS,\n",
    "            patience=3,\n",
    "            embeddings_storage_mode=sm,\n",
    "        )\n",
    "\n",
    "        self.load(os.path.join(trained_model_path, 'final-model.pt'))\n",
    "\n",
    "    def load(self, model_path: str):\n",
    "        \"\"\"Load the chunked frequency dict from the given folder.\n",
    "\n",
    "        Args:\n",
    "            model_path (str): The filename containing the model.\n",
    "               Can also be the name of a repo on Huggingface.\n",
    "        \"\"\"\n",
    "        TextClassifier.load(model_path)\n",
    "\n",
    "    def inference(self, row: list) -> str:\n",
    "        \"\"\"Run the inference over the given document.\n",
    "\n",
    "        Args:\n",
    "            row (list): The row to predict the relation of.\n",
    "\n",
    "        Returns:\n",
    "            str: The relation type.\n",
    "        \"\"\"\n",
    "        \n",
    "        s = Sentence(\" \".join(rel[:5]))\n",
    "        label = \"O\"\n",
    "        self.model.predict(s)\n",
    "        if len(s.labels) > 0:\n",
    "            label = str(s.labels[0].value)\n",
    "        return label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2. 'SimpleMWO' RE model\n",
    "\n",
    "Because maintenance work orders are very short (5-7 words typically), generally speaking we can create a useful knowledge graph by simply linking each Item entity in the work order and each other entity in that work order. For example:\n",
    "\n",
    "    replace pump\n",
    "    \n",
    "We can say the \"pump\" entity `HAS_ACTIVITY` \"replace\". Likewise for the following:\n",
    "\n",
    "    fix air conditioner , not working\n",
    "    \n",
    "We can say that \"air conditioner\" `HAS_ACTIVITY` \"fix\", and `HAS_OBSERVATION` \"not working\".\n",
    "\n",
    "This is not a foolproof method, though - it is a heuristic, i.e. a rule-based method designed to exploit a pattern in the data. For creating this specific type of knowledge graph, though, it works quite well, and thus we can define a model to use this heuristic as a weaker alternative to a deep learning model.\n",
    "\n",
    "Just like the dictionary-based NER model, the model is super simple, so we won't show the code here, but feel free to have a look under `helpers/SimpleMWOREModel.py` if you are interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import SimpleMWOREModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example output from the model. Note we have set the last column (i.e. the relation type) to `None`, as it is our model's job to predict that column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HAS_OBSERVATION'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = SimpleMWOREModel()\n",
    "\n",
    "r.inference([\n",
    "  \"rod support\",\n",
    "  \"broken\",\n",
    "  \"Item\",\n",
    "  \"Observation\",\n",
    "  \"rod support broken\",\n",
    "  \"1\",\n",
    "  \"0\",\n",
    "  None\n",
    " ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Train the model/load the pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the pretrained Flair RE model from Huggingface.\n",
    "\n",
    "(or alternatively you can train it yourself by uncommenting the train line, and commenting the load line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-21 20:37:32,560 Reading data from data\\re_dataset\n",
      "2022-11-21 20:37:32,562 Train: data\\re_dataset\\train.csv\n",
      "2022-11-21 20:37:32,562 Dev: data\\re_dataset\\dev.csv\n",
      "2022-11-21 20:37:32,563 Test: data\\re_dataset\\test.csv\n",
      "2022-11-21 20:37:32,653 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24804it [00:04, 6156.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-21 20:37:36,688 Dictionary created for label 'relation' with 13 values: O (seen 15398 times), HAS_ACTIVITY (seen 2825 times), HAS_OBSERVATION (seen 2174 times), APPEARS_WITH (seen 1982 times), HAS_LOCATION (seen 1556 times), HAS_CONSUMABLE (seen 334 times), HAS_SPECIFIER (seen 173 times), HAS_AGENT (seen 143 times), HAS_CARDINALITY (seen 114 times), HAS_ATTRIBUTE (seen 76 times), HAS_TIME (seen 25 times), HAS_EVENT (seen 4 times)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\flair\\trainers\\trainer.py:65: UserWarning: There should be no best model saved at epoch 1 except there is a model from previous trainings in your training folder. All previous best models will be deleted.\n",
      "  \"There should be no best model saved at epoch 1 except there \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-21 20:37:39,123 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-21 20:37:39,125 Model: \"TextClassifier(\n",
      "  (decoder): Linear(in_features=256, out_features=13, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (locked_dropout): LockedDropout(p=0.0)\n",
      "  (word_dropout): WordDropout(p=0.0)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): PooledFlairEmbeddings(\n",
      "        (context_embeddings): FlairEmbeddings(\n",
      "          (lm): LanguageModel(\n",
      "            (drop): Dropout(p=0.25, inplace=False)\n",
      "            (encoder): Embedding(275, 100)\n",
      "            (rnn): LSTM(100, 2048)\n",
      "            (decoder): Linear(in_features=2048, out_features=275, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_1): PooledFlairEmbeddings(\n",
      "        (context_embeddings): FlairEmbeddings(\n",
      "          (lm): LanguageModel(\n",
      "            (drop): Dropout(p=0.25, inplace=False)\n",
      "            (encoder): Embedding(275, 100)\n",
      "            (rnn): LSTM(100, 2048)\n",
      "            (decoder): Linear(in_features=2048, out_features=275, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=8192, out_features=8192, bias=True)\n",
      "    (rnn): GRU(8192, 256, batch_first=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2022-11-21 20:37:39,126 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-21 20:37:39,127 Corpus: \"Corpus: 24804 train + 3310 dev + 3234 test sentences\"\n",
      "2022-11-21 20:37:39,128 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-21 20:37:39,128 Parameters:\n",
      "2022-11-21 20:37:39,129  - learning_rate: \"0.100000\"\n",
      "2022-11-21 20:37:39,130  - mini_batch_size: \"32\"\n",
      "2022-11-21 20:37:39,131  - patience: \"3\"\n",
      "2022-11-21 20:37:39,132  - anneal_factor: \"0.5\"\n",
      "2022-11-21 20:37:39,133  - max_epochs: \"1\"\n",
      "2022-11-21 20:37:39,134  - shuffle: \"True\"\n",
      "2022-11-21 20:37:39,135  - train_with_dev: \"False\"\n",
      "2022-11-21 20:37:39,135  - batch_growth_annealing: \"False\"\n",
      "2022-11-21 20:37:39,136 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-21 20:37:39,137 Model training base path: \"models\\re_models\\flair\"\n",
      "2022-11-21 20:37:39,138 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-21 20:37:39,139 Device: cuda:0\n",
      "2022-11-21 20:37:39,140 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-21 20:37:39,140 Embeddings storage mode: gpu\n",
      "2022-11-21 20:37:39,141 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-21 20:37:39,143 train mode resetting embeddings\n",
      "2022-11-21 20:37:39,143 train mode resetting embeddings\n",
      "2022-11-21 20:37:53,636 epoch 1 - iter 77/776 - loss 0.05875709 - samples/sec: 174.18 - lr: 0.100000\n",
      "2022-11-21 20:38:09,078 epoch 1 - iter 154/776 - loss 0.04883092 - samples/sec: 164.38 - lr: 0.100000\n",
      "2022-11-21 20:38:24,116 epoch 1 - iter 231/776 - loss 0.04067766 - samples/sec: 168.98 - lr: 0.100000\n",
      "2022-11-21 20:38:39,196 epoch 1 - iter 308/776 - loss 0.03364710 - samples/sec: 167.23 - lr: 0.100000\n",
      "2022-11-21 20:38:54,197 epoch 1 - iter 385/776 - loss 0.02853559 - samples/sec: 169.50 - lr: 0.100000\n",
      "2022-11-21 20:39:09,990 epoch 1 - iter 462/776 - loss 0.02485532 - samples/sec: 159.51 - lr: 0.100000\n",
      "2022-11-21 20:39:24,857 epoch 1 - iter 539/776 - loss 0.02212722 - samples/sec: 171.02 - lr: 0.100000\n",
      "2022-11-21 20:39:39,629 epoch 1 - iter 616/776 - loss 0.02006071 - samples/sec: 170.70 - lr: 0.100000\n",
      "2022-11-21 20:39:55,776 epoch 1 - iter 693/776 - loss 0.01834604 - samples/sec: 155.97 - lr: 0.100000\n",
      "2022-11-21 20:40:10,464 epoch 1 - iter 770/776 - loss 0.01691758 - samples/sec: 171.81 - lr: 0.100000\n",
      "2022-11-21 20:40:11,521 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-21 20:40:11,523 EPOCH 1 done: loss 0.0168 - lr 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 104/104 [00:16<00:00,  6.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-21 20:40:28,143 Evaluating as a multi-label problem: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-21 20:40:28,163 DEV : loss 0.0031255579087883234 - f1-score (micro avg)  0.9164\n",
      "2022-11-21 20:40:28,894 BAD EPOCHS (no improvement): 0\n",
      "2022-11-21 20:40:28,896 saving best model\n",
      "2022-11-21 20:40:31,928 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-21 20:40:31,929 loading file models\\re_models\\flair\\best-model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 102/102 [00:15<00:00,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-21 20:40:49,408 Evaluating as a multi-label problem: False\n",
      "2022-11-21 20:40:49,427 0.9587\t0.8521\t0.9023\t0.9332\n",
      "2022-11-21 20:40:49,428 \n",
      "Results:\n",
      "- F-score (micro) 0.9023\n",
      "- F-score (macro) 0.9294\n",
      "- Accuracy 0.9332\n",
      "\n",
      "By class:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "HAS_OBSERVATION     1.0000    1.0000    1.0000       313\n",
      "   HAS_ACTIVITY     1.0000    1.0000    1.0000       279\n",
      "   HAS_LOCATION     1.0000    1.0000    1.0000       238\n",
      "   APPEARS_WITH     0.5114    0.2064    0.2941       218\n",
      " HAS_CONSUMABLE     1.0000    1.0000    1.0000        59\n",
      "      HAS_AGENT     1.0000    1.0000    1.0000        21\n",
      "  HAS_SPECIFIER     1.0000    1.0000    1.0000        15\n",
      "  HAS_ATTRIBUTE     1.0000    1.0000    1.0000        12\n",
      "HAS_CARDINALITY     1.0000    1.0000    1.0000        10\n",
      "       HAS_TIME     1.0000    1.0000    1.0000         5\n",
      "\n",
      "      micro avg     0.9587    0.8521    0.9023      1170\n",
      "      macro avg     0.9511    0.9206    0.9294      1170\n",
      "   weighted avg     0.9090    0.8521    0.8685      1170\n",
      "\n",
      "2022-11-21 20:40:49,429 ----------------------------------------------------------------------------------------------------\n",
      "2022-11-21 20:40:49,430 loading file models/re_models/flair\\final-model.pt\n"
     ]
    }
   ],
   "source": [
    "re_model = FlairREModel()\n",
    "re_model.train(RE_DATASET_PATH, \"models/re_models/flair\") # Uncomment to train manually\n",
    "\n",
    "# TODO: Load from huggingface\n",
    "# re_model.load('nlp-tlp/mwo-re')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. Inference\n",
    "\n",
    "Now we have our RE model, the next step is to run inference on the MWO dataset to extract the relationships between the entities.\n",
    "\n",
    "We need our data to be in the same format as required by the model, i.e. a list of rows where each row has five columns (entity 1, entity 2, etc), just like the training data used to train the model.\n",
    "\n",
    "So before we can run RE, we need to 'wrangle' our data again to get it into the right format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1. Converting the BIO format to the \"Mention\"-based format\n",
    "\n",
    "The BIO-based format from the NER model has one key downside - it is not good for representing 'phrases' of more than one token in length. This makes it difficult to work with for future steps, such as constructing nodes from the entities and running relation extraction. In light of this, we will now convert the BIO-formatted predictions into Mention format, i.e. go from this:\n",
    "\n",
    "    {'tokens': ['a/c', 'not', 'working'],\n",
    "     'labels': ['B-Item', 'B-Observation', 'I-Observation']}\n",
    "    \n",
    "To this:\n",
    "\n",
    "    {'tokens': ['a/c', 'not', 'working'],\n",
    "     'mentions': [\n",
    "         {'start': 0, 'labels': ['Item'], 'end': 1},\n",
    "         {'start': 1, 'labels': ['Observation'], 'end': 3}]}\n",
    "    \n",
    "Note that this format is also able to now support multiple labels per mention (though we will only be using single labels for simplicity). Researchers use this format for **entity typing**, which is similar to NER but with >= 1 label per mention.\n",
    "\n",
    "This step is just a bit of data wrangling - here we have defined a helper function to convert a BIO-tagged sentence into a Mention-tagged sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"tokens\": [\n",
      "  \"a/c\",\n",
      "  \"not\",\n",
      "  \"working\"\n",
      " ],\n",
      " \"mentions\": [\n",
      "  {\n",
      "   \"start\": 0,\n",
      "   \"labels\": [\n",
      "    \"Item\"\n",
      "   ],\n",
      "   \"end\": 1,\n",
      "   \"phrase\": \"a/c\"\n",
      "  },\n",
      "  {\n",
      "   \"start\": 1,\n",
      "   \"labels\": [\n",
      "    \"Observation\"\n",
      "   ],\n",
      "   \"end\": 3,\n",
      "   \"phrase\": \"not working\"\n",
      "  }\n",
      " ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def bio_to_mention(bio_doc: dict):\n",
    "    \"\"\"Return a Mention-format representation of a BIO-formatted\n",
    "    tagged sentence.\n",
    "\n",
    "    Args:\n",
    "        bio_doc (dict): The BIO doc to convert to the Mention-based doc.\n",
    "\n",
    "    Returns:\n",
    "        dict: A mention-formatted dict created from the bio_doc.\n",
    "    \"\"\"\n",
    "    tokens = bio_doc[\"tokens\"]\n",
    "    labels = bio_doc[\"labels\"]\n",
    "    mentions_list = []\n",
    "\n",
    "    start = 0\n",
    "    end = 0\n",
    "    label = None\n",
    "    for i, (token, label) in enumerate(\n",
    "        zip(tokens, labels)\n",
    "    ):\n",
    "        if label.startswith(\"B-\"):\n",
    "            if len(mentions_list) > 0:\n",
    "                mentions_list[-1][\"end\"] = i\n",
    "            mentions_list.append({\"start\": i, \"labels\": [label[2:]]})\n",
    "        elif label == \"O\" and len(mentions_list) > 0:\n",
    "            mentions_list[-1][\"end\"] = i\n",
    "        if len(mentions_list) == 0:\n",
    "            continue\n",
    "        if i == (len(tokens) - 1) and \"end\" not in mentions_list[-1]:\n",
    "            mentions_list[-1][\"end\"] = i + 1\n",
    "            \n",
    "    for m in mentions_list:\n",
    "        m['phrase'] = \" \".join(tokens[m['start']:m['end']])\n",
    "    return {'tokens': tokens, 'mentions': mentions_list}\n",
    "\n",
    "\n",
    "# For each BIO tagged sentence in tagged_sents, convert it to the mention-based\n",
    "# representation\n",
    "tagged_sents = []\n",
    "for doc in tagged_bio_sents:\n",
    "    mention_doc = bio_to_mention(doc)\n",
    "    tagged_sents.append(mention_doc)\n",
    "\n",
    "# Let's print our example sentence again, this time with the mention-based\n",
    "# representation.\n",
    "# We'll use json.dumps to make it a bit easier to read.\n",
    "print(json.dumps(tagged_sents[12],indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we have added a \"phrase\" to each mention. We technically could get this phrase by looking at the list of tokens from the `start` to the `end` of the mention, but storing it inside `mentions` directly makes things easier later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2. Building a list of potential relations between entities\n",
    "\n",
    "Now we have our data in a more amenable format, but we still need tabular data as required by the RE model. To refresh your memory, this is the required format:\n",
    "\n",
    " - entity 1\n",
    " - entity 2\n",
    " - label of entity 1\n",
    " - label of entity 2\n",
    " - The text between 'broken' and 'rod support', inclusive\n",
    " - The position of entity 1\n",
    " - The position of entity 2\n",
    " - The relation type. \"O\" means no relation.\n",
    " \n",
    "We don't need that last column here as this is what we want our model to predict. We will set it to `None` to denote that no relation has been assigned yet.\n",
    "\n",
    "We also need to add a new column to represent the document index - we will see why later.\n",
    "\n",
    "Here is a helper function to transform our mention-based entity format of a single document into a list of potential relationships between each entity and each other entity in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['repair', 'cracked', 'Activity', 'Observation', 'repair cracked', 0, 1, None, 0]\n"
     ]
    }
   ],
   "source": [
    "def build_potential_relations(tagged_sents) -> list:\n",
    "    \"\"\"Build a list of potential relations, i.e. all possible relationships\n",
    "    between each entity in each document. The 8th column (which denotes the\n",
    "    relationship type) will be set to None. The 9th column is the document index.\n",
    "    \n",
    "    Args:\n",
    "        tagged_sents(list): The list of tagged sentences, where each sentence is a\n",
    "            dict of tokens: [list of tokens] and mentions: [list of mentions].\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of rows, where each row is a potential relationship.\n",
    "    \"\"\"\n",
    "\n",
    "    relations = []\n",
    "    for doc_idx, doc in enumerate(tagged_sents):\n",
    "        for m1_idx, mention_1 in enumerate(doc['mentions']):\n",
    "            entity_1 = \" \".join(doc['tokens'][mention_1['start']: mention_1['end']])\n",
    "            label_1 = mention_1['labels'][0]\n",
    "\n",
    "            for m2_idx, mention_2 in enumerate(doc['mentions']):\n",
    "                if m1_idx == m2_idx:\n",
    "                    continue\n",
    "                entity_2 = \" \".join(doc['tokens'][mention_2['start']: mention_2['end']])\n",
    "                label_2 = mention_2['labels'][0]\n",
    "                mention_text = \" \".join(doc['tokens'][mention_1['start']:mention_2['end']]   )         \n",
    "\n",
    "                relations.append(\n",
    "                    [entity_1, entity_2, label_1, label_2, mention_text, m1_idx, m2_idx, None, doc_idx]         \n",
    "                )\n",
    "    return relations\n",
    "            \n",
    "relations = build_potential_relations(tagged_sents)\n",
    "print(relations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3 Running inference over every row\n",
    "\n",
    "Now that our data is in the same format that we used to train the RE model, we can run the inference on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['repair', 'cracked', 'Activity', 'Observation', 'repair cracked', 0, 1, 'O', 0]\n",
      "['repair', 'hyd', 'Activity', 'Item', 'repair cracked hyd', 0, 2, 'O', 0]\n",
      "['repair', 'tank', 'Activity', 'Item', 'repair cracked hyd tank', 0, 3, 'O', 0]\n",
      "['cracked', 'repair', 'Observation', 'Activity', '', 1, 0, 'O', 0]\n",
      "['cracked', 'hyd', 'Observation', 'Item', 'cracked hyd', 1, 2, 'O', 0]\n",
      "['cracked', 'tank', 'Observation', 'Item', 'cracked hyd tank', 1, 3, 'O', 0]\n",
      "['hyd', 'repair', 'Item', 'Activity', '', 2, 0, 'HAS_ACTIVITY', 0]\n",
      "['hyd', 'cracked', 'Item', 'Observation', '', 2, 1, 'HAS_OBSERVATION', 0]\n",
      "['hyd', 'tank', 'Item', 'Item', 'hyd tank', 2, 3, 'HAS_ITEM', 0]\n",
      "['tank', 'repair', 'Item', 'Activity', '', 3, 0, 'HAS_ACTIVITY', 0]\n"
     ]
    }
   ],
   "source": [
    "def tag_all_relations(relations: list):\n",
    "    \"\"\"Run model inference over every potential relation in the list of\n",
    "    relations.\n",
    "    \n",
    "    Args:\n",
    "        relations(list): The list of (untagged) relations.\n",
    "        \n",
    "    Returns:\n",
    "        tagged_relations(list): The same list, but with the rel_type in the\n",
    "           8th column.\n",
    "    \n",
    "    \"\"\"\n",
    "    tagged_relations = []\n",
    "\n",
    "    for rel in relations:\n",
    "        tagged_rel = rel[:]\n",
    "        rel_type = rel_model.inference(rel)\n",
    "        tagged_rel[7] = rel_type\n",
    "        tagged_relations.append(tagged_rel)\n",
    "    return tagged_relations\n",
    "        \n",
    "rel_model = SimpleMWOREModel() # or FlairREModel()\n",
    "tagged_relations = tag_all_relations(relations)\n",
    "\n",
    "# Print the first 10 rows\n",
    "for row in tagged_relations[:10]:\n",
    "    print(row)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Combining NER+RE\n",
    "\n",
    "Now we have outputs from both the NER model and the RE model. The NER model's output looks like this:\n",
    "\n",
    "    {'tokens': ['a/c', 'not', 'working'],\n",
    "     'mentions': [\n",
    "         {'start': 0, 'labels': ['Item'], 'end': 1},\n",
    "         {'start': 1, 'labels': ['Observation'], 'end': 3}]}         \n",
    "While the RE model's output is shown in the cell above.\n",
    "\n",
    "The next step is to combine the two outputs. Fortunately we stored the document index in the relations, so we can easily join them up.\n",
    "\n",
    "Let's add a 'relations' key to this dictionary. It will capture the relationships between mentions, e.g.\n",
    "\n",
    "    'relations': {'mention_1_id': 0, 'mention_2_id': 1, 'type': 'HAS_OBSERVATION'}\n",
    "    \n",
    "... which denotes that mention 0 ('a/c') has the observation of mention 1 ('not working')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"tokens\": [\n",
      "  \"pump\",\n",
      "  \"fault\"\n",
      " ],\n",
      " \"mentions\": [\n",
      "  {\n",
      "   \"start\": 0,\n",
      "   \"labels\": [\n",
      "    \"Item\"\n",
      "   ],\n",
      "   \"end\": 1,\n",
      "   \"phrase\": \"pump\"\n",
      "  },\n",
      "  {\n",
      "   \"start\": 1,\n",
      "   \"labels\": [\n",
      "    \"Observation\"\n",
      "   ],\n",
      "   \"end\": 2,\n",
      "   \"phrase\": \"fault\"\n",
      "  }\n",
      " ],\n",
      " \"relations\": [\n",
      "  {\n",
      "   \"start\": 0,\n",
      "   \"end\": 1,\n",
      "   \"type\": \"HAS_OBSERVATION\"\n",
      "  }\n",
      " ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for i, sent in enumerate(tagged_sents):\n",
    "    \n",
    "    # Note we only care about the relations that do not have the class \"O\".\n",
    "    doc_relations = [row for row in tagged_relations if row[7] != \"O\" and row[8] == i]\n",
    "    \n",
    "    sent['relations'] = []    \n",
    "    for row in doc_relations:\n",
    "        rel = {'start': row[5], 'end': row[6], 'type': row[7]}     \n",
    "        sent['relations'].append(rel)\n",
    "\n",
    "# Let's print an example...\n",
    "print(json.dumps(tagged_sents[10], indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Creating the graph\n",
    "\n",
    "We now have a data structure that stores the tokens, entity mentions, and relationships between those mentions, for each document. The last step is to put it all into a Neo4j graph so that we can query this information.\n",
    "\n",
    "There are two popular methods for doing this:\n",
    "\n",
    "- Using `py2neo` to programatically insert data into Neo4j\n",
    "- Saving CSVs of your entities and relations, then reading them in via a `LOAD CSV` query in Neo4j\n",
    "\n",
    "The first option is simple but a bit slow, and the second option is a little more complex but much faster. We will go with the first option here in this notebook for simplicity.\n",
    "\n",
    "> Before proceeding, make sure you have created a new graph in Neo4j and that your new Neo4j graph is running.\n",
    "\n",
    "You can download and install Neo4j from here if you haven't already: https://neo4j.com/download/. I will be demonstrating the graph during the class so there's no need to have it installed unless you are also interested in trying out some graph queries yourself.\n",
    "\n",
    "> If you need to build your graph again, make sure to run this cell before running subsequent cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "from py2neo.data import Node, Relationship\n",
    "\n",
    "GRAPH_PASSWORD = \"password\" # Set this to the password of your Neo4J graph\n",
    "\n",
    "\n",
    "def get_node_id(phrase, entity_class):\n",
    "    \"\"\"A simple function to generate an id.\n",
    "    This ensures an entity that can be different classes (pump for example) can have\n",
    "    a unique node for each class type.\n",
    "    \"\"\"\n",
    "    return f\"{phrase}__{entity_class}\"\n",
    "    \n",
    "def create_graph(tagged_sents):\n",
    "    \"\"\"Build the Neo4j graph.\n",
    "    We do this by iterating over each tagged_sentence, and constructing the\n",
    "    graph as follows:\n",
    "     - Create a node to represent the document itself.\n",
    "     - Create nodes for each entity appearing in that document, if they have not\n",
    "       already been created. Each unique combination of entity + class will be added, so\n",
    "       pump (the Item) is different from pump (the Activity).\n",
    "     - Create a relationship between each entity and each document in which it appears.\n",
    "     - Create a relationship between each entity and each other entity it is related to,\n",
    "       via the list of relations.\n",
    "     \n",
    "    Args:\n",
    "        tagged_sents(list): The list of tagged sentences.\n",
    "    \"\"\"\n",
    "    graph = Graph(password = GRAPH_PASSWORD)\n",
    "\n",
    "    # We will start by deleting all nodes and edges in the current graph.\n",
    "    # If we don't do this, we will end up with duplicate nodes and edges when running this script again.\n",
    "    graph.delete_all() \n",
    "\n",
    "    tx = graph.begin()\n",
    "    \n",
    "    # Keep track of the created entity nodes.\n",
    "    # We need a way to map the id of the nodes to the py2neo Node objects so that we can\n",
    "    # easily create relationships between these nodes.\n",
    "    created_entity_nodes = {}\n",
    "    \n",
    "    # Iterate over the list of tagged sentences and programmatically create the graph.\n",
    "    for sent in tagged_sents:\n",
    "        \n",
    "        # Create a node to represent the document.\n",
    "        # Note that if you had additional properties in tagged_sents (such as dates, costs, etc)\n",
    "        # you could add them as properties of the Document nodes here.\n",
    "        document_node = Node(\"Document\", name=\" \".join(sent['tokens']))\n",
    "        tx.create(document_node)\n",
    "        \n",
    "        tokens = sent['tokens']\n",
    "        mentions = sent['mentions']\n",
    "        relations = sent['relations']\n",
    "        \n",
    "        for m in mentions:\n",
    "            start = m['start']\n",
    "            end = m['end']\n",
    "            entity_class = m['labels'][0]        \n",
    "            phrase = \" \".join(tokens[start: end])     \n",
    "                    \n",
    "            # Create a node for this entity mention.\n",
    "            # If the node has already been created (i.e. it exists in created_nodes), \n",
    "            # simply retrieve that Node from created_entity_nodes.\n",
    "            # Otherwise, create it, and add it to created_entity_nodes.\n",
    "            entity_node_id = get_node_id(phrase, entity_class)\n",
    "\n",
    "            if entity_node_id in created_entity_nodes:\n",
    "                entity_node = created_entity_nodes[entity_node_id]\n",
    "            else:\n",
    "                entity_node = Node(\"Entity\", entity_class, _id=entity_node_id, name=phrase)\n",
    "                created_entity_nodes[entity_node_id] = entity_node\n",
    "                tx.create(entity_node)            \n",
    "                        \n",
    "                \n",
    "            # Create a relationship between that node and the document\n",
    "            # in which it appears.               \n",
    "            r = Relationship(entity_node, \"APPEARS_IN\", document_node)\n",
    "            tx.create(r)\n",
    "            \n",
    "        # Create relationships between each (entity_1, entity_2) in the\n",
    "        # list of relations for this document.\n",
    "        for rel in relations:\n",
    "            start = rel['start']\n",
    "            end = rel['end']\n",
    "            \n",
    "            phrase_1 = mentions[start]['phrase']\n",
    "            entity_class_1 = mentions[start]['labels'][0]\n",
    "            \n",
    "            phrase_2 = mentions[end]['phrase']\n",
    "            entity_class_2 = mentions[end]['labels'][0]\n",
    "                       \n",
    "            node_1 = created_entity_nodes[get_node_id(phrase_1, entity_class_1)]\n",
    "            node_2 = created_entity_nodes[get_node_id(phrase_2, entity_class_2)]\n",
    "            \n",
    "            r = Relationship(node_1, rel['type'], node_2)\n",
    "            tx.create(r)\n",
    "    tx.commit()\n",
    "\n",
    "create_graph(tagged_sents)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Querying the graph\n",
    "\n",
    "## TODO: Update with GQVis\n",
    "\n",
    "Now that the graph has been created, we can query it in Neo4j. This section lists some example queries that we can run on our graph. If you would like to try these yourself you can paste them directly into the Neo4j console.\n",
    "\n",
    "First, let's try a simple query. Here is a query that searches for __all failure modes observed on engines__:\n",
    "\n",
    "    MATCH (e:Entity {name: \"engine\"})-[r:HAS_OBSERVATION]->(o:observation)\n",
    "    RETURN e, r, o\n",
    "\n",
    "We can also use our graph as a way to quickly search and access work orders for the entities appearing in those work orders. For example, searching for __all work orders containing a leak__:\n",
    "\n",
    "    MATCH (d:WorkOrder)<-[a:APPEARS_IN]-(o:observation {name: \"leak\"})\n",
    "    RETURN d, a, o\n",
    "\n",
    "We could extend this to also show the items on which the leaks were present:\n",
    "\n",
    "    MATCH (d:WorkOrder)<-[a:APPEARS_IN]-(o:observation {name: \"leak\"})<-[r:HAS_OBSERVATION]-(e:Entity)\n",
    "    RETURN d, a, o, r, e\n",
    "\n",
    "Our queries can also incorporate structured data, such as the start dates of the work orders. Here is an example query for __all assets that had leaks from 25 to 28 July__:\n",
    "\n",
    "    MATCH (d:WorkOrder)<-[a:APPEARS_IN]-(e:Entity)-[r:HAS_OBSERVATION]->(o:observation {name: \"leak\"})-[:APPEARS_IN]->(d)\n",
    "    WHERE d.StartDate >= 20050725\n",
    "    AND d.StartDate <= 20050728\n",
    "    RETURN e, r, o\n",
    "\n",
    "On a larger graph this would also work well with other forms of structured data such as costs. We could query based on specific asset costs, for example.\n",
    "\n",
    "Now that our work orders and downtime events are in one graph, we can also make queries about downtime events. Here is an example query for the __downtime events associated with assets appearing in work orders from 25 to 28 July (where the downtime events occurred in July)__:\n",
    "\n",
    "    MATCH (d:WorkOrder)<-[a:APPEARS_IN]-(e:Entity)-[r:HAS_EVENT]->(x:DowntimeEvent)\n",
    "    WHERE d.StartDate > 20050725\n",
    "    AND d.StartDate < 20050728\n",
    "    AND 20050700 <= x.StartDate <= 20050731\n",
    "    RETURN e, r, x\n",
    "\n",
    "We can of course extend this to specific assets, such as pumps:\n",
    "\n",
    "    MATCH (d:WorkOrder)<-[a:APPEARS_IN]-(e:Entity {name: \"pump\"})-[r:HAS_EVENT]->(x:DowntimeEvent)\n",
    "    WHERE d.StartDate > 20050725\n",
    "    AND d.StartDate < 20050728\n",
    "    AND 20050700 <= x.StartDate <= 20050731\n",
    "    RETURN e, r, x\n",
    "\n",
    "In larger graphs the downtime events could even be further queried based on duration, cost, lost feed, or date ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Future improvements\n",
    "\n",
    "TODO: Get rid of reference to 'downtime'. Perhaps introduce Seq2KG and other models here as alternate ways of text to graph\n",
    "\n",
    "## Incorporating FLOCs\n",
    "\n",
    "Our downtime events are currently linked to Item nodes, but it would make more sense to link them to nodes representing the functional locations.\n",
    "\n",
    "If you are interested in continuing work on this small graph, the next best step would be to create nodes for the functional location data (`floc_data`) and to link the downtime events to those nodes as opposed to the Item nodes.\n",
    "\n",
    "![alt text](images/adding-flocs.png \"Adding FLOCs\")\n",
    "\n",
    "## Frequencies on edge properties\n",
    "\n",
    "We could also improve the graph by incorporating frequencies onto the edge properties. For example, if a \"leak\" occurred on a pump in two different work orders, our link between \"pump\" and \"leak\" could have a property called `frequency` with a value of `2`. This would allow us to query, for example, assets that had a particularly high number of leaks.\n",
    "\n",
    "\n",
    "## Constructing a graph from your own work order data\n",
    "\n",
    "If you have a work order dataset of your own, feel free to download this code and try it out on your dataset. I would be happy to chat if you would like to further discuss the code or if you run into any issues.\n",
    "\n",
    "If you need to extract entities not listed in the lexicon, you will need to update the lexicon file to include your new entities. Alternatively, the LexiconTagger can be substituted for a named entity recognition model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floc_file = \"data/sample_flocs.csv\"\n",
    "floc_data = load_csv(floc_file)\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------\n",
    "\n",
    "## OLD Normalise the entities\n",
    "\n",
    "### MS: Probably going to take this out to save time, I can leave it as a future step for people. It should probably go before NER/RE anyway\n",
    "\n",
    "The next step is to normalise the ngrams, i.e. convert each ngram into a normalised form. This is important as we would prefer to have a single node for a single concept, e.g. one node for \"engine\" as opposed to two nodes for \"engin\" and \"engine\".\n",
    "\n",
    "We will once again be using a lexicon for this task, but it would typically be performed by machine learning.\n",
    "\n",
    "![alt text](images/normalising-entities.png \"Normalising entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_n_file = \"data/lexicon_normalisation.csv\"\n",
    "lexicon_normaliser = LexiconTagger(lexicon_n_file)\n",
    "\n",
    "normalised_work_order_entities = []\n",
    "\n",
    "# For every row in work_order_entities, replace each ngram with its normalised counterpart\n",
    "# as per the normalisation lexicon.\n",
    "# For example, \"engin\" will become \"engine\", \"leaking\" will become \"leak\", etc.\n",
    "for row in work_order_entities:\n",
    "    normalised_work_order_entities.append([(lexicon_normaliser.normalise_ngram(ngram), entity_class) \n",
    "                                           for (ngram, entity_class) in row])\n",
    "    \n",
    "    \n",
    "for row in normalised_work_order_entities:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD Extending the graph to incorporate Downtime events\n",
    "\n",
    "The next step is to incorporate the downtime events.\n",
    "\n",
    "For this exercise we are going to link the Downtime events to the first Item node appearing in the work orders with the same FLOC as the downtime event.\n",
    "\n",
    "\n",
    "![alt text](images/adding-downtime-events.png \"Adding downtime events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = graph.begin()\n",
    "\n",
    "created_downtime_nodes = {}\n",
    "\n",
    "# Create a DowntimeEvent node for each row\n",
    "for i, downtime_row in enumerate(downtime_data):\n",
    "    node = create_structured_node(i, downtime_row, \"DowntimeEvent\", created_downtime_nodes)\n",
    "    \n",
    "    # Get all work order nodes with the same FLOC and link the DowntimeEvent to the Items appearing\n",
    "    # in those work orders\n",
    "    for j, work_order_row in enumerate(work_order_data):\n",
    "        if work_order_row[\"FLOC\"] == downtime_row[\"FLOC\"]:\n",
    "            \n",
    "            work_order_entities = normalised_work_order_entities[j]\n",
    "            \n",
    "            for (ngram, entity_class) in work_order_entities:\n",
    "                if entity_class != \"item\": continue    # We don't need to link non-items to downtime events               \n",
    "                    \n",
    "                item_node = created_entity_nodes[ngram]\n",
    "                relationship = Relationship( item_node, \"HAS_EVENT\", node )\n",
    "                tx.create(relationship)\n",
    "                break\n",
    "\n",
    "    \n",
    "tx.commit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
